{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "452d627c",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52221a59",
   "metadata": {},
   "source": [
    "##### Langchain Document Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2c66308",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "import os\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cac29bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_pdf(pdf_directory):\n",
    "    all_documents =[]\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    print(\"Total pdf files: \", len(pdf_files))\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing pdf with name {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            document = loader.load()\n",
    "\n",
    "            for doc in document:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = \"pdf\"\n",
    "                print(\"Metadata added Successfully\")\n",
    "\n",
    "            all_documents.extend(document)\n",
    "            print(f\"{len(document)} pages loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred in {pdf_file.name}\")\n",
    "    print(f\"Total {len(all_documents)} pages\")\n",
    "    return all_documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "aa1c4387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pdf files:  3\n",
      "\n",
      "Processing pdf with name attention.pdf\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "11 pages loaded\n",
      "\n",
      "Processing pdf with name llms.pdf\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "47 pages loaded\n",
      "\n",
      "Processing pdf with name agentic_rag.pdf\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "39 pages loaded\n",
      "Total 97 pages\n"
     ]
    }
   ],
   "source": [
    "pdf_dir = \"../data/pdfs\"\n",
    "documents  = process_all_pdf(pdf_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "371c1706",
   "metadata": {},
   "outputs": [],
   "source": [
    "  ### Split into Chunks  \n",
    "  def split_document(document, chunk_size=1000, chunk_overlap=200):\n",
    "     text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size,\n",
    "        chunk_overlap = chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "     )\n",
    "\n",
    "     split_docs = text_splitter.split_documents(document)\n",
    "     print(f\"Splitted {len(document)} documents into {len(split_docs)} chunks\")\n",
    "\n",
    "     if(split_docs):\n",
    "        print(\"Example chunk\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}......\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "     return split_docs\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b6ad207f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted 97 documents into 539 chunks\n",
      "Example chunk\n",
      "Content: Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz......\n",
      "Metadata: {'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdfs/attention.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}\n"
     ]
    }
   ],
   "source": [
    "chunks = split_document(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741cdb34",
   "metadata": {},
   "source": [
    "### Embeddings and VectorStore DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "75881e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushigupta/Documents/Projects/rag/ytrag/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4c883241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded Successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x162ba7590>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using sentence transformers\"\"\"\n",
    "\n",
    "    def __init__(self, model_name : str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"Initializing embedding manager\n",
    "        Args: model_name : hugging face model name for sentence transformer\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load sentence transformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded Successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred in loading model {self.model_name} : {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Generate embedding for a list of texts\n",
    "        Args:\n",
    "            texts: list of text strings to embed\n",
    "        Returns: \n",
    "            numpy array of embeddings with shape (len(texts), embedding_dims)\n",
    "        \"\"\"\n",
    "\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        print(f\"Generating embeddings for {len(texts)} texts\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar = True)\n",
    "        print(f\"Generating embeddings with shape {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "    def get_embeddings_dimensions(self) -> int:\n",
    "        \"\"\"Get the embedding dimensions of the model\"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        return self.mode.get_sentence_embedding_dimension()\n",
    "\n",
    "##initializing embedding manager\n",
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3bac9b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x165417b10>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "import numpy as np\n",
    "import chromadb\n",
    "from typing import List, Any\n",
    "\n",
    "class VectorStore:\n",
    "    \"\"\"For storing embeddings in Chroma vector store\"\"\"\n",
    "\n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            collection_name : Name of the ChromaDB collection\n",
    "            persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory \n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "\n",
    "        Args:\n",
    "            documents: List of LangChain Document objects\n",
    "            embeddings: Corresponding embeddings for documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "\n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"  \n",
    "            ids.append(doc_id)\n",
    "\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata[\"doc_index\"] = i\n",
    "            metadata[\"content_length\"] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            documents_text.append(doc.page_content)\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "\n",
    "        try:\n",
    "            self.collection.add( \n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store.\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "# Initialize and test\n",
    "vector_store = VectorStore()\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8bfc25c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 539 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 17/17 [00:09<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings with shape (539, 384)\n",
      "Adding 539 documents to vector store...\n",
      "Successfully added 539 documents to vector store.\n",
      "Total documents in collection: 539\n"
     ]
    }
   ],
   "source": [
    "texts = [doc.page_content for doc in chunks]\n",
    "\n",
    "##Generate the embeddings\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "##Store embedding in vector store\n",
    "vector_store.add_documents(chunks, embeddings) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f1d276",
   "metadata": {},
   "source": [
    "### Retriever Pipeline from Vector Store "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f7927962",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query based retriever from the vectore store\"\"\"\n",
    "\n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "            Initialize the retriever\n",
    "            Args:\n",
    "                vector_store: vector database that contains vector embeddings\n",
    "                embedding_manager: Embedding manager for generate query embeddings\n",
    "        \"\"\"\n",
    "\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a particular query\n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to run\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "        \n",
    "        Return:\n",
    "            List of Dictionaries containing retrieved documnets with their metadata\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"Retrieving documents for query: {query}\")\n",
    "        print(f\"Top K {top_k} , Score Threshold: {score_threshold}\")\n",
    "\n",
    "        ##Generate embedding for the asked query\n",
    "        query_embedding = embedding_manager.generate_embeddings([query])[0]\n",
    "\n",
    "        ##Search context in vector store\n",
    "        try: \n",
    "            results =  self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results = top_k\n",
    "            )\n",
    "\n",
    "            retrieved_docs = []\n",
    "\n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "\n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    similarity = 1-distance\n",
    "\n",
    "                    if similarity >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id' : doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity,\n",
    "                            'distance': distance,\n",
    "                            'rank': i+1\n",
    "                        })\n",
    "                    \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "         \n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            return retrieved_docs \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return [] \n",
    "rag_retriever = RAGRetriever(vector_store, embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e6b448f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x1654cdc50>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3b773343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: What is Large Language Models\n",
      "Top K 5 , Score Threshold: 0.0\n",
      "Generating embeddings for 1 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings with shape (1, 384)\n",
      "Retrieved 5 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_973cf1c7_47',\n",
       "  'content': 'Qiu), muhammad.saqib@data61.csiro.au (Muhammad Saqib),\\nsaeed.anwar@kfupm.edu.sa (Saeed Anwar),\\nmuhammad.usman@kfupm.edu.sa (Muhammad Usman),\\nnaveed.akhtar1@unimelb.edu.au (Naveed Akhtar),\\nnick.barnes@anu.edu.au (Nick Barnes), ajmal.mian@uwa.edu.au\\n(Ajmal Mian)\\nFigure 1: The trend of papers released over the years containing keywords\\n\"Large Language Model\", \"Large Language Model+ Fine-Tuning\", and \"Large\\nLanguage Model + Alignment\".\\nPreprint submitted to Elsevier October 18, 2024\\narXiv:2307.06435v10  [cs.CL]  17 Oct 2024',\n",
       "  'metadata': {'keywords': '',\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'creationdate': '2024-10-18T00:29:28+00:00',\n",
       "   'source_file': 'llms.pdf',\n",
       "   'doc_index': 47,\n",
       "   'trapped': '/False',\n",
       "   'subject': '',\n",
       "   'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'source': '../data/pdfs/llms.pdf',\n",
       "   'page': 0,\n",
       "   'file_type': 'pdf',\n",
       "   'content_length': 525,\n",
       "   'title': 'A Comprehensive Overview of Large Language Models',\n",
       "   'moddate': '2024-10-18T00:29:28+00:00',\n",
       "   'total_pages': 47,\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'page_label': '1'},\n",
       "  'similarity_score': 0.5394578576087952,\n",
       "  'distance': 0.46054214239120483,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_dce1c55d_321',\n",
       "  'content': 'windows for large language models, in: Proceedings of the 61st Annual\\nMeeting of the Association for Computational Linguistics (V olume 1:\\nLong Papers), 2023, pp. 6383–6402. 18\\n[191] W. Wang, L. Dong, H. Cheng, X. Liu, X. Yan, J. Gao, F. Wei,\\nAugmenting language models with long-term memory, arXiv preprint\\narXiv:2306.07174 (2023). 18\\n[192] X. Xu, Z. Gou, W. Wu, Z.-Y . Niu, H. Wu, H. Wang, S. Wang, Long\\ntime no see! open-domain conversation with long-term persona memory,\\narXiv preprint arXiv:2203.05797 (2022). 18\\n[193] S. Borgeaud, A. Mensch, J. Ho ffmann, T. Cai, E. Rutherford, K. Milli-\\ncan, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark, et al.,\\nImproving language models by retrieving from trillions of tokens, in:\\nInternational conference on machine learning, PMLR, 2022, pp. 2206–\\n2240. 18, 19, 34\\n[194] W. Zhong, L. Guo, Q. Gao, Y . Wang, Memorybank: Enhanc-\\ning large language models with long-term memory, arXiv preprint\\narXiv:2305.10250 (2023). 18',\n",
       "  'metadata': {'source': '../data/pdfs/llms.pdf',\n",
       "   'title': 'A Comprehensive Overview of Large Language Models',\n",
       "   'keywords': '',\n",
       "   'source_file': 'llms.pdf',\n",
       "   'content_length': 976,\n",
       "   'trapped': '/False',\n",
       "   'page_label': '40',\n",
       "   'doc_index': 321,\n",
       "   'creationdate': '2024-10-18T00:29:28+00:00',\n",
       "   'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;',\n",
       "   'subject': '',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'total_pages': 47,\n",
       "   'file_type': 'pdf',\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'moddate': '2024-10-18T00:29:28+00:00',\n",
       "   'page': 39},\n",
       "  'similarity_score': 0.4400674104690552,\n",
       "  'distance': 0.5599325895309448,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_69f05f03_43',\n",
       "  'content': 'A Comprehensive Overview of Large Language Models\\nHumza Naveeda, Asad Ullah Khanb,∗, Shi Qiuc,∗, Muhammad Saqibd,e,∗, Saeed Anwarf,g, Muhammad Usmanf,g, Naveed Akhtarh,j,\\nNick Barnesi, Ajmal Mianj\\naThe University of Sydney, Sydney, Australia\\nbUniversity of Engineering and Technology (UET), Lahore, Pakistan\\ncThe Chinese University of Hong Kong (CUHK), HKSAR, China\\ndUniversity of Technology Sydney (UTS), Sydney, Australia\\neCommonwealth Scientific and Industrial Research Organisation (CSIRO), Sydney, Australia\\nfKing Fahd University of Petroleum and Minerals (KFUPM), Dhahran, Saudi Arabia\\ngSDAIA-KFUPM Joint Research Center for Artificial Intelligence (JRCAI), Dhahran, Saudi Arabia\\nhThe University of Melbourne (UoM), Melbourne, Australia\\niAustralian National University (ANU), Canberra, Australia\\njThe University of Western Australia (UWA), Perth, Australia\\nAbstract\\nLarge Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and',\n",
       "  'metadata': {'keywords': '',\n",
       "   'file_type': 'pdf',\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'total_pages': 47,\n",
       "   'page_label': '1',\n",
       "   'title': 'A Comprehensive Overview of Large Language Models',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'source': '../data/pdfs/llms.pdf',\n",
       "   'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'trapped': '/False',\n",
       "   'doc_index': 43,\n",
       "   'content_length': 992,\n",
       "   'page': 0,\n",
       "   'creationdate': '2024-10-18T00:29:28+00:00',\n",
       "   'source_file': 'llms.pdf',\n",
       "   'subject': '',\n",
       "   'moddate': '2024-10-18T00:29:28+00:00'},\n",
       "  'similarity_score': 0.3886106610298157,\n",
       "  'distance': 0.6113893389701843,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_ad8ceee6_328',\n",
       "  'content': 'guage models with massive tools via tool embeddings, arXiv preprint\\narXiv:2305.11554 (2023). 19\\n[221] S. G. Patil, T. Zhang, X. Wang, J. E. Gonzalez, Gorilla: Large language\\nmodel connected with massive apis, arXiv preprint arXiv:2305.15334\\n(2023). 19\\n[222] Q. Xu, F. Hong, B. Li, C. Hu, Z. Chen, J. Zhang, On the tool manipu-\\nlation capability of open-source large language models, arXiv preprint\\narXiv:2305.16504 (2023). 19\\n[223] Y . Qin, S. Liang, Y . Ye, K. Zhu, L. Yan, Y . Lu, Y . Lin, X. Cong, X. Tang,\\nB. Qian, et al., Toolllm: Facilitating large language models to master\\n16000+ real-world apis, arXiv preprint arXiv:2307.16789 (2023). 19,\\n40',\n",
       "  'metadata': {'page_label': '40',\n",
       "   'trapped': '/False',\n",
       "   'source': '../data/pdfs/llms.pdf',\n",
       "   'doc_index': 328,\n",
       "   'title': 'A Comprehensive Overview of Large Language Models',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'keywords': '',\n",
       "   'content_length': 651,\n",
       "   'page': 39,\n",
       "   'subject': '',\n",
       "   'total_pages': 47,\n",
       "   'file_type': 'pdf',\n",
       "   'creationdate': '2024-10-18T00:29:28+00:00',\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'moddate': '2024-10-18T00:29:28+00:00',\n",
       "   'source_file': 'llms.pdf'},\n",
       "  'similarity_score': 0.3035588264465332,\n",
       "  'distance': 0.6964411735534668,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_11ec11e5_287',\n",
       "  'content': 'Q. Liu, Aligning large language models with human: A survey, arXiv\\npreprint arXiv:2307.12966 (2023). 2\\n[57] X. Zhu, J. Li, Y . Liu, C. Ma, W. Wang, A survey on model compression\\nfor large language models, arXiv preprint arXiv:2308.07633 (2023). 2\\n[58] S. Yin, C. Fu, S. Zhao, K. Li, X. Sun, T. Xu, E. Chen, A survey on multi-\\nmodal large language models, arXiv preprint arXiv:2306.13549 (2023).\\n2, 22, 23\\n[59] J. J. Webster, C. Kit, Tokenization as the initial phase in nlp, in: COL-\\nING 1992 volume 4: The 14th international conference on computa-\\ntional linguistics, 1992. 4\\n[60] T. Kudo, Subword regularization: Improving neural network translation\\nmodels with multiple subword candidates, in: Proceedings of the 56th\\nAnnual Meeting of the Association for Computational Linguistics (V ol-\\nume 1: Long Papers), 2018, pp. 66–75. 4\\n[61] R. Sennrich, B. Haddow, A. Birch, Neural machine translation of rare\\nwords with subword units, in: Proceedings of the 54th Annual Meet-',\n",
       "  'metadata': {'title': 'A Comprehensive Overview of Large Language Models',\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'doc_index': 287,\n",
       "   'subject': '',\n",
       "   'moddate': '2024-10-18T00:29:28+00:00',\n",
       "   'source': '../data/pdfs/llms.pdf',\n",
       "   'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;',\n",
       "   'keywords': '',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'file_type': 'pdf',\n",
       "   'source_file': 'llms.pdf',\n",
       "   'total_pages': 47,\n",
       "   'content_length': 972,\n",
       "   'page_label': '37',\n",
       "   'page': 36,\n",
       "   'creationdate': '2024-10-18T00:29:28+00:00',\n",
       "   'trapped': '/False',\n",
       "   'creator': 'LaTeX with hyperref'},\n",
       "  'similarity_score': 0.26904547214508057,\n",
       "  'distance': 0.7309545278549194,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"What is Large Language Models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d61426",
   "metadata": {},
   "source": [
    "## Integrationg VectorDB Context Pipeline with LLM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "51b2e00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## simple rag pipeline with GROQ LLM\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() \n",
    "\n",
    "##Initialize groq LLM and set GROQ_API_KEY in Environments\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\") \n",
    "llm=ChatGroq(groq_api_key=groq_api_key, model_name=\"llama-3.1-8b-instant\",temperature=0.1,max_tokens=1024)\n",
    "\n",
    "def rag(query, retriever, llm , top_k=3):\n",
    "    \"\"\"Retreieve the context\"\"\"\n",
    "    results = retriever.retrieve(query, top_k=top_k)\n",
    "    context=\"\\n\\n\".join([doc[\"content\"] for doc in results]) if results else \"\"\n",
    "    if not context:\n",
    "        print(f\"No relevant context found to answer the question\")\n",
    "    \n",
    "    \"\"\"Generate the answer using groq LLM\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"Use the following context to answer the question concisely.\n",
    "        Context: {context}\n",
    "        Question: {query}\n",
    "        Answer: \n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "    return response.content\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "320189f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: What is a Large Language Model\n",
      "Top K 3 , Score Threshold: 0.0\n",
      "Generating embeddings for 1 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings with shape (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A Large Language Model (LLM) is a type of artificial intelligence model that has demonstrated remarkable capabilities in natural language processing tasks.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = rag(\"What is a Large Language Model\", rag_retriever, llm)\n",
    "ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3502d6",
   "metadata": {},
   "source": [
    "### Enhanced RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4df23b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_advanced(query, retriever, llm, top_k=5, min_score=0.2, return_context=False):\n",
    "    \"\"\"\n",
    "    Rag with extra features :\n",
    "        retruns answers, sources, confidence, and optionally full context\n",
    "    \"\"\"\n",
    "\n",
    "    results = retriever.retrieve(query, top_k=top_k, score_threshold=min_score)\n",
    "    if not results:\n",
    "        return {\"source\": \"No relevant context found\", \"source\": [] , \"confidence\": 0.0, \"context\": \"\"}\n",
    "\n",
    "    context = \"\\n\\n\".join([doc[\"content\"] for doc in results])\n",
    "    sources = [{\n",
    "         \"source\" : doc[\"metadata\"].get(\"source\", doc[\"metadata\"].get(\"source\", \"unknown\")),\n",
    "         \"page\" : doc[\"metadata\"].get(\"page\", \"unknown\"),\n",
    "         \"score\" : doc[\"similarity_score\"],\n",
    "         \"preview\" : doc[\"content\"][:300] + \".....\"\n",
    "    } for doc in results]\n",
    "\n",
    "    confidence = max([doc[\"similarity_score\"] for doc in results])\n",
    "\n",
    "    prompt = f\"\"\"Use the follwoing context to answer the following question.\n",
    "        context : {context}\n",
    "        question : {query}\n",
    "        answer : \"\"\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "\n",
    "    output = {\n",
    "        \"answer\" : response.content,\n",
    "        \"source\" : sources,\n",
    "        \"confidence\" : confidence\n",
    "    }\n",
    "\n",
    "    if return_context:\n",
    "        output[\"context\"] = context\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6353b39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: What is multi head attention?\n",
      "Top K 3 , Score Threshold: 0.0\n",
      "Generating embeddings for 1 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings with shape (1, 384)\n",
      "Retrieved 2 documents (after filtering)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': 'Multi-head attention is a mechanism used in the Transformer model that allows the model to jointly attend to information from different representation subspaces at different positions. It is achieved by applying attention in parallel to linearly projected versions of the queries, keys, and values, and then concatenating and projecting the output. This is done using multiple attention \"heads\" (in this case, 8 parallel attention layers), each with its own set of learned linear projections.',\n",
       " 'source': [{'source': '../data/pdfs/attention.pdf',\n",
       "   'page': 4,\n",
       "   'score': 0.13510406017303467,\n",
       "   'preview': 'MultiHead(Q,K,V ) = Concat(head1,..., headh)WO\\nwhere headi = Attention(QWQ\\ni ,KW K\\ni ,VW V\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈Rdmodel×dk , WK\\ni ∈Rdmodel×dk , WV\\ni ∈Rdmodel×dv\\nand WO ∈Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these .....'},\n",
       "  {'source': '../data/pdfs/attention.pdf',\n",
       "   'page': 3,\n",
       "   'score': 0.104733407497406,\n",
       "   'preview': 'we found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensio.....'}],\n",
       " 'confidence': 0.13510406017303467,\n",
       " 'context': 'MultiHead(Q,K,V ) = Concat(head1,..., headh)WO\\nwhere headi = Attention(QWQ\\ni ,KW K\\ni ,VW V\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈Rdmodel×dk , WK\\ni ∈Rdmodel×dk , WV\\ni ∈Rdmodel×dv\\nand WO ∈Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h= 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n\\nwe found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = ∑dk\\ni=1 qiki, has mean 0 and variance dk.\\n4'}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = rag_advanced(\"What is multi head attention?\", rag_retriever, llm, top_k=3, min_score=0.0, return_context=True)\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "99cf39b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Advanced RAG Pipeline: Streaming, Citations, History, Summarization ---\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "class AdvancedRAGPipeline:\n",
    "    def __init__(self, retriever, llm):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "        self.history = []  # Store query history\n",
    "\n",
    "    def query(self, question: str, top_k: int = 5, min_score: float = 0.2, stream: bool = False, summarize: bool = False) -> Dict[str, Any]:\n",
    "        # Retrieve relevant documents\n",
    "        results = self.retriever.retrieve(question, top_k=top_k, score_threshold=min_score)\n",
    "        if not results:\n",
    "            answer = \"No relevant context found.\"\n",
    "            sources = []\n",
    "            context = \"\"\n",
    "        else:\n",
    "            context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "            sources = [{\n",
    "                'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "                'page': doc['metadata'].get('page', 'unknown'),\n",
    "                'score': doc['similarity_score'],\n",
    "                'preview': doc['content'][:120] + '...'\n",
    "            } for doc in results]\n",
    "            # Streaming answer simulation\n",
    "            prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\"\"\n",
    "            if stream:\n",
    "                print(\"Streaming answer:\")\n",
    "                for i in range(0, len(prompt), 80):\n",
    "                    print(prompt[i:i+80], end='', flush=True)\n",
    "                    time.sleep(0.05)\n",
    "                print()\n",
    "            response = self.llm.invoke([prompt.format(context=context, question=question)])\n",
    "            answer = response.content\n",
    "\n",
    "        # Add citations to answer\n",
    "        citations = [f\"[{i+1}] {src['source']} (page {src['page']})\" for i, src in enumerate(sources)]\n",
    "        answer_with_citations = answer + \"\\n\\nCitations:\\n\" + \"\\n\".join(citations) if citations else answer\n",
    "\n",
    "        # Optionally summarize answer\n",
    "        summary = None\n",
    "        if summarize and answer:\n",
    "            summary_prompt = f\"Summarize the following answer in 2 sentences:\\n{answer}\"\n",
    "            summary_resp = self.llm.invoke([summary_prompt])\n",
    "            summary = summary_resp.content\n",
    "\n",
    "        # Store query history\n",
    "        self.history.append({\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'sources': sources,\n",
    "            'summary': summary\n",
    "        })\n",
    "\n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer_with_citations,\n",
    "            'sources': sources,\n",
    "            'summary': summary,\n",
    "            'history': self.history\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "955f8931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: what are Large Language Models\n",
      "Top K 3 , Score Threshold: 0.0\n",
      "Generating embeddings for 1 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings with shape (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n",
      "Streaming answer:\n",
      "Use the following context to answer the question concisely.\n",
      "Context:\n",
      "Qiu), muhammad.saqib@data61.csiro.au (Muhammad Saqib),\n",
      "saeed.anwar@kfupm.edu.sa (Saeed Anwar),\n",
      "muhammad.usman@kfupm.edu.sa (Muhammad Usman),\n",
      "naveed.akhtar1@unimelb.edu.au (Naveed Akhtar),\n",
      "nick.barnes@anu.edu.au (Nick Barnes), ajmal.mian@uwa.edu.au\n",
      "(Aj"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mal Mian)\n",
      "Figure 1: The trend of papers released over the years containing keywords\n",
      "\"Large Language Model\", \"Large Language Model+ Fine-Tuning\", and \"Large\n",
      "Language Model + Alignment\".\n",
      "Preprint submitted to Elsevier October 18, 2024\n",
      "arXiv:2307.06435v10  [cs.CL]  17 Oct 2024\n",
      "\n",
      "windows for large language models, in: Proceedings of the 61st Annual\n",
      "Meeting of the Association for Computational Linguistics (V olume 1:\n",
      "Long Papers), 2023, pp. 6383–6402. 18\n",
      "[191] W. Wang, L. Dong, H. Cheng, X. Liu, X. Yan, J. Gao, F. Wei,\n",
      "Augmenting language models with long-term memory, arXiv preprint\n",
      "arXiv:2306.07174 (2023). 18\n",
      "[192] X. Xu, Z. Gou, W. Wu, Z.-Y . Niu, H. Wu, H. Wang, S. Wang, Long\n",
      "time no see! open-domain conversation with long-term persona memory,\n",
      "arXiv preprint arXiv:2203.05797 (2022). 18\n",
      "[193] S. Borgeaud, A. Mensch, J. Ho ffmann, T. Cai, E. Rutherford, K. Milli-\n",
      "can, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark, et al.,\n",
      "Improving language models by retrieving from trillions of tokens, in:\n",
      "International conference on machine learning, PMLR, 2022, pp. 2206–\n",
      "2240. 18, 19, 34\n",
      "[194] W. Zhong, L. Guo, Q. Gao, Y . Wang, Memorybank: Enhanc-\n",
      "ing large language models with long-term memory, arXiv preprint\n",
      "arXiv:2305.10250 (2023). 18\n",
      "\n",
      "A Comprehensive Overview of Large Language Models\n",
      "Humza Naveeda, Asad Ullah Khanb,∗, Shi Qiuc,∗, Muhammad Saqibd,e,∗, Saeed Anwarf,g, Muhammad Usmanf,g, Naveed Akhtarh,j,\n",
      "Nick Barnesi, Ajmal Mianj\n",
      "aThe University of Sydney, Sydney, Australia\n",
      "bUniversity of Engineering and Technology (UET), Lahore, Pakistan\n",
      "cThe Chinese University of Hong Kong (CUHK), HKSAR, China\n",
      "dUniversity of Technology Sydney (UTS), Sydney, Australia\n",
      "eCommonwealth Scientific and Industrial Research Organisation (CSIRO), Sydney, Australia\n",
      "fKing Fahd University of Petroleum and Minerals (KFUPM), Dhahran, Saudi Arabia\n",
      "gSDAIA-KFUPM Joint Research Center for Artificial Intelligence (JRCAI), Dhahran, Saudi Arabia\n",
      "hThe University of Melbourne (UoM), Melbourne, Australia\n",
      "iAustralian National University (ANU), Canberra, Australia\n",
      "jThe University of Western Australia (UWA), Perth, Australia\n",
      "Abstract\n",
      "Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and\n",
      "\n",
      "Question: what are Large Language Models\n",
      "\n",
      "Answer:\n",
      "\n",
      "Final Answer: Large Language Models (LLMs) are a type of artificial intelligence model that have demonstrated remarkable capabilities in natural language processing tasks.\n",
      "\n",
      "Citations:\n",
      "[1] llms.pdf (page 0)\n",
      "[2] llms.pdf (page 39)\n",
      "[3] llms.pdf (page 0)\n",
      "Summary: Large Language Models (LLMs) are a type of artificial intelligence model that excel in natural language processing tasks. They have shown impressive capabilities in handling and understanding human language.\n",
      "History: {'question': 'what are Large Language Models', 'answer': 'Large Language Models (LLMs) are a type of artificial intelligence model that have demonstrated remarkable capabilities in natural language processing tasks.', 'sources': [{'source': 'llms.pdf', 'page': 0, 'score': 0.5477049350738525, 'preview': 'Qiu), muhammad.saqib@data61.csiro.au (Muhammad Saqib),\\nsaeed.anwar@kfupm.edu.sa (Saeed Anwar),\\nmuhammad.usman@kfupm.edu....'}, {'source': 'llms.pdf', 'page': 39, 'score': 0.4616367816925049, 'preview': 'windows for large language models, in: Proceedings of the 61st Annual\\nMeeting of the Association for Computational Lingu...'}, {'source': 'llms.pdf', 'page': 0, 'score': 0.396869421005249, 'preview': 'A Comprehensive Overview of Large Language Models\\nHumza Naveeda, Asad Ullah Khanb,∗, Shi Qiuc,∗, Muhammad Saqibd,e,∗, Sa...'}], 'summary': 'Large Language Models (LLMs) are a type of artificial intelligence model that excel in natural language processing tasks. They have shown impressive capabilities in handling and understanding human language.'}\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "adv_rag = AdvancedRAGPipeline(rag_retriever, llm)\n",
    "result = adv_rag.query(\"what are Large Language Models\", top_k=3, min_score=0.0, stream=True, summarize=True)\n",
    "print(\"\\nFinal Answer:\", result['answer'])\n",
    "print(\"Summary:\", result['summary'])\n",
    "print(\"History:\", result['history'][-1])\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52743d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
