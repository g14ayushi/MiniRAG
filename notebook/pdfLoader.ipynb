{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "452d627c",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52221a59",
   "metadata": {},
   "source": [
    "##### Langchain Document Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2c66308",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "import os\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cac29bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_pdf(pdf_directory):\n",
    "    all_documents =[]\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    print(\"Total pdf files: \", len(pdf_files))\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing pdf with name {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            document = loader.load()\n",
    "\n",
    "            for doc in document:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = \"pdf\"\n",
    "                print(\"Metadata added Successfully\")\n",
    "\n",
    "            all_documents.extend(document)\n",
    "            print(f\"{len(document)} pages loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred in {pdf_file.name}\")\n",
    "    print(f\"Total {len(all_documents)} pages\")\n",
    "    return all_documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa1c4387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pdf files:  3\n",
      "\n",
      "Processing pdf with name attention.pdf\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "11 pages loaded\n",
      "\n",
      "Processing pdf with name llms.pdf\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "47 pages loaded\n",
      "\n",
      "Processing pdf with name agentic_rag.pdf\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "Metadata added Successfully\n",
      "39 pages loaded\n",
      "Total 97 pages\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdfs/attention.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser ∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdfs/attention.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Recurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigniﬁcant improvements in computational efﬁciency through factorization tricks [18] and conditional\\ncomputation [26], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [28].\\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\\nHere, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence\\nof continuous representations z = (z1,...,z n). Given z, the decoder then generates an output\\nsequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive\\n[9], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\\n2'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdfs/attention.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Figure 1: The Transformer - model architecture.\\nwise fully connected feed-forward network. We employ a residual connection [10] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x+ Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\n3'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdfs/attention.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices Kand V. We compute\\nthe matrix of outputs as:\\nAttention(Q,K,V ) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = ∑dk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdfs/attention.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='MultiHead(Q,K,V ) = Concat(head1,..., headh)WO\\nwhere headi = Attention(QWQ\\ni ,KW K\\ni ,VW V\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈Rdmodel×dk , WK\\ni ∈Rdmodel×dk , WV\\ni ∈Rdmodel×dv\\nand WO ∈Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h= 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation ﬂow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0,xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [24]. In the embedding layers, we multiply those weights by √dmodel.\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n5'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdfs/attention.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 ·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel )\\nPE(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto 10000 ·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1,...,x n) to another sequence of equal length (z1,...,z n), with xi,zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdfs/attention.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='the input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k<n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 15], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+ n·d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [31]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [17] with β1 = 0.9, β2 = 0.98 and ϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate= d−0.5\\nmodel ·min(step_num−0.5,step_num·warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the ﬁrst warmup_stepstraining steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps= 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\nResidual Dropout We apply dropout [27] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\n7'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdfs/attention.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [15] 23.75\\nDeep-Att + PosUnk [32] 39.2 1.0 ·1020\\nGNMT + RL [31] 24.6 39.92 2.3 ·1019 1.4 ·1020\\nConvS2S [8] 25.16 40.46 9.6 ·1018 1.5 ·1020\\nMoE [26] 26.03 40.56 2.0 ·1019 1.2 ·1020\\nDeep-Att + PosUnk Ensemble [32] 40.4 8.0 ·1020\\nGNMT + RL Ensemble [31] 26.30 41.16 1.8 ·1020 1.1 ·1021\\nConvS2S Ensemble [8] 26.36 41.29 7.7 ·1019 1.2 ·1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.0 2.3 ·1019\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [30]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The conﬁguration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α= 0.6 [31]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [31].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of ﬂoating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision ﬂoating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdfs/attention.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdfs/attention.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='References\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[9] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n10'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdfs/attention.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 0, 'page_label': '1', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='A Comprehensive Overview of Large Language Models\\nHumza Naveeda, Asad Ullah Khanb,∗, Shi Qiuc,∗, Muhammad Saqibd,e,∗, Saeed Anwarf,g, Muhammad Usmanf,g, Naveed Akhtarh,j,\\nNick Barnesi, Ajmal Mianj\\naThe University of Sydney, Sydney, Australia\\nbUniversity of Engineering and Technology (UET), Lahore, Pakistan\\ncThe Chinese University of Hong Kong (CUHK), HKSAR, China\\ndUniversity of Technology Sydney (UTS), Sydney, Australia\\neCommonwealth Scientific and Industrial Research Organisation (CSIRO), Sydney, Australia\\nfKing Fahd University of Petroleum and Minerals (KFUPM), Dhahran, Saudi Arabia\\ngSDAIA-KFUPM Joint Research Center for Artificial Intelligence (JRCAI), Dhahran, Saudi Arabia\\nhThe University of Melbourne (UoM), Melbourne, Australia\\niAustralian National University (ANU), Canberra, Australia\\njThe University of Western Australia (UWA), Perth, Australia\\nAbstract\\nLarge Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and\\nbeyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse\\ntopics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs,\\nrobotics, datasets, benchmarking, e fficiency, and more. With the rapid development of techniques and regular breakthroughs in\\nLLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering\\nthe rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise\\nyet comprehensive overview of the recent developments in this field. This article provides an overview of the literature on a broad\\nrange of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background concepts\\nalong with covering the advanced topics at the frontier of research in LLMs. This review article is intended to provide not only a\\nsystematic survey but also a quick, comprehensive reference for the researchers and practitioners to draw insights from extensive,\\ninformative summaries of the existing works to advance the LLM research.\\nKeywords:\\nLarge Language Models, LLMs, chatGPT, Augmented LLMs, Multimodal LLMs, LLM training, LLM Benchmarking\\n1. Introduction\\nLanguage plays a fundamental role in facilitating commu-\\nnication and self-expression for humans and their interaction\\nwith machines. The need for generalized models stems from\\nthe growing demand for machines to handle complex language\\ntasks, including translation, summarization, information re-\\ntrieval, conversational interactions, etc. Recently, significant\\nbreakthroughs have been witnessed in language models, pri-\\nmarily attributed to transformers [1], increased computational\\ncapabilities, and the availability of large-scale training data.\\nThese developments have brought about a revolutionary trans-\\nformation by enabling the creation of LLMs that can approxi-\\nmate human-level performance on various tasks [2, 3]. Large\\n∗Equal contribution\\nEmail addresses: humza_naveed@yahoo.com (Humza Naveed),\\naukhanee@gmail.com (Asad Ullah Khan), shiqiu@cse.cuhk.edu.hk (Shi\\nQiu), muhammad.saqib@data61.csiro.au (Muhammad Saqib),\\nsaeed.anwar@kfupm.edu.sa (Saeed Anwar),\\nmuhammad.usman@kfupm.edu.sa (Muhammad Usman),\\nnaveed.akhtar1@unimelb.edu.au (Naveed Akhtar),\\nnick.barnes@anu.edu.au (Nick Barnes), ajmal.mian@uwa.edu.au\\n(Ajmal Mian)\\nFigure 1: The trend of papers released over the years containing keywords\\n\"Large Language Model\", \"Large Language Model+ Fine-Tuning\", and \"Large\\nLanguage Model + Alignment\".\\nPreprint submitted to Elsevier October 18, 2024\\narXiv:2307.06435v10  [cs.CL]  17 Oct 2024'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 1, 'page_label': '2', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='2019\\nT5 (Oct)\\nGPT-3 (May)\\nWebGPT (Dec)\\nOPT-IML\\nTK-Instruct (May)\\nmT0 (Dec)\\n Wizard-LM\\nVicuna\\nAlpaca (Mar)\\nHuaTuo (Apr)\\nKoala (May)\\nWizard-Coder (Jun)\\nGoat\\nPanGu-α (Apr)\\nCPM-2 (Jun)\\nGPT-NeoX-20B (Apr)\\nCodeGen (Mar) \\nGalactica (Nov)\\nGLM (Oct)\\nOPT\\nUL2 (May)\\nLLaMA (Feb)\\nLLaMA 2 (Jul)\\nMPT (Jun)\\nCodeT5+\\nCode Llama (Aug)\\nStarCoder\\nXuan Yuan 2.0 (May)\\n20202021 2022 2023 2024\\nmT5 (Oct)\\nHyperCLOVA (Sep)\\nERNIE 3.0\\nCodex (Jul)\\nJurassic-1 (Aug)\\nYuan 1.0 (Oct)\\nGopher (Dec)\\nERNIE 3.0 Titan\\nGLaM\\nLaMDA\\nT0 (Oct)\\nChatGPT (Nov)\\nSparrow (Sep)\\nFLAN-U-PaLM (Oct)\\n Bard (Oct)\\nMT-NLG (Jan)\\nAlphaCode (Feb)\\nChinchilla (Mar)\\nPaLM (Apr)\\nU-PALM (Oct)\\nBLOOM (Nov)\\nAlexaTM (Aug)\\n PaLM2 (May)\\nGPT-4\\nPanGu-Σ (Mar)\\nBloombergGPT\\nClaude\\nGemini (Dec)\\nDeepSeek (Jan)\\nLLaMA 3 \\nGrok-1 (Mar)\\nSnowflake Arctic (Apr)\\nDeepSeek-V2 (May)\\nMixtral 8x22B\\nNemotron (Feb)\\nGPT-4o (May)\\nOpenAI o1 (Sep)\\nGemini-1.5 (Feb)\\nGrok-1.5 (Apr)\\nFigure 2: Chronological display of LLM releases: blue cards represent ‘pre-trained’ models, while orange cards correspond to ‘instruction-tuned’ models. Models\\non the upper half signify open-source availability, whereas those on the bottom are closed-source. The chart illustrates the increasing trend towards instruction-tuned\\nand open-source models, highlighting the evolving landscape and trends in natural language processing research.\\nLanguage Models (LLMs) have emerged as cutting-edge arti-\\nficial intelligence systems that can process and generate text\\nwith coherent communication [4] and generalize to multiple\\ntasks [5, 6].\\nThe historical progress in natural language processing (NLP)\\nevolved from statistical to neural language modeling and then\\nfrom pre-trained language models (PLMs) to LLMs. While\\nconventional language modeling (LM) trains task-specific mod-\\nels in supervised settings, PLMs are trained in a self-supervised\\nsetting on a large corpus of text [7, 8, 9] with the aim of learning\\na generic representation that is shareable among various NLP\\ntasks. After fine-tuning for downstream tasks, PLMs surpass\\nthe performance gains of traditional language modeling (LM).\\nThe larger PLMs bring more performance gains, which has led\\nto the transitioning of PLMs to LLMs by significantly increas-\\ning model parameters (tens to hundreds of billions) [10] and\\ntraining dataset (many GBs and TBs) [10, 11]. Following this\\ndevelopment, numerous LLMs have been proposed in the lit-\\nerature [10, 11, 12, 6, 13, 14, 15]. An increasing trend in the\\nnumber of released LLMs and names of a few significant LLMs\\nproposed over the years are shown in Fig 1 and Fig 2, respec-\\ntively.\\nThe early work on LLMs, such as T5 [10] and mT5 [11] em-\\nployed transfer learning until GPT-3 [6] showed LLMs are\\nzero-shot transferable to downstream tasks without fine-tuning.\\nLLMs accurately respond to task queries when prompted with\\ntask descriptions and examples. However, pre-trained LLMs\\nfail to follow user intent and perform worse in zero-shot set-\\ntings than in few-shot. Fine-tuning them with task instruc-\\ntions data [16, 17, 18, 19] and aligning with human prefer-\\nences [20, 21] enhances generalization to unseen tasks, im-\\nproving zero-shot performance significantly and reducing mis-\\naligned behavior.\\nIn addition to better generalization and domain adaptation,\\nLLMs appear to have emergent abilities, such as reasoning,\\nplanning, decision-making, in-context learning, answering in\\nzero-shot settings, etc. These abilities are known to be ac-\\nquired by them due to their gigantic scale even when the pre-\\ntrained LLMs are not trained specifically to possess these at-\\ntributes [22, 23, 24]. Such abilities have led LLMs to be widely\\nadopted in diverse settings, including multi-modal, robotics,\\ntool manipulation, question answering, autonomous agents, etc.\\nVarious improvements have also been suggested in these areas\\neither by task-specific training [25, 26, 27, 28, 29, 30, 31] or\\nbetter prompting [32].\\nThe LLMs abilities to solve diverse tasks with human-level\\nperformance come at the cost of slow training and inference,\\nextensive hardware requirements, and higher running costs.\\nSuch requirements have limited their adoption and opened up\\nopportunities to devise better architectures [15, 33, 34, 35]\\nand training strategies [36, 37, 21, 38, 39, 40, 41]. Param-\\neter e fficient tuning [38, 41, 40], pruning [42, 43], quantiza-\\ntion [44, 45], knowledge distillation, and context length inter-\\npolation [46, 47, 48, 49] among others are some of the methods\\nwidely studied for efficient LLM utilization.\\nDue to the success of LLMs on a wide variety of tasks, the\\nresearch literature has recently experienced a large influx of\\nLLM-related contributions. Researchers have organized the\\nLLMs literature in surveys [50, 51, 52, 53], and topic-specific\\nsurveys in [54, 55, 56, 57, 58]. In contrast to these surveys, our\\ncontribution focuses on providing a comprehensive yet concise\\noverview of the general direction of LLM research. This arti-\\ncle summarizes architectural and training details of pre-trained\\nLLMs and delves deeper into the details of concepts like fine-\\ntuning, multi-modal LLMs, augmented LLMs, datasets, eval-\\nuation, applications, challenges, and others to provide a self-\\ncontained comprehensive overview. Our key contributions are\\nsummarized as follows.\\n• We present a survey on the developments in LLM research,\\nproviding a concise, comprehensive overview of the direc-\\ntion.\\n• We present extensive summaries of pre-trained models that\\ninclude fine-grained details of architecture and training de-\\ntails.\\n• We summarize major findings of the popular contributions\\nand provide a detailed discussion on the key design and\\ndevelopment aspects of LLMs to help practitioners e ffec-\\ntively leverage this technology.\\n• In this self-contained article, we cover a range of con-\\ncepts to present the general direction of LLMs compre-\\nhensively, including background, pre-training, fine-tuning,\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 2, 'page_label': '3', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='Figure 3: A broader overview of LLMs, dividing LLMs into seven branches: 1. Pre-Training 2. Fine-Tuning 3. E fficient 4. Inference 5. Evaluation 6. Applications\\n7. Challenges\\nmulti-modal LLMs, augmented LLMs, LLMs-powered\\nagents, datasets, evaluation, etc.\\nWe loosely follow the existing terminology to ensure a stan-\\ndardized outlook of this research direction. For instance, fol-\\nlowing [50], our survey discusses pre-trained LLMs with 10B\\nparameters or more. We refer the readers interested in smaller\\npre-trained models to [51, 52, 53].\\nThe organization of this paper is as follows. Section 2 discusses\\nthe background of LLMs. Section 3 focuses on LLMs overview,\\narchitectures, training pipelines and strategies, fine-tuning, and\\nutilization in different domains. Section 4 highlights the config-\\nuration and parameters that play a crucial role in the function-\\ning of these models. Summary and discussions are presented\\nin section 3.8. The LLM training and evaluation, datasets, and\\nbenchmarks are discussed in section 5, followed by challenges\\nand future directions, and conclusion in sections 7 and 8, re-\\nspectively.\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 3, 'page_label': '4', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='2. Background\\nWe provide the relevant background to understand the fun-\\ndamentals related to LLMs in this section. We briefly discuss\\nnecessary components in LLMs and refer the readers interested\\nin details to the original works.\\n2.1. Tokenization\\nTokenization [59] is an essential pre-processing step in\\nLLM training that parses the text into non-decomposing units\\ncalled tokens. Tokens can be characters, subwords [60], sym-\\nbols [61], or words, depending on the tokenization process.\\nSome of the commonly used tokenization schemes in LLMs\\ninclude wordpiece [62], byte pair encoding (BPE) [61], and un-\\nigramLM [60]. Readers are encouraged to refer to [63] for a\\ndetailed survey.\\n2.2. Encoding Positions\\nThe transformer processes input sequences in parallel and\\nindependently of each other. Moreover, the attention mod-\\nule in the transformer does not capture positional information.\\nAs a result, positional encodings were introduced in trans-\\nformer [64], where a positional embedding vector is added to\\nthe token embedding. Variants of positional embedding include\\nabsolute, relative, or learned positional encodings. Within rel-\\native encoding, Alibi and RoPE are two widely used positional\\nembeddings in LLMs.\\nAlibi [65]: It subtracts a scalar bias from the attention score\\nthat increases with the distance between token positions. This\\nfavors using recent tokens for attention.\\nRoPE [66]: It rotates query and key representations at an an-\\ngle proportional to the token absolute position in the input\\nsequence, resulting in a relative positional encoding scheme\\nwhich decays with the distance between the tokens.\\n2.3. Attention in LLMs\\nAttention assigns weights to input tokens based on impor-\\ntance so that the model gives more emphasis to relevant tokens.\\nAttention in transformers [64] calculates query, key, and value\\nmappings for input sequences, where the attention score is\\nobtained by multiplying the query and key, and later used to\\nweight values. We discuss different attention strategies used in\\nLLMs below.\\nSelf-Attention [64]: Calculates attention using queries, keys,\\nand values from the same block (encoder or decoder).\\nCross Attention: It is used in encoder-decoder architectures,\\nwhere encoder outputs are the queries, and key-value pairs\\ncome from the decoder.\\nSparse Attention [67]: Self-attention has O(n2) time complex-\\nity which becomes infeasible for large sequences. To speed\\nup the computation, sparse attention [67] iteratively calculates\\nattention in sliding windows for speed gains.\\nFlash Attention [68]: Memory access is the major bottleneck\\nin calculating attention using GPUs. To speed up, flash\\nattention employs input tiling to minimize the memory reads\\nand writes between the GPU high bandwidth memory (HBM)\\nand the on-chip SRAM.\\n2.4. Activation Functions\\nThe activation functions serve a crucial role in the curve-\\nfitting abilities of neural networks [69]. We discuss activation\\nfunctions used in LLMs in this section.\\nReLU [70]: The Rectified linear unit (ReLU) is defined as:\\nReLU(x) = max(0,x) (1)\\nGeLU [71]: The Gaussian Error Linear Unit (GeLU) is the\\ncombination of ReLU, dropout [72] and zoneout [73].\\nGLU variants [74]: The Gated Linear Unit [75] is a neural\\nnetwork layer that is an element-wise product ( ⊗) of a linear\\ntransformation and a sigmoid transformed (σ) linear projection\\nof the input given as:\\nGLU(x,W,V,b,c) = (xW + b) ⊗σ(xV + c), (2)\\nwhere X is the input of layer and l, W,b,V and c are learned\\nparameters. Other GLU variants [74] used in LLMs are:\\nReGLU(x,W,V,b,c) = max(0,xW + b)⊗,\\nGEGLU (x,W,V,b,c) = GELU (xW + b) ⊗(xV + c),\\nS wiGLU(x,W,V,b,c,β) = S wishβ(xW + b) ⊗(xV + c).\\n2.5. Layer Normalization\\nLayer normalization leads to faster convergence and is an in-\\ntegrated component of transformers [64]. In addition to Layer-\\nNorm [76] and RMSNorm [77], LLMs use pre-layer normal-\\nization [78], applying it before multi-head attention (MHA).\\nPre-norm is shown to provide training stability in LLMs. An-\\nother normalization variant, DeepNorm [79] fixes the issue with\\nlarger gradients in pre-norm.\\n2.6. Distributed LLM Training\\nThis section describes distributed LLM training approaches\\nbriefly. More details are available in [13, 37, 80, 81].\\nData Parallelism: Data parallelism replicates the model on\\nmultiple devices where data in a batch gets divided across de-\\nvices. At the end of each training iteration weights are synchro-\\nnized across all devices.\\nTensor Parallelism: Tensor parallelism shards a tensor compu-\\ntation across devices. It is also known as horizontal parallelism\\nor intra-layer model parallelism.\\nPipeline Parallelism: Pipeline parallelism shards model layers\\nacross different devices. This is also known as vertical paral-\\nlelism.\\nModel Parallelism: A combination of tensor and pipeline par-\\nallelism is known as model parallelism.\\n3D Parallelism: A combination of data, tensor, and model par-\\nallelism is known as 3D parallelism.\\nOptimizer Parallelism: Optimizer parallelism also known as\\nzero redundancy optimizer [37] implements optimizer state\\npartitioning, gradient partitioning, and parameter partitioning\\nacross devices to reduce memory consumption while keeping\\nthe communication costs as low as possible.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 4, 'page_label': '5', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='2.7. Libraries\\nSome commonly used libraries for LLMs training are:\\nTransformers [82]: The library provides access to various pre-\\ntrained transformer models with APIs to train, fine-tune, infer,\\nand develop custom models.\\nDeepSpeed [36]: A library for scalable distributed training and\\ninference of deep learning models.\\nMegatron-LM [80]: It provides GPU-optimized techniques for\\nlarge-scale training of LLMs.\\nJAX [83]: A Python library for high-performance numerical\\ncomputing and scaleable machine learning. It can di fferenti-\\nate native Python and NumPy functions and execute them on\\nGPUs.\\nColossal-AI [84]: A collection of components to write dis-\\ntributed deep learning models.\\nBMTrain [81]: A library to write e fficient stand-alone LLMs\\ntraining code.\\nFastMoE [85]: Provides API to build mixture-of-experts\\n(MoE) model in PyTorch.\\nMindSpore [86]: A deep learning training and inference frame-\\nwork extendable to mobile, edge, and cloud computing.\\nPyTorch [87]: A framework developed by Facebook AI Re-\\nsearch lab (FAIR) to build deep learning models. The main\\nfeatures of PyTorch include a dynamic computation graph and\\na pythonic coding style.\\nTensorflow [88]: A deep learning framework written by\\nGoogle. The key features of TensorFlow are graph-based com-\\nputation, eager execution, scalability, etc.\\nMXNet [89]: Apache MXNet is a deep learning framework\\nwith support to write programs in multiple languages, includ-\\ning, Python, C ++, Scala, R, etc. It also provides support for\\ndynamic and static computation graphs.\\n2.8. Data PreProcessing\\nThis section briefly summarizes data preprocessing tech-\\nniques used in LLMs training.\\nQuality Filtering: For better results, training data quality is\\nessential. Some approaches to filtering data are: 1) classifier-\\nbased and 2) heuristics-based. Classifier-based approaches\\ntrain a classifier on high-quality data and predict the quality of\\ntext for filtering, whereas heuristics-based employ some rules\\nfor filtering like language, metrics, statistics, and keywords.\\nData Deduplication: Duplicated data can a ffect model per-\\nformance and increase data memorization; therefore, to train\\nLLMs, data deduplication is one of the preprocessing steps.\\nThis can be performed at multiple levels, like sentences,\\ndocuments, and datasets.\\nPrivacy Reduction: Most of the training data for LLMs is\\ncollected through web sources. This data contains private\\ninformation; therefore, many LLMs employ heuristics-based\\nmethods to filter information such as names, addresses, and\\nphone numbers to avoid learning personal information.\\n2.9. Architectures\\nHere we discuss the variants of the transformer architectures\\nused in LLMs. The di fference arises due to the application of\\nFigure 4: An example of attention patterns in language models, image is taken\\nfrom [93].\\nFigure 5: An example of language model training objectives, image from [93].\\nthe attention and the connection of transformer blocks. An il-\\nlustration of attention patterns of these architectures is shown\\nin Figure 4.\\nEncoder Decoder: This architecture processes inputs through\\nthe encoder and passes the intermediate representation to the\\ndecoder to generate the output. Here, the encoder sees the\\ncomplete sequence utilizing self-attention whereas the decoder\\nprocesses the sequence one after the other with implementing\\ncross-attention.\\nCausal Decoder: A type of architecture that does not have an\\nencoder and processes and generates output using a decoder,\\nwhere the predicted token depends only on the previous time\\nsteps.\\nPrefix Decoder: It is also known as a non-causal decoder,\\nwhere the attention calculation is not strictly dependent on the\\npast information and the attention is bidirectional. An example\\nof a non-causal attention mask is shown in Figure 4.\\nMixture-of-Experts: It is a variant of transformer architecture\\nwith parallel independent experts and a router to route tokens\\nto experts. These experts are feed-forward layers after the at-\\ntention block [90]. Mixture-of-Experts (MoE) is an e fficient\\nsparse architecture that offers comparable performance to dense\\nmodels and allows increasing the model size without increas-\\ning the computational cost by activating only a few experts at a\\ntime [91, 92].\\n2.10. Pre-Training Objectives\\nThis section describes LLMs pre-training objectives. For\\nmore details see the paper [93].\\nFull Language Modeling: An autoregressive language model-\\ning objective where the model is asked to predict future tokens\\ngiven the previous tokens, an example is shown in Figure 5.\\nPrefix Language Modeling: A non-causal training objective,\\nwhere a prefix is chosen randomly and only remaining target\\ntokens are used to calculate the loss. An example is shown in\\nFigure 5.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 5, 'page_label': '6', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='Figure 6: A basic flow diagram depicting various stages of LLMs from pre-training to prompting /utilization. Prompting LLMs to generate responses is possible at\\ndifferent training stages like pre-training, instruction-tuning, or alignment tuning. “RL” stands for reinforcement learning, “RM” represents reward-modeling, and\\n“RLHF” represents reinforcement learning with human feedback.\\nMasked Language Modeling: In this training objective, tokens\\nor spans (a sequence of tokens) are masked randomly and the\\nmodel is asked to predict masked tokens given the past and\\nfuture context. An example is shown in Figure 5.\\nUnified Language Modeling: Unified language modeling [94]\\nis a combination of causal, non-causal, and masked language\\ntraining objectives. Here in masked language modeling, the\\nattention is not bidirectional but unidirectional, attending either\\nleft-to-right or right-to-left context.\\n2.11. LLMs Scaling Laws\\nScaling laws study the optimal combination of model param-\\neters, dataset size, and computational resources that predict the\\nimprovement in the model performance. It has been shown\\nthat the loss scales according to the power-law with model size,\\ndataset size, and compute resources [95]. This study suggests\\nlarger models are more important than big data for better perfor-\\nmance. Another variant of scaling law [96] suggests the model\\nsize and the number of training tokens should be scaled equally.\\n2.12. LLMs Adaptation Stages\\nThis section discusses the fundamentals of LLMs adaptation\\nstages, from pre-training to fine-tuning for downstream tasks\\nand utilization. An example of di fferent training stages and in-\\nference in LLMs is shown in Figure 6. In this paper, we refer\\nto alignment-tuning as aligning with human preferences, while\\noccasionally the literature uses the term alignment for different\\npurposes.\\n2.12.1. Pre-Training\\nIn the very first stage, the model is trained in a self-\\nsupervised manner on a large corpus to predict the next to-\\nkens given the input. The design choices of LLMs vary from\\nencoder-decoder to decoder-only architectures with di fferent\\nbuilding blocks and loss functions in sections 2.5, 2.4, 2.10.\\n2.12.2. Fine-Tuning\\nThere are different styles to fine-tune an LLM. This section\\nbriefly discusses fine-tuning approaches.\\nTransfer Learning: The pre-trained LLMs perform well for\\nvarious tasks [6, 15]. However, to improve the performance for\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 6, 'page_label': '7', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='a downstream task, pre-trained models are fine-tuned with the\\ntask-specific data [10, 11], known as transfer learning.\\nInstruction-tuning: To enable a model to respond to user\\nqueries effectively, the pre-trained model is fine-tuned on in-\\nstruction formatted data i.e., instruction and an input-output\\npair. Instructions generally comprise multi-task data in plain\\nnatural language, guiding the model to respond according to the\\nprompt and the input. This type of fine-tuning improves zero-\\nshot generalization and downstream task performance. Details\\non formatting instruction data and its various styles are avail-\\nable in [16, 50, 97].\\nAlignment-tuning: LLMs are prone to generating false, biased,\\nand harmful text. To make them helpful, honest, and harmless,\\nmodels are aligned using human feedback. Alignment involves\\nasking LLMs to generate unexpected responses and then updat-\\ning their parameters to avoid such responses [20, 21, 98].\\nIt ensures LLMs operate according to human intentions and\\nvalues. A model is defined to be an “aligned” model if the\\nmodel fulfills three criteria of helpful, honest, and harmless or\\n“HHH” [99].\\nResearchers employ reinforcement learning with human feed-\\nback (RLHF) [100] for model alignment. In RLHF, a fine-tuned\\nmodel on demonstrations is further trained with reward model-\\ning (RM) and reinforcement learning (RL), shown in Figure 6.\\nBelow we briefly discuss RM and RL pipelines in RLHF.\\nReward modeling: trains a model to rank generated responses\\naccording to human preferences using a classification objec-\\ntive. To train the classifier humans annotate LLMs generated\\nresponses based on the HHH criteria.\\nReinforcement learning: in combination with the reward model\\nis used for alignment in the next stage. The previously trained\\nreward model ranks LLM-generated responses into preferred\\nvs. non-preferred, which is used to align the model with proxi-\\nmal policy optimization (PPO). This process repeats iteratively\\nuntil convergence.\\n2.12.3. Prompting /Utilization\\nPrompting is a method to query trained LLMs for generating\\nresponses, as illustrated in Figure 6. LLMs can be prompted in\\nvarious prompt setups, where they can be adapted to the instruc-\\ntions without fine-tuning and in other cases with fine-tuning on\\ndata containing different prompt styles [16, 101, 102]. A good\\nguide on prompt engineering is available at [32]. Below, we\\nwill discuss various widely used prompt setups.\\nZero-Shot Prompting: LLMs are zero-shot learners and ca-\\npable of answering queries never seen before. This style of\\nprompting requires LLMs to answer user questions without see-\\ning any examples in the prompt.\\nIn-context Learning: Also known as few-shot learning, here,\\nmultiple input-output demonstration pairs are shown to the\\nmodel to generate the desired response. This adaptation style\\nis also called few-shot learning. A discussion on formatting in-\\ncontext learning (ICL) templates is available in [54, 50, 18, 16].\\nReasoning in LLMs: LLMs are zero-shot reasoners and can\\nbe provoked to generate answers to logical problems, task\\nplanning, critical thinking, etc. with reasoning. Generating\\nreasons is possible only by using di fferent prompting styles,\\nwhereas to improve LLMs further on reasoning tasks many\\nmethods [16, 97] train them on reasoning datasets. We discuss\\nvarious prompting techniques for reasoning below.\\nChain-of-Thought (CoT): A special case of prompting where\\ndemonstrations contain reasoning information aggregated with\\ninputs and outputs so that the model generates outcomes with\\nstep-by-step reasoning. More details on CoT prompts are avail-\\nable in [55, 103, 101].\\nSelf-Consistency: Improves CoT performance by generat-\\ning multiple responses and selecting the most frequent an-\\nswer [104].\\nTree-of-Thought (ToT): Explores multiple reasoning paths\\nwith possibilities to look ahead and backtrack for problem-\\nsolving [105].\\nSingle-Turn Instructions: In this prompting setup, LLMs are\\nqueried only once with all the relevant information in the\\nprompt. LLMs generate responses by understanding the con-\\ntext either in a zero-shot or few-shot setting.\\nMulti-Turn Instructions: Solving a complex task requires mul-\\ntiple interactions with LLMs, where feedback and responses\\nfrom the other tools are given as input to the LLM for the next\\nrounds. This style of using LLMs in the loop is common in\\nautonomous agents.\\n3. Large Language Models\\nThis section reviews LLMs, briefly describing their architec-\\ntures, training objectives, pipelines, datasets, and fine-tuning\\ndetails.\\n3.1. Pre-Trained LLMs\\nHere, we provide summaries of various well-known pre-\\ntrained LLMs with significant discoveries, changing the course\\nof research and development in NLP. These LLMs have consid-\\nerably improved the performance in NLU and NLG domains,\\nand are widely fine-tuned for downstream tasks. Moreover, We\\nalso identify key findings and insights of pre-trained LLMs in\\nTable 1 and 2 that improve their performance.\\n3.1.1. General Purpose\\nT5 [10]: An encoder-decoder model employing a unified text-\\nto-text training for all NLP problems is shown in Figure 7. T5\\nplaces layer normalization outside the residual path in a conven-\\ntional transformer model [64]. It uses masked language mod-\\neling as a pre-training objective where spans (consecutive to-\\nkens) are replaced with a single mask instead of separate masks\\nfor each token. This type of masking speeds up the training as\\nit produces shorter sequences. After pre-training, the model is\\nfine-tuned using adapter layers [106] for downstream tasks.\\nGPT-3 [6]: The GPT-3 architecture is the same as the GPT-\\n2 [5] but with dense and sparse attention in transformer layers\\nsimilar to the Sparse Transformer [67]. It shows that large mod-\\nels can train on larger batch sizes with a lower learning rate to\\ndecide the batch size during training, GPT-3 uses the gradient\\nnoise scale as in [107]. Overall, GPT-3 increases model param-\\neters to 175B showing that the performance of large language\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 7, 'page_label': '8', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='Figure 7: Unified text-to-text training example, source image from [10].\\nFigure 8: The image is the article of [108], showing an example of PanGu- α\\narchitecture.\\nmodels improves with the scale and is competitive with the fine-\\ntuned models.\\nmT5 [11]: A multilingual T5 model [10] trained on the mC4\\ndataset with 101 languages. The dataset is extracted from the\\npublic common crawl scrape. The model uses a larger vocab-\\nulary size of 250,000 to cover multiple languages. To avoid\\nover-fitting or under-fitting for a language, mT5 employs a data\\nsampling procedure to select samples from all languages. The\\npaper suggests using a small amount of pre-training datasets,\\nincluding all languages when fine-tuning for a task using En-\\nglish language data. This allows the model to generate correct\\nnon-English outputs.\\nPanGu-α [108]: An autoregressive model that has a query\\nlayer at the end of standard transformer layers, example shown\\nin Figure 8, to predict the next token. Its structure is similar to\\nthe transformer layer but with an additional embedding for the\\nnext position in the attention mechanism, given in Eq. 3.\\na = pnWq\\nh Wk\\nh T HT\\nL (3)\\nCPM-2 [12]: Cost-efficient Pre-trained language Models\\n(CPM-2) pre-trains bilingual (English and Chinese) 11B and\\n198B mixture-of-experts (MoE) models on the WuDaoCor-\\npus [109] dataset. The tokenization process removes “_” white\\nspace tokens in the sentencepiece tokenizer. The models are\\ntrained with knowledge inheritance, starting with only the Chi-\\nnese language in the first stage and then adding English and\\nChinese data. This trained model gets duplicated multiple times\\nto initialize the 198B MoE model. Moreover, to use the model\\nfor downstream tasks, CPM-2 experimented with both com-\\nplete fine-tuning and prompt fine-tuning as in [40] where only\\nprompt-related parameters are updated by inserting prompts at\\nvarious positions, front, middle, and back. CPM-2 also pro-\\nposes the INFMOE, a memory-efficient framework with a strat-\\negy to dynamically offload parameters to the CPU for inference\\nat a 100B scale. It overlaps data movement with inference com-\\nputation for lower inference time.\\nERNIE 3.0 [110]: ERNIE 3.0 takes inspiration from multi-\\ntask learning to build a modular architecture using Transformer-\\nXL [111] as the backbone. The universal representation mod-\\nule is shared by all the tasks, which serve as the basic block\\nfor task-specific representation modules, which are all trained\\njointly for natural language understanding, natural language\\ngeneration, and knowledge extraction. This LLM is primar-\\nily focused on the Chinese language. It claims to train on the\\nlargest Chinese text corpora for LLM training, and achieved\\nstate-of-the-art in 54 Chinese NLP tasks.\\nJurassic-1 [112]: A pair of auto-regressive language mod-\\nels, including a 7B-parameter J1-Large model and a 178B-\\nparameter J1-Jumbo model. The training vocabulary of\\nJurassic-1 comprise word pieces, complete words, and multi-\\nword expressions without any word boundaries, where possible\\nout-of-vocabulary instances are interpreted as Unicode bytes.\\nCompared to the GPT-3 counterparts, the Jurassic-1 models\\napply a more balanced depth-to-width self-attention architec-\\nture [113] and an improved tokenizer for a faster prediction\\nbased on broader resources, achieving a comparable perfor-\\nmance in zero-shot learning tasks and a superior performance in\\nfew-shot learning tasks given the ability to feed more examples\\nas a prompt.\\nHyperCLOVA [114]: A Korean language model with GPT-3\\narchitecture.\\nYuan 1.0 [115]: Trained on a Chinese corpus with 5TB of\\nhigh-quality text collected from the Internet. A Massive Data\\nFiltering System (MDFS) built on Spark is developed to pro-\\ncess the raw data via coarse and fine filtering techniques. To\\nspeed up the training of Yuan 1.0 to save energy expenses and\\ncarbon emissions, various factors that improve the performance\\nof distributed training are incorporated in architecture and train-\\ning: like increasing the hidden state size improves pipeline and\\ntensor parallelism performance, larger micro batches improve\\npipeline parallelism performance, and larger global batch size\\nimprove data parallelism performance. In practice, the Yuan 1.0\\nmodel performs well on text classification, Winograd Schema,\\nnatural language inference, and reading comprehension tasks.\\nGopher [116]: The Gopher family of models ranges from\\n44M to 280B parameters in size to study the e ffect of scale\\non the LLMs performance. The 280B model beats GPT-3 [6],\\nJurrasic-1 [112], MT-NLG [117], and others on 81% of the\\nevaluated tasks.\\nERNIE 3.0 TITAN [35]:ERNIE 3.0 Titan extends ERNIE 3.0\\nby training a larger model with 26x the number of parameters\\nof the latter. This bigger model outperformed other state-of-the-\\nart models in 68 NLP tasks. LLMs produce text with incorrect\\nfacts. In order to have control of the generated text with fac-\\ntual consistency, ERNIE 3.0 Titan adds another task, Credible\\nand Controllable Generations, to its multi-task learning setup.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 8, 'page_label': '9', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='It introduces additional self-supervised adversarial and control-\\nlable language modeling losses to the pre-training step, which\\nenables ERNIE 3.0 Titan to beat other LLMs in their manually\\nselected Factual QA task set evaluations.\\nGPT-NeoX-20B [118]: An auto-regressive model that largely\\nfollows GPT-3 with a few deviations in architecture design,\\ntrained on the Pile dataset without any data deduplication. GPT-\\nNeoX has parallel attention and feed-forward layers in a trans-\\nformer block, given in Eq. 4, that increases throughput by 15%.\\nIt uses rotary positional embedding [66], applying it to only\\n25% of embedding vector dimension as in [119]. This reduces\\nthe computation without performance degradation. As opposed\\nto GPT-3, which uses dense and sparse layers, GPT-NeoX-20B\\nuses only dense layers. The hyperparameter tuning at this scale\\nis difficult; therefore, the model chooses hyperparameters from\\nthe method [6] and interpolates values between 13B and 175B\\nmodels for the 20B model. The model training is distributed\\namong GPUs using both tensor and pipeline parallelism.\\nx + Attn(LN1(x)) + FF (LN2(x)) (4)\\nOPT [14]: It is a clone of GPT-3, developed to open-source\\na model that replicates GPT-3 performance. Training of OPT\\nemploys dynamic loss scaling [120] and restarts from an earlier\\ncheckpoint with a lower learning rate whenever loss divergence\\nis observed. Overall, the performance of OPT-175B models is\\ncomparable to the GPT3-175B model.\\nBLOOM [13]: A causal decoder model trained on the ROOTS\\ncorpus to open-source an LLM. The architecture of BLOOM is\\nshown in Figure 9, with di fferences like ALiBi positional em-\\nbedding, an additional normalization layer after the embedding\\nlayer as suggested by the bitsandbytes 1 library. These changes\\nstabilize training with improved downstream performance.\\nGLaM [91]: Generalist Language Model (GLaM) represents a\\nfamily of language models using a sparsely activated decoder-\\nonly mixture-of-experts (MoE) structure [121, 90]. To gain\\nmore model capacity while reducing computation, the experts\\nare sparsely activated where only the best two experts are used\\nto process each input token. The largest GLaM model, GLaM\\n(64B/64E), is about 7×larger than GPT-3 [6], while only part of\\nthe parameters are activated per input token. The largest GLaM\\n(64B/64E) model achieves better overall results as compared\\nto GPT-3 while consuming only one-third of GPT-3’s training\\nenergy.\\nMT-NLG [117]: A 530B causal decoder based on the GPT-\\n2 architecture that has roughly 3 ×GPT-3 model parameters.\\nMT-NLG is trained on filtered high-quality data collected from\\nvarious public datasets and blends various types of datasets in a\\nsingle batch, which beats GPT-3 on several evaluations.\\nChinchilla [96]: A causal decoder trained on the same dataset\\nas the Gopher [116] but with a little di fferent data sampling\\ndistribution (sampled from MassiveText). The model architec-\\nture is similar to the one used for Gopher, with the exception of\\nAdamW optimizer instead of Adam. Chinchilla identifies the\\n1https://github.com/TimDettmers/bitsandbytes\\nFigure 9: The BLOOM architecture example sourced from [13].\\nrelationship that model size should be doubled for every dou-\\nbling of training tokens. Over 400 language models ranging\\nfrom 70 million to over 16 billion parameters on 5 to 500 bil-\\nlion tokens are trained to get the estimates for compute-optimal\\ntraining under a given budget. The authors train a 70B model\\nwith the same compute budget as Gopher (280B) but with 4\\ntimes more data. It outperforms Gopher [116], GPT-3 [6], and\\nothers on various downstream tasks, after fine-tuning.\\nAlexaTM [122]: An encoder-decoder model, where encoder\\nweights and decoder embeddings are initialized with a pre-\\ntrained encoder to speed up training. The encoder stays frozen\\nfor the initial 100k steps and is later unfrozen for end-to-end\\ntraining. The model is trained on a combination of denoising\\nand causal language modeling (CLM) objectives, concatenat-\\ning a [CLM ] token at the beginning for mode switching. Dur-\\ning training, the CLM task is applied for 20% of the time, which\\nimproves the in-context learning performance.\\nPaLM [15]: A causal decoder with parallel attention and\\nfeed-forward layers similar to Eq. 4, speeding up training by\\na factor of 15. Additional changes to the conventional trans-\\nformer model include SwiGLU activation, RoPE embeddings,\\nmulti-query attention that saves computation cost during decod-\\ning, and shared input-output embeddings. During training, loss\\nspiking was observed, and to fix it, model training was restarted\\nfrom a 100-step earlier checkpoint by skipping 200-500 batches\\naround the spike. Moreover, the model was found to memo-\\nrize around 2.4% of the training data at the 540B model scale,\\nwhereas this number was lower for smaller models.\\nPaLM-2 [123]: A smaller multi-lingual variant of PaLM,\\ntrained for larger iterations on a better quality dataset. PaLM-\\n2 shows significant improvements over PaLM, while reducing\\ntraining and inference costs due to its smaller size. To lessen\\ntoxicity and memorization, it appends special tokens with a\\nfraction of pre-training data, which shows a reduction in gener-\\nating harmful responses.\\nU-PaLM [124]: This method trains PaLM for 0.1% addi-\\ntional compute with the UL2 (also named as UL2Restore) ob-\\njective [125], using the same dataset it outperforms the baseline\\nsignificantly on various NLP tasks, including zero-shot, few-\\nshot, commonsense reasoning, CoT, etc. Training with UL2R\\ninvolves converting a causal decoder PaLM to a non-causal de-\\ncoder PaLM and employing 50% sequential denoising, 25%\\nregular denoising, and 25% extreme denoising loss functions.\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 9, 'page_label': '10', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='UL2 [125]: An encoder-decoder architecture trained using a\\nmixture of denoisers (MoD) objective. Denoisers include 1)\\nR-Denoiser: a regular span masking, 2) S-Denoiser: which cor-\\nrupts consecutive tokens of a large sequence and 3) X-Denoiser:\\nwhich corrupts a large number of tokens randomly. During pre-\\ntraining, UL2 includes a denoiser token from R,S,X to rep-\\nresent a denoising setup. It helps improve fine-tuning perfor-\\nmance for downstream tasks that bind the task to one of the up-\\nstream training modes. This MoD style of training outperforms\\nthe T5 model on many benchmarks.\\nGLM-130B [33]: GLM-130B is a bilingual (English and Chi-\\nnese) model trained using an auto-regressive mask infilling pre-\\ntraining objective similar to the GLM [126]. This training style\\nmakes the model bidirectional as compared to GPT-3, which is\\nunidirectional. As opposed to GLM, the training of GLM-130B\\nincludes a small amount of multi-task instruction pre-training\\ndata (5% of the total data) along with self-supervised mask in-\\nfilling. To stabilize the training, it applies embedding layer gra-\\ndient shrink.\\nLLaMA [127, 21]: A set of decoder-only language models\\nvarying from 7B to 70B parameters. LLaMA models series is\\nthe most famous among the community for parameter efficiency\\nand instruction tuning.\\nLLaMA-1 [127]: Implements efficient causal attention [128]\\nby not storing and computing masked attention weights and\\nkey/query scores. Another optimization is reducing the number\\nof activations recomputed in the backward pass, as in [129].\\nLLaMA-2 [21]: This work is more focused on fine-tuning a\\nsafer and better LLaMA-2-Chat model for dialogue generation.\\nThe pre-trained model has 40% more training data with a larger\\ncontext length and grouped-query attention.\\nLLaMA-3/3.1 [130]: A collection of models trained on a\\nseven times larger dataset as compared to LLaMA-2 with dou-\\nble the context length, outperforming its previous variants and\\nother models.\\nPanGu-Σ [92]: An autoregressive model with parameters\\ncopied from PanGu-αand extended to a trillion scale with Ran-\\ndom Routed Experts (RRE), the architectural diagram is shown\\nin Figure 10. RRE is similar to the MoE architecture, with\\ndistinctions at the second level, where tokens are randomly\\nrouted to experts in a domain instead of using a learnable gat-\\ning method. The model has bottom layers densely activated and\\nshared across all domains, whereas top layers are sparsely ac-\\ntivated according to the domain. This training style allows for\\nextracting task-specific models and reduces catastrophic forget-\\nting effects in the case of continual learning.\\nMixtral8x22b [131]: A mixture-of-experts (MoE) model with\\neight distinct experts routes each token to two experts at each\\nlayer and combines the outputs additively.\\nSnowflake Arctic [132]: Arctic LLM is a hybrid of dense and\\nmixture-of-experts (MoE) architecture. The MoE (128 ×3.66B\\nMLP experts) is parallel to the dense transformer (10B) with\\nonly two experts activated. The model has many experts, com-\\npared to other MoE LLMs [131, 133], to increase the model\\ncapacity and provide an opportunity to choose among many ex-\\nperts for a diverse configuration. The model has 480B param-\\neters, and only 17B are active during a forward pass, reducing\\nthe computation significantly.\\nGrok [133, 134]: Grok is a family of LLMs including Grok-1\\nand Grok-1.5, released by XAI.\\nGrok-1 [133]: Grok-1 is a 314B parameters language MoE\\nmodel (eight experts), where two experts are activated per to-\\nken.\\nGrok-1.5 [134]: Grok-1.5 is a multi-modal LLM with a larger\\ncontext length and improved performance.\\nGemini [135, 136]: Gemini replaces Bard (based on PaLM)\\nwith multi-modal capabilities and significant language model-\\ning performance improvements.\\nGemini-1 [135]: The first-ever auto-regressive model to\\nachieve human-level capabilities on the MMLU benchmark.\\nGemini-1.5 [136]: A multi-modal LLM with MoE architec-\\nture builds on the findings of Gemini-1. The model has a 2M\\ncontext window and can reason over information up to 10M\\ntokens. Such large context windows were never achieved pre-\\nviously and shown to have a huge impact on performance gain.\\nNemotron-4 340B [137]: A decoder-only model that has been\\naligned on 98% synthetic data and only 2% manually annotated\\ndata. Utilizing synthetic data at a large proportion improves the\\nmodel performance significantly. The paper suggested intro-\\nducing alignment data with a smaller subset of previously seen\\ndata during the late stage of the model pre-training, enabling the\\nsmooth transition from the pre-trained stage to the final train-\\ning stage. To train better instruction-following models, weaker\\nmodels are trained into stronger models iteratively. The syn-\\nthetic data generated by the weaker instruction-tuned model is\\nused to train a base model which is later supervised fine-tuned\\noutperforming the weaker model.\\nDeepSeek [138]: DeepSeek studies the LLMs scaling laws\\nin detail to determine the optimal non-embedding model size\\nand training data. The experiments were performed for 8 bud-\\ngets ranging from 1e 17 to 3e20 training FLOPs. Each compute\\nbudget was tested against ten different models/data scales. The\\nbatch size and learning rates were also fitted for the given com-\\npute budget finding that the batch size should increase with\\nthe increased compute budget while decreasing the learning\\nrate. Following are the equations for the optimal batch-size (B),\\nlearning rate (η), model size (M), and data (D):\\nBopt = 0.2920.C0.3271\\nηopt = 0.3118.C−0.1250\\nMopt = Mbase.Ca\\nDopt = Dbase.Cb\\nMbase = 0.1715,Dbase = 5.8316,a = 0.5243,b = 0.4757\\n(5)\\nDeepSeek-v2 [139]: An MoE model that introduces multi-\\nhead latent attention (MLA) to reduce inference costs, by com-\\npressing Key-Value (KV) cache into a latent vector. MLA\\nachieves better performance than multi-head attention (MHA),\\nand other efficient attention mechanisms such as grouped query\\nattention (GQA), multi-query attention (MQA), etc. Because\\nof MLA, DeepSeek-v2 achieves 5.76 times faster inference\\nthroughput as compared to DeepSeek [138].\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 10, 'page_label': '11', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='3.1.2. Coding\\nCodeGen [140]: CodeGen has a similar architecture to\\nPaLM [15], i.e., parallel attention, MLP layers, and RoPE em-\\nbeddings. The model is trained on both natural language and\\nprogramming language data sequentially (trained on the first\\ndataset, then the second, and so on) on the following datasets\\n1) PILE, 2) BIGQUERY , and 3) BIGPYTHON. CodeGen pro-\\nposed a multi-step approach to synthesizing code. The purpose\\nis to simplify the generation of long sequences where the previ-\\nous prompt and generated code are given as input with the next\\nprompt to generate the next code sequence. CodeGen open-\\nsource a Multi-Turn Programming Benchmark (MTPB) to eval-\\nuate multi-step program synthesis.\\nCodex [141]: This LLM is trained on a subset of public Python\\nGithub repositories to generate code from docstrings. Com-\\nputer programming is an iterative process where the programs\\nare often debugged and updated before fulfilling the require-\\nments. Similarly, Codex generates 100 versions of a program\\nby repetitive sampling for a given description, which produces\\na working solution for 77.5% of the problems passing unit tests.\\nIts powerful version powers Github Copilot2.\\nAlphaCode [142]: A set of large language models, ranging\\nfrom 300M to 41B parameters, designed for competition-level\\ncode generation tasks. It uses the multi-query attention [143] to\\nreduce memory and cache costs. Since competitive program-\\nming problems highly require deep reasoning and an under-\\nstanding of complex natural language algorithms, the Alpha-\\nCode models are pre-trained on filtered GitHub code in popular\\nlanguages and then fine-tuned on a new competitive program-\\nming dataset named CodeContests. The CodeContests dataset\\nmainly contains problems, solutions, and test cases collected\\nfrom the Codeforces platform3. The pre-training employs stan-\\ndard language modeling objectives, while GOLD [144] with\\ntempering [145] serves as the training objective for the fine-\\ntuning on CodeContests data. To evaluate the performance of\\nAlphaCode, simulated programming competitions are hosted\\non the Codeforces platform: overall, AlphaCode ranks at the\\ntop 54.3% among over 5000 competitors, where its Codeforces\\nrating is within the top 28% of recently participated users.\\nCodeT5+ [34]: CodeT5+ is based on CodeT5 [146], with\\nshallow encoder and deep decoder, trained in multiple stages\\ninitially unimodal data (code) and later bimodal data (text-code\\npairs). Each training stage has di fferent training objectives and\\nactivates different model blocks encoder, decoder, or both ac-\\ncording to the task. The unimodal pre-training includes span\\ndenoising and CLM objectives, whereas bimodal pre-training\\nobjectives contain contrastive learning, matching, and CLM for\\ntext-code pairs. CodeT5 + adds special tokens with the text to\\nenable task modes, for example, [ CLS ] for contrastive loss,\\n[Match] for text-code matching, etc.\\nStarCoder [147]: A decoder-only model with the SantaCoder\\narchitecture, employing Flash attention to scale up the context\\nlength to 8k. The StarCoder trains an encoder to filter names,\\n2https://github.com/features/copilot\\n3https://codeforces.com/\\nemails, and other personal data from the training data. Its fine-\\ntuned variant outperforms PaLM, LLaMA, and LAMDA on\\nHumanEval and MBPP benchmarks.\\n3.1.3. Scientific Knowledge\\nGalactica [148]: A large curated corpus of human scientific\\nknowledge with 48 million papers, textbooks, lecture notes,\\nmillions of compounds and proteins, scientific websites, en-\\ncyclopedias, and more are trained using the metaseq library3,\\nwhich is built on PyTorch and fairscale [149]. The model wraps\\nreasoning datasets with the <work >token to provide step-by-\\nstep reasoning context to the model, which has been shown to\\nimprove the performance on reasoning tasks.\\n3.1.4. Dialog\\nLaMDA [150]: A decoder-only model pre-trained on pub-\\nlic dialog data, public dialog utterances, and public web doc-\\numents, where more than 90% of the pre-training data is in\\nEnglish. LaMDA is trained with the objective of producing re-\\nsponses that exhibit high levels of quality, safety, and grounded-\\nness. To achieve this, discriminative and generative fine-tuning\\ntechniques are incorporated to enhance the model’s safety and\\nquality aspects. As a result, the LaMDA models can be utilized\\nas a general language model performing various tasks.\\n3.1.5. Finance\\nBloombergGPT [151]: A non-causal decoder model trained\\nusing both financial (“FINPILE” from the Bloomberg archive)\\nand general-purpose datasets. The model’s architecture is sim-\\nilar to the BLOOM [13] and OPT [14]. It allocates 50B param-\\neters to different blocks of the model using the approach [113].\\nFor e ffective training, BloombergGPT packs documents to-\\ngether with < |endo f text| > to use the maximum sequence\\nlength, uses warmup batch size starting from 1024 to 2048, and\\nmanually reduces the learning rate multiple times during the\\ntraining.\\nXuan Yuan 2.0 [152]: A Chinese financial chat model with\\nBLOOM’s [13] architecture trained on a combination of general\\npurpose, financial, general purpose instructions, and financial\\ninstitutions datasets. Xuan Yuan 2.0 combined the pre-training\\nand fine-tuning stages to avoid catastrophic forgetting.\\n3.2. Fine-Tuned LLMs\\nPre-trained LLMs have excellent generalization abilities to\\nunseen tasks. However, because they are generally trained with\\nthe objective of next token prediction, LLMs have limited ca-\\npacity to follow user intent and are prone to generate unethical,\\ntoxic or inaccurate responses [20]. For their e ffective utiliza-\\ntion, LLMs are fine-tuned to follow instructions [16, 17, 97] and\\ngenerate safe responses [20], which also results in increasing\\nzero-shot, few-shot, and cross-task generalization [97, 16, 18],\\nwith minimal compute increment, e.g., 0.2% of the total pre-\\ntraining for PaLM 540B [16].\\nWe review various fine-tuned LLMs and strategies for effective\\nfine-tuning in this section.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 11, 'page_label': '12', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='Table 1: Noteworthy findings and insights of pre-trained Large Language Models.\\nModels Findings & Insights\\nT5\\n• Encoder and decoder with shared parameters perform equivalently when parameters are not shared\\n• Fine-tuning model layers (adapter layers) work better than the conventional way of training on only\\nclassification layers\\nGPT-3\\n• Few-shot performance of LLMs is better than the zero-shot, suggesting that LLMs are meta-\\nlearners\\nmT5\\n• Large multi-lingual models perform equivalently to single language models on downstream tasks.\\nHowever, smaller multi-lingual models perform worse\\nPanGu-α • LLMs have good few shot capabilities\\nCPM-2\\n• Prompt fine-tuning requires updating very few parameters while achieving performance compara-\\nble to full model fine-tuning\\n• Prompt fine-tuning takes more time to converge as compared to full model fine-tuning\\n• Inserting prompt tokens in-between sentences can allow the model to understand relations between\\nsentences and long sequences\\n• In an analysis, CPM-2 finds that prompts work as a provider (additional context) and aggregator\\n(aggregate information with the input text) for the model\\nERNIE 3.0\\n• A modular LLM architecture with a universal representation module and task-specific representa-\\ntion module helps in the finetuning phase\\n• Optimizing the parameters of a task-specific representation network during the fine-tuning phase is\\nan efficient way to take advantage of the powerful pre-trained model\\nJurassic-1\\n• The performance of LLM is highly related to the network size\\n• To improve runtime performance, more operations can be performed in parallel (width) rather than\\nsequential (depth)\\n• To efficiently represent and fit more text in the same context length, the model uses a larger vo-\\ncabulary to train a SentencePiece tokenizer without restricting it to word boundaries. This further\\nbenefits in few-shot learning tasks\\nHyperCLOV A • By employing prompt-based tuning, the performances of models can be improved, often surpassing\\nthose of state-of-the-art models when the backward gradients of inputs are accessible\\nYuan 1.0 • The model architecture that excels in pre-training and fine-tuning cases may exhibit contrasting\\nbehavior in zero-shot and few-shot learning\\nGopher • Relative encodings enable the model to evaluate for longer sequences than training.\\nERNIE 3.0 Titan\\n• Additional self-supervised adversarial loss to distinguish between real and generated text improves\\nthe model performance as compared to ERNIE 3.0\\nGPT-NeoX-20B\\n• Parallel attention + FF layers speed-up training 15% with the same performance as with cascaded\\nlayers\\n• Initializing feed-forward output layers before residuals with scheme in [153] avoids activations\\nfrom growing with increasing depth and width\\n• Training on Pile outperforms GPT-3 on five-shot\\nTable Continued on Next Page\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 12, 'page_label': '13', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='Models Findings & Insights\\nOPT • Restart training from an earlier checkpoint with a lower learning rate if loss diverges\\n• Model is prone to generate repetitive text and stuck in a loop\\nGalactica\\n• Galactica’s performance has continued to improve across validation set, in-domain, and out-of-\\ndomain benchmarks, even with multiple repetitions of the corpus, which is superior to existing\\nresearch on LLMs\\n• A working memory token approach can achieve strong performance over existing methods on\\nmathematical MMLU and MATH benchmarks. It sets a new state-of-the-art on several downstream\\ntasks such as PubMedQA (77.6%) and MedMCQA dev (52.9%)\\nGLaM\\n• The model capacity can be maintained at reduced computation by replacing the feed-forward layer\\nin each transformer layer with a mixture-of-experts (MoE)\\n• The model trained on filtered data shows consistently better performances on both NLG and NLU\\ntasks, where the effect of filtering is more significant on the former tasks\\n• Filtered pretraining corpora play a crucial role in the generation capability of LLMs, especially for\\nthe downstream tasks\\n• The scaling of GLaM MoE models can be achieved by increasing the size or number of experts in\\nthe MoE layer. Given a fixed budget of computation, more experts contribute to a better perfor-\\nmance\\nLaMDA • The model can be fine-tuned to learn to call different external information resources and tools\\nAlphaCode\\n• For higher e ffectiveness and e fficiency, a transformer model can be asymmetrically constructed\\nwith a shallower encoder and a deeper decoder\\n• To achieve better performances, it is necessary to employ strategies such as massively scaling\\nupsampling, followed by the filtering and clustering of samples into a compact set\\n• The utilization of novel sampling-e fficient transformer architectures designed to facilitate large-\\nscale sampling is crucial\\n• Simplifying problem descriptions can effectively improve the model’s performance\\nChinchilla\\n• The model size and the number of training tokens should be scaled proportionately: for each dou-\\nbling of the model size, the number of training tokens should be doubled as well\\nPaLM\\n• English-centric models produce better translations when translating to English as compared to non-\\nEnglish\\n• Generalized models can have equivalent performance for language translation to specialized small\\nmodels\\n• Larger models have a higher percentage of training data memorization\\n• Performance has not yet saturated even at 540B scale, which means larger models are likely to\\nperform better\\nAlexaTM\\n• Encoder-decoder architecture is more suitable to train LLMs given bidirectional attention to the\\ncontext than decoder-only\\n• Causal Language Modeling (CLM) task can be added to benefit the model with efficient in-context\\nlearning\\n• Placing layer norm at the beginning of each transformer layer improves the training stability\\nTable Continued on Next Page\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 13, 'page_label': '14', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='Models Findings & Insights\\nU-PaLM\\n• Training with a mixture of denoisers outperforms PaLM when trained further for a few more FLOPs\\n• Training with a mixture of denoisers improves the infilling ability and open-ended text generation\\ndiversity\\nUL2 • Mode switching training enables better performance on downstream tasks\\n• CoT prompting outperforms standard prompting for UL2\\nGLM-130B\\n• Pre-training data with a small proportion of multi-task instruction data improves the overall model\\nperformance\\nCodeGen\\n• Multi-step prompting for code synthesis leads to a better user intent understanding and code gen-\\neration\\nLLaMA • A constant performance improvement is observed when scaling the model\\n• Smaller models can achieve good performances with more training data and computing time\\nPanGu-Σ\\n• Sparse models provide the benefits of large models at a lower computation cost\\n• Randomly Routed Experts reduces catastrophic forgetting e ffects which in turn is essential for\\ncontinual learning\\n• Randomly Routed Experts allow extracting a domain-specific sub-model in deployment which is\\ncost-efficient while maintaining a performance similar to the original\\nBloombergGPT\\n• Pre-training with general-purpose and task-specific data improves task performance without hurt-\\ning other model capabilities\\nXuanYuan 2.0 • Combining pre-training and fine-tuning stages in single training avoids catastrophic forgetting\\nCodeT5+\\n• Causal LM is crucial for a model’s generation capability in encoder-decoder architectures\\n• Multiple training objectives like span corruption, Causal LM, matching, etc complement each other\\nfor better performance\\nStarCoder • HHH prompt by Anthropic allows the model to follow instructions without fine-tuning\\nLLaMA-2\\n• Model trained on unfiltered data is more toxic but may perform better on downstream tasks after\\nfine-tuning\\n• Model trained on unfiltered data requires fewer samples for safety alignment\\nPaLM-2\\n• Data quality is important to train better models\\n• Model and data size should be scaled with 1:1 proportions\\n• Smaller models trained for larger iterations outperform larger models\\nLLaMA-3/3.1\\n• Increasing batch size gradually stabilizes the training without loss spikes\\n• High-quality data at the final stages of training improves the model performance\\n• Increasing model context length windows step-wise allows it to better adapt to various sequence\\nlengths\\nNemotron-40B\\n• Model aligned iteratively on synthetic data with data generated from the previously aligned model\\nachieves competitive performance\\nDeepSeek • Batch size should increase with the increase in compute budget while decreasing the learning rate\\nDeepSeek-v2\\n• Mult-head latent attention (MLA) performs better than multi-head attention (MHA) while requiring\\na significantly smaller KV cache, therefore achieving faster data generation\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 14, 'page_label': '15', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='Table 2: Key insights and findings from the study of instruction-tuned Large Language Models.\\nModels Findings & Insights\\nT0 • Multi-task prompting enables zero-shot generalization and outperforms baselines\\n• Even a single prompt per dataset task is enough to improve performance\\nWebGPT\\n• To aid the model in e ffectively filtering and utilizing relevant information, human labelers play a\\ncrucial role in answering questions regarding the usefulness of the retrieved documents\\n• Interacting a fine-tuned language model with a text-based web-browsing environment can improve\\nend-to-end retrieval and synthesis via imitation learning and reinforcement learning\\n• Generating answers with references can make labelers easily judge the factual accuracy of answers\\nTk-INSTRUCT\\n• Instruction tuning leads to a stronger generalization of unseen tasks\\n• More tasks improve generalization whereas only increasing task instances does not help\\n• Supervised trained models are better than generalized models\\n• Models pre-trained with instructions and examples perform well for different types of inputs\\nmT0 and BLOOMZ\\n• Instruction tuning enables zero-shot generalization to tasks never seen before\\n• Multi-lingual training leads to even better zero-shot generalization for both English and non-\\nEnglish\\n• Training on machine-translated prompts improves performance for held-out tasks with non-English\\nprompts\\n• English only fine-tuning on multilingual pre-trained language model is enough to generalize to\\nother pre-trained language tasks\\nOPT-IML\\n• Creating a batch with multiple task examples is important for better performance\\n• Only example proportional sampling is not enough, training datasets should also be proportional\\nfor better generalization/performance\\n• Fully held-out and partially supervised tasks performance improves by scaling tasks or categories\\nwhereas fully supervised tasks have no effect\\n• Including small amounts i.e. 5% of pretraining data during fine-tuning is effective\\n• Only 1% reasoning data improves the performance, adding more deteriorates performance\\n• Adding dialogue data makes the performance worse\\nSparrow\\n• Labelers’ judgment and well-defined alignment rules help the model generate better responses\\n• Good dialogue goals can be broken down into detailed natural language rules for the agent and the\\nraters\\n• The combination of reinforcement learning (RL) with reranking yields optimal performance in\\nterms of preference win rates and resilience against adversarial probing\\nFlan\\n• Finetuning with CoT improves performance on held-out tasks\\n• Fine-tuning along with CoT data improves reasoning abilities\\n• CoT tuning improves zero-shot reasoning\\n• Performance improves with more tasks\\n• Instruction fine-tuning improves usability which otherwise is challenging for pre-trained models\\n• Improving the model’s performance with instruction tuning is compute-efficient\\n• Multitask prompting enables zero-shot generalization abilities in LLM\\nWizardCoder • Fine-tuning with re-written instruction-tuning data into a complex set improves performance\\nLLaMA-2-Chat\\n• Model learns to write safe responses with fine-tuning on safe demonstrations, while additional\\nRLHF step further improves model safety and make it less prone to jailbreak attacks\\nLIMA • Less high quality data is enough for fine-tuned model generalization\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 15, 'page_label': '16', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='Figure 10: This example illustrates the PanGu- P architecture, as depicted in\\nthe image sourced from [92].\\n3.2.1. Instruction-Tuning with Manually Created Datasets\\nNumerous hand-crafted instruction-tuning datasets with\\ndifferent design choices are proposed in the literature to\\ninstruction-tune LLMs. The performance of fine-tuned LLMs\\ndepends on multiple factors, such as dataset, instruction diver-\\nsity, prompting templates, model size, and training objectives.\\nKeeping this in view, diverse fine-tuned models have emerged\\nin the literature using manually created datasets.\\nThe models T0 [17] and mT0 (multi-lingual) [154] employ\\ntemplates to convert existing datasets into prompt datasets.\\nThey have shown improvements in generalization to zero-shot\\nand held-out tasks. Tk-Instruct [18] fine-tuned the T5 model\\nwith in-context instructions to study generalization on unseen\\ntasks when given in-context instructions during test time. The\\nmodel outperformed Instruct-GPT, despite being smaller in\\nsize, i.e., 11B parameters as compared to 175B of GPT-3.\\nIncreasing Tasks and Prompt Setups: Zero-shot and few-shot\\nperformance improves significantly by expanding task collec-\\ntion and prompt styles. OPT-IML [97] and Flan [16] curated\\nlarger 2k and 1.8k task datasets, respectively. While increasing\\ntask size alone is not enough, OPT-IML and Flan add more\\nprompting setups in their datasets, zero-shot, few-shot, and\\nCoT. In continuation, CoT Collection [101] fine-tunes Flan-T5\\nfurther on 1.88M CoT samples. Another method [102] uses\\nsymbolic tasks with tasks in T0, Flan, etc.\\n3.2.2. Instruction-Tuning with LLMs Generated Datasets\\nGenerating an instruction-tuning dataset requires carefully\\nwriting instructions and input-output pairs, which are often\\nwritten by humans, smaller in size, and less diverse. To over-\\ncome this, self-instruct [19] proposed an approach to prompt\\navailable LLMs to generate instruction-tuning datasets. Self-\\ninstruct outperformed models trained on manually created\\ndataset SUPER-NATURALINSTRUCTIONS (a dataset with\\n1600+ tasks) [18] by 33%. It starts with a seed of 175 tasks, 1\\ninstruction, and 1 sample per task and iteratively generates new\\ninstructions (52k) and instances (82k input-output pairs) using\\nFigure 11: An example image shows an instance of the Flan training paradigm,\\ntaken from [16].\\nGPT-3 [6]. Contrary to this, Dynosaur [155] uses the meta-data\\nof datasets on Huggingface to prompt LLMs to generate multi-\\nple task instruction-tuning datasets.\\nLLaMA Tuned: Various models in the literature instruction-\\ntune LLaMA [156] with GPT-3 [6] or GPT-4 [157] gener-\\nated datasets. Among these, Alpaca [158], Vicuna [159],\\nand LLaMA-GPT-4 [160] are a few general-purpose fine-tuned\\nmodels, where Alpaca is trained on 52k samples from text-\\ndavinci-003, Vicuna on 70k samples from ShareGPT.com, and\\nLLaMA-GPT-4 by re-creating Alpaca instructions from GPT-\\n4. Goat [161] fine-tunes LLaMA for arithmetic tasks (1 million\\nsamples) by generating data from ChatGPT and outperforms\\nGPT-4, PaLM, BLOOM, OPT, etc., attributing its success to the\\nLLaMA’s consistent tokenization of numbers. HuaTuo [162] is\\na medical knowledge model, fine-tuned with a generated QA\\ndataset of 8k instructions.\\nComplex Instructions: Evol-Instruct [163, 164] prompts LLMs\\nto convert given instructions into a more complex set. The in-\\nstructions are iteratively evolved with re-writing instructions in\\ncomplex wording and creating new instructions. With this style\\nof automated instruction generation, WizardLM [163] (fine-\\ntuned LLaMA on 250k instructions), outperforms Vicuna and\\nAlpaca, and WizardCoder [164] (fine-tuned StarCoder) beats\\nClaude-Plus, Bard, and others.\\n3.2.3. Aligning with Human Preferences\\nIncorporating human preferences into LLMs presents a\\nsignificant advantage in mitigating undesirable behaviors and\\nensuring accurate outputs. The initial work on alignment, such\\nas InstructGPT [20] aligns GPT-3 using a 3-step approach,\\ninstruction-tuning, reward modeling, and fine-tuning with\\nreinforcement learning (RL). The supervised fine-tuned GPT-3\\non demonstrations is queried to generate responses, which\\nhuman labelers rank according to human values, and a reward\\nmodel is trained on the ranked data. Lastly, the GPT-3 is trained\\nwith proximal policy optimization (PPO) using rewards on the\\ngenerated data from the reward model. LLaMA 2-Chat [21]\\nimproves alignment by dividing reward modeling into help-\\nfulness and safety rewards and using rejection sampling in\\naddition to PPO. The initial four versions of LLaMA 2-Chat\\nare fine-tuned with rejection sampling and then with PPO on\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 16, 'page_label': '17', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='top of rejection sampling.\\nAligning with Supported Evidence: This style of alignment\\nallows the model to generate responses with proofs and facts,\\nreduces hallucination, and assists humans more e ffectively,\\nwhich increases trust in the model’s output. Similar to\\nthe RLHF training style, a reward model is trained to rank\\ngenerated responses containing web citations in answers\\nto questions, which is later used to train the model, as in\\nGopherCite [165], WebGPT [166], and Sparrow [167]. The\\nranking model in Sparrow [167] is divided into two branches,\\npreference reward and rule reward, where human annotators\\nadversarial probe the model to break a rule. These two rewards\\ntogether rank a response to train with RL.\\nAligning Directly with SFT: The PPO in the RLHF pipeline\\nis complex, memory-intensive, and unstable, requiring mul-\\ntiple models, reward, value, policy, and reference models.\\nAvoiding this sophisticated alignment pipeline is possible by\\nincorporating minimal changes in the supervised fine-tuning\\n(SFT) pipeline as in [168, 169, 170], with better or compa-\\nrable performance to PPO. Direct preference optimization\\n(DPO) [168] trains a model directly on the human-preferred\\nresponses to maximize the likelihood of preferred against\\nunpreferred responses, with per-sample importance weight.\\nReward ranked fine-tuning RAFT [169] fine-tunes the model\\non ranked responses by the reward model. Preference ranking\\noptimization (PRO) [171] and RRHF [170] penalize the model\\nto rank responses with human preferences and supervised loss.\\nOn the other hand, chain-of-hindsight (CoH) [172] provides\\nfeedback to the model in language rather than reward, to learn\\ngood versus bad responses.\\nAligning with Synthetic Feedback: Aligning LLMs with\\nhuman feedback is slow and costly. The literature suggests a\\nsemi-automated process to align LLMs by prompting LLMs to\\ngenerate helpful, honest, and ethical responses to the queries,\\nand fine-tuning using the newly created dataset. Constitutional\\nAI [173] replaces human feedback in RLHF with AI, calling\\nit RL from AI feedback (RLAIF). AlpacaFarm [174] designs\\nprompts to imitate human feedback using LLMs APIs. Oppo-\\nsite to constitutional AI, AlpacaFarm injects noise in feedback\\nto replicate human mistakes. Self-Align [98] prompts the\\nLLM with ICL examples, instructing the LLM about what the\\nresponse should contain to be considered useful and ethical.\\nThe same LLM is later fine-tuned with the new dataset.\\nAligning with Prompts: LLMs can be steered with prompts to\\ngenerate desirable responses without training [175, 176]. The\\nself-correction prompting in [176] concatenates instructions\\nand CoT with questions, guiding the model to answer its\\ninstruction following a strategy to ensure moral safety before\\nthe actual answer. This strategy is shown to reduce the harm in\\ngenerated responses significantly.\\nRed-Teaming/Jailbreaking/Adversarial Attacks: LLMs\\nexhibit harmful behaviors, hallucinations, leaking personal in-\\nformation, and other shortcomings through adversarial probing.\\nThe models are susceptible to generating harmful responses\\neven though they are aligned for safety [177, 178]. Red-\\nteaming is a common approach to address illicit outputs, where\\nthe LLMs are prompted to generate harmful outputs [178, 179].\\nThe dataset collected through red-teaming is used to fine-tune\\nmodels for safety. While red-teaming largely relies on human\\nannotators, another work [180] red-team LLMs to find prompts\\nthat lead to harmful outputs for other LLMs.\\n3.2.4. Continue Pre-Training\\nAlthough fine-tuning boosts a model’s performance, it leads\\nto catastrophic forgetting of previously learned information.\\nConcatenating fine-tuning data with a few randomly selected\\npre-training samples in every iteration avoids network forget-\\nting [181, 152]. This is also e ffective in adapting LLMs for\\ncases where fine-tuning data is small and the original capac-\\nity is to be maintained. Prompt-based continued pre-training\\n(PCP) [182] trains the model with text and instructions related\\nto tasks and then finally instruction-tunes the model for down-\\nstream tasks.\\n3.2.5. Sample E fficiency\\nWhile fine-tuning data is generally many-fold smaller than\\nthe pre-training data, it still has to be large enough for accept-\\nable performance [16, 97, 18] and requires proportional com-\\nputing resources. Studying the effects on performance with less\\ndata, existing literature [183, 184] finds that models trained\\non less data can outperform models trained with more data.\\nIn [183], 25% of the total downstream data is found enough\\nfor state-of-the-art performance. Selecting coreset-based 0.5%\\nof the total instruction-tuning data improves the model perfor-\\nmance by 2% in [184], as compared to the complete data tun-\\ning. Less is more for alignment (LIMA) [185] uses only 1000\\ncarefully created demonstrations to fine-tune the model and has\\nachieved comparable performance to GPT-4.\\n3.3. Increasing Context Window\\nLLMs are trained with limited context windows due to ex-\\npensive attention and high memory requirements. A model\\ntrained on limited sequence lengths fails to generalize to unseen\\nlengths at inference time [186, 49]. Alternatively, LLMs with\\nALiBi [65] positional encodings can perform zero-shot length\\nextrapolation. However, ALiBi has less expressive power [66]\\nand inferior performance on multiple benchmarks [46], and\\nmany LLMs use RoPE positional embedding that is unable to\\nperform zero-shot extrapolation. A larger context length has\\nbenefits such as a better understanding of longer documents,\\nmore samples in in-context learning, execution of bigger rea-\\nsoning processes, etc. Expanding context length during fine-\\ntuning is slow, inefficient, and computationally expensive [49].\\nTherefore, researchers employ various context window extrap-\\nolation techniques discussed below.\\nPosition Interpolation: Rather than extrapolating, [49] shows\\nthat interpolating position encodings within the pre-trained con-\\ntext window are more e ffective. The work demonstrates that\\nonly 1000 steps of fine-tuning are enough to achieve better re-\\nsults on larger windows without reducing performance com-\\npared to the original context size. Giraffe [46] uses power scal-\\ning in RoPE, and YaRN [47] proposed NTK-aware interpola-\\ntion.\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 17, 'page_label': '18', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='Efficient Attention Mechanism: Dense global attention is\\none of the major constraints in training larger context win-\\ndow LLMs. Using e fficient attention variants, such as lo-\\ncal, sparse, and dilated attention, reduces the computation cost\\nsignificantly. LongT5 [48] proposes transient global atten-\\ntion (TGlobal), applying attention to local and global tokens\\n(windowed token averaging). The model replaces attention\\nin T5 [10] with TGlobal attention, pre-trains the model on\\n4098 sequence length, fine-tunes on larger window sizes, as\\nlarge as 16k, and improves task performance on longer inputs.\\nThis shows the extrapolation ability of TGlobal attention with\\nonly fine-tuning. COLT5 [187] uses two branches, one with\\nlightweight and the other with heavyweight attention and feed-\\nforward layers. All tokens are processed from the lightweight\\nbranch, and only important tokens are routed to the heavy-\\nweight branch. LongNet [188] replaces standard attention with\\ndilated attention, expanding sequence length to 1 billion tokens.\\nLongLoRA [189] proposes shift-short attention, used during\\nfine-tuning to reduce dense attention costs. However, the model\\nduring inference uses dense attention and achieves similar per-\\nformance as full attention fine-tuning.\\nExtrapolation without Training: LM-Infinite [186] and par-\\nallel context windows (PCW) [190] show length extrapolation\\nis possible using pre-trained LLMs. LM-Infinite suggested Λ-\\nshaped attention applied within the original context window\\nlimits. Likewise, PCW chunks larger inputs into the pre-trained\\ncontext lengths and applies the same positional encodings to\\neach chunk.\\n3.4. Augmented LLMs\\nLLMs are capable of learning from the examples concate-\\nnated with the input, known as context augmentation, in-\\ncontext learning (ICL), or few-shot prompting. They show ex-\\ncellent generalization to unseen tasks with few-shot prompt-\\ning, enabling LLMs to answer queries beyond the capacity ac-\\nquired during training [6, 55]. These emergent abilities allow\\nfor adapting the model without fine-tuning—a costly process.\\nAside from this, hallucination, producing inaccurate, unsafe,\\nor factually incorrect responses, is common for LLMs, which is\\navoided by augmenting contextual data. While the user can pro-\\nvide in-context samples in the query [54, 32], here we specifi-\\ncally refer to the methods that access external storage program-\\nmatically, calling them augmented LLMs.\\nThe literature suggests various external memory designs to aug-\\nment LLMs, long-term [191, 192, 193, 194], short-term [195],\\nsymbolic [196], and non-symbolic [197, 198]. The memory\\ncan be maintained in different formats such as documents, vec-\\ntors, or databases. A few systems maintain intermediate mem-\\nory representations to retain information across multiple iter-\\nations [194, 192], while others extract important information\\nfrom the datasets and save it in memory for recall [199]. The\\nmemory read and write operations are performed either with\\nor without LLMs cooperation [192, 200, 194, 201], acting as\\na feedback signal in [195]. We discuss di fferent types of aug-\\nmented LLMs below.\\nFigure 12: A flow diagram of Retrieval Augmented LLMs. The retriever ex-\\ntracts a similar context to the input and forwards it to the LLM either in simple\\nlanguage or encoded through Fusion-in-Decoder (FiD). Depending on the task,\\nretrieval and generation may repeat multiple times.\\n3.4.1. Retrieval Augmented LLMs\\nLLMs may have limited memory and outdated information,\\nleading to inaccurate responses. Retrieving relevant informa-\\ntion from external up-to-date storage enables the LLMs to\\naccurately answer with references and utilize more informa-\\ntion. With retrieval augmentation, smaller models have been\\nshown to perform at par with larger models. For instance, the\\n11B model can become competitive to 540B PaLM in [25] and\\n7.5B to 280B Gopher in [193]. Retrieval augmented language\\nmodeling (RALM) has two major components, shown in\\nFigure 12, namely: 1) retriever and 2) language model. In\\nRALM, the retriever plays a crucial role in driving LLM\\nresponse, where incorrect information can steer LLMs to false\\nbehavior. This leads to the development of various methods to\\nretrieve accurate information and fuse with the query for better\\nperformance.\\nZero-Shot Retrieval Augmentation: This kind of augmen-\\ntation keeps the original LLM architecture and weights\\nunchanged and uses BM25 [202], nearest neighbors, or frozen\\npre-trained models like Bert [7] as a retriever. The retrieved\\ninformation is provided as input to the model for response\\ngeneration, shown to improve performance over LLMs without\\nretrieval [198, 203]. In some scenarios, multiple retrieval\\niterations are required to complete the task. The output\\ngenerated in the first iteration is forwarded to the retriever\\nto fetch similar documents. Forward-looking active retrieval\\n(FLARE) [197] initially generates the response and corrects\\nthe output by retrieving relevant documents if the response\\ncontains low-confidence tokens. Similarly, RepoCoder [204]\\nfetches code snippets recursively for code completion.\\nTraining with Retrieval Augmentation: To reduce failures in\\nretrieval augmentation generation (RAG), researchers train or\\nfine-tune retrievers and LLMs with a retrieval augmentation\\npipeline. We discuss the literature below based on their focus\\non the respective training processes of the pipeline.\\nTraining LLM:Retrieval-enhanced transformer (RETRO) [193]\\nshows pre-training smaller LLMs with RAG pipeline outper-\\nforms larger LLMs, such as GPT-3 trained without RAG.\\nRETRO uses a 2-trillion token subset of MassiveText as\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 18, 'page_label': '19', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='a database. The retrieval pipeline divides the input query\\ninto subsets and retrieves relevant chunks from the database\\nfor each subset, encoded together with input intermediate\\nrepresentations for generating tokens. It uses cross-chunked\\nattention to attend to previous chunks auto-regressively. A\\nstudy on RETRO [205] shows models pre-trained without RAG\\nbut fine-tuned using RAG lack the performance gains obtained\\nby pre-training with RAG.\\nTraining Retriever: Quality of responses generated by LLMs\\nis highly dependent on the in-context examples. There-\\nfore, [206, 207, 208, 209] train retrievers to retrieve accurate\\nfew-shot samples while keeping the LLM frozen for gener-\\nation. Retrieved samples are ranked to build ground-truth\\ndata to train retrievers with contrastive learning in [206, 208].\\nRoBERTa is trained for downstream tasks in [207] for ICL\\nsamples retrieval. REPLUG [209] trains the retriever with\\nsupervised signals from the frozen LLM-generated outputs.\\nTraining Retriever and LLM: Further benefits are achieved by\\ntraining both the retriever and the model in [25, 210, 211]. In\\nthis case, the error propagates back to the retriever, updating\\nboth the language model and the retriever. While masked\\nlanguage modeling (MLM) is a common pre-training objec-\\ntive [25, 211], retrieval pre-trained transformer (RPT) [210]\\nused document chunk prediction as a pre-training objective for\\nlong text modeling.\\nEncoded Context Augmentation: Concatenating retrieved\\ndocuments with the query becomes infeasible as the sequence\\nlength and sample size grow. Encoding the context and fusing\\nit with the decoder (Fusion-in-Decoder) using cross-attention\\nmakes it possible to augment more samples without increasing\\ncomputation costs significantly [212, 193, 210, 25].\\nWeb Augmented: Locally stored memory, but external to\\nLLM, has limited information. However, a large amount of\\ninformation is available on the internet, which gets updated\\nregularly. Rather than storing information locally, various\\nmethods retrieve query-related context through a web search\\nand forward it to LLMs [213, 214, 166].\\n3.4.2. Tool Augmented LLMs\\nWhile RAG relies on the retriever to provide context to the\\nLLM to answer queries, tool augmented LLMs capitalize on the\\nreasoning abilities of LLMs to iteratively plan by dividing tasks\\ninto sub-tasks, selecting necessary tools, and taking actions to\\ncomplete the task [215, 216, 217, 27]. A generic pipeline of\\ntool-augmented LLMs is shown in Figure 13, where di fferent\\nmodules in Figure 13 are selected in a loop until the task com-\\npletion.\\nZero-Shot Tool Augmentation: LLMs in-context learning and\\nreasoning abilities enable them to interact with tools with-\\nout training. Automatic reasoning and tool-use (ART) [217]\\nbuilds a task library with demonstrations of reasoning steps and\\ncalling external tools. It retrieves similar task examples and\\nprovides the context to the LLM for inference. Aside from\\nthis, [218] shows tool documentation is enough to teach LLMs\\nto use tools without demonstrations. RestGPT [219] integrates\\nLLMs with RESTful APIs by decomposing tasks into planning\\nFigure 13: A basic flow diagram of tool augmented LLMs. Given an input and\\na set of available tools, the model generates a plan to complete the task. The\\ntool augmented LLMs utilize di fferent modules iteratively, such as retriever,\\ntool execution, read-write to memory, feedback, etc., depending on the task.\\nand API selection steps. The API selector understands the API\\ndocumentation to select a suitable API for the task and plan the\\nexecution. ToolkenGPT [220] uses tools as tokens by concate-\\nnating tool embeddings with other token embeddings. During\\ninference, the LLM generates the tool tokens representing the\\ntool call, stops text generation, and restarts using the tool exe-\\ncution output.\\nTraining with Tool Augmentation: LLMs are trained to inter-\\nact with diverse tools, enhancing planning abilities to overcome\\nthe limitations of zero-shot tool augmentation [221, 27, 222,\\n223]. Gorilla [221] instruction-tunes LLaMA with information\\nretrieval from API documentation. It uses the self-instruct [19]\\ndata generation pipeline with GPT-4 by providing in-context\\nexamples retrieved from API documentation. Tool augmented\\nlanguage model (TALM) [27] fine-tunes T5 [10] for tool use\\nwith a self-play approach, where it iteratively completes tool\\nmanipulation tasks and includes them back in the training set.\\nToolLLM [223] collects 16k APIs from RapidAPI. It samples\\nAPIs from the list to generate an instruction-tuning dataset us-\\ning ChatGPT in single-tool and multi-tool scenarios. For high-\\nquality datasets, ToolLLM suggested a depth-first search-based\\ndecision tree (DFSDT) method to generate ground-truths with\\ndiverse reasoning and planning.\\nMultimodal Tool Augmentation: The compositional reasoning\\ncapacity of LLMs allows them to manipulate tools in multi-\\nmodal settings [215, 216, 224]. Following the pipeline shown\\nin Figure 13, the LLM outlines a plan, generally executing in a\\nsequence: Plan →Tool selection →Execute →Inspect →\\nGenerate, to respond to the user query. Here, the database of\\ntools is rich in modalities, including text, images, etc. Many of\\nthe multimodal tool augmentation systems employ multimodal\\nLLMs [31, 225, 224, 216], while others utilize single modality\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 19, 'page_label': '20', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='LLMs and generate a plan on using di fferent modality tools to\\nsolve multimodal queries [226].\\n3.5. LLMs-Powered Agents\\nAI agents are autonomous entities, capable of planning,\\ndecision-making, and performing actions to achieve complex\\ngoals. In the early days, AI agents were rule-based, de-\\nsigned for narrow tasks, and had limited capabilities, such\\nas Clippy [227] and Deep Blue [228]. In contrast to this,\\nLLMs abilities to respond to dynamic scenarios have made it\\npossible to incorporate them in diverse applications, includ-\\ning LLMs-powered agents [224, 216], where LLMs behave\\nas the brain of agents. LLMs have been incorporated in web\\nagents [166, 167], coding agents [229], tool agents [27, 223],\\nembodied agents [26], and conversational agents [195], requir-\\ning minimal to no fine-tuning\". Below we summarize the re-\\nsearch in LLMs-based autonomous agents. For a more detailed\\ndiscussion, please refer to [230, 231].\\nLLMs Steering Autonomous Agents: LLMs are the cognitive\\ncontrollers of the autonomous agents. They generate plans, rea-\\nson about tasks, incorporate memory to complete tasks, and\\nadapt the outline depending on the feedback from the environ-\\nment. Depending on the acquired capabilities of LLMs, many\\nmethods fine-tune, propose a better prompting approach, or uti-\\nlize different modules to enhance agents’ performance. Mod-\\nules and strategies employed in autonomous agents are briefly\\ndiscussed below.\\nPlanning and Reasoning: Completing a complex task requires\\nhuman-like logical thinking, planning necessary steps, and\\nreasoning current and future directions. Prompting methods\\nlike chain-of-thoughts [103], tree-of-thoughts [105], and self-\\nconsistency [104] are central to agents, eliciting LLMs to rea-\\nson its actions and choose among di fferent paths for task com-\\npletion. When LLMs are prompted with a task description and\\na sequence of actions, they can accurately generate plan ac-\\ntions without any fine-tuning [232]. Reasoning via planning\\n(RAP) [233] incorporates a re-purposed LLM as a world model\\nto reason about future outcomes and explore alternative paths\\nfor task completion. Retroformer [234] uses a retrospective\\nLLM to improve main LLM planning and reasoning capabil-\\nities by providing helpful task cues.\\nFeedback: LLMs in open-loop systems generate plans and as-\\nsume that the agent will complete them successfully. However,\\nthe actual scenario is di fferent with failures and variable re-\\nsponses from the environment. To correctly complete tasks,\\nmany methods use LLMs in a closed-loop where the action re-\\nsponse is provided as feedback to the LLMs to re-assess and\\nupdate the plan as required [235, 236, 237, 195]. Another di-\\nrection of research exploits LLMs as reward functions to train\\nreinforcement learning (RL) policies instead of humans [238].\\nMemory: LLMs can learn from the context provided in the\\nprompt. In addition to internal memory, various systems em-\\nploy external memory to save the response history. Reflex-\\nion [195] maintains an episodic memory to use previous re-\\nsponses as feedback to improve future decision-making. Retro-\\nformer [234] improves its responses by employing short-term\\nand long-term memory, where short-term memory contains re-\\ncent responses and long-term memory keeps summarized failed\\nattempts to add in the prompt as reflection.\\nMulti-Agents Systems: LLMs can play user-defined roles and\\nbehave like a specific domain expert. In multi-agent systems,\\neach LLM is assigned a unique role, simulating human behav-\\nior and collaborating with other agents to complete a complex\\ntask [229, 239].\\nLLMs in Physical Environment: LLMs are good at\\ninstruction-following, however, utilizing them for physically\\ngrounded tasks requires adaptation, as they lack real-world\\nknowledge. This could lead to generating illogical responses\\nfor a particular physical situation [240, 26]. SayCan [240]\\nmake LLMs aware of the available low-level task operations.\\nLLM (Say) builds a high-level plan to complete the task and\\na learned affordance function (Can) explores the possibility of\\nexecuting the plan in the real world. SayCan uses RL to train\\nthe language-conditioned affordance function. PaLM-E enables\\nthe LLM to solve grounded tasks by training multi-modal LLM\\nfeeding inputs directly from the sensors.\\nManipulation: In the area of manipulation [236, 241], LLMs\\nenhance a robot’s dexterity and adaptability, excelling in tasks\\nlike object recognition, grasping, and collaboration. They ana-\\nlyze visual and spatial information to determine the most effec-\\ntive approach to interact with objects.\\nNavigation: LLMs enhance a robot’s ability to navigate com-\\nplex environments with precision and adaptability [242, 243,\\n244, 245]. They generate feasible paths and trajectories for\\nrobots, accounting for intricate environmental details [246].\\nThis ability is valuable in scenarios requiring precise and\\ndynamically adaptable navigation in environments like ware-\\nhouses, transport, healthcare facilities, and residences.\\n3.6. E fficient LLMs\\nDeploying LLMs in production is expensive. Reducing their\\nrunning costs while preserving performance is an appealing\\narea of research. This section summarizes the approaches sug-\\ngested to enhance LLMs’ efficiency.\\n3.6.1. Parameter E fficient Fine-Tuning\\nFine-tuning LLMs with tens or hundreds of billions of pa-\\nrameters, such as GPT-3 (175B), BLOOM (176B), MT-NLG\\n(540B), etc., is computationally intensive and time-consuming.\\nTo avoid complete model fine-tuning, numerous parameter-\\nefficient fine-tuning (PEFT) techniques [40, 247, 41, 38, 39] try\\nto achieve acceptable model fine-tuning performance at reduced\\ncosts. As compared to full fine-tuning [248], PEFT performs\\nbetter in low-resource setups, achieves comparable perfor-\\nmance on medium-resource scenarios, and performs worse than\\nfull fine-tuning under high-resource availability. An overview\\nof different PEFT approaches is shown in Figure 14.\\nAdapter Tuning: Adds a few trainable parameters within the\\ntransformer block. The adapter layer is a sequence of feature\\ndownscaling, non-linearity, and upscaling [106]. Variants of\\nadapter tuning inject adapter layers sequentially [106] and in\\nparallel [38], whereas the mixture of adapter (AdaMix) [249]\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 20, 'page_label': '21', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='Figure 14: Illustration of parameter-e fficient fine-tuning paradigms, where x is input and h is hidden state, figure courtesy [38]. Parallel adapter and LoRA fall in\\nthe adapter tuning category.\\nemploys multiple adapter modules in a single layer. AdaMix\\nroutes input instances randomly to one of the multiple down-\\nscale and upscale modules. The mixture of adapters is averaged\\nout for inference to avoid additional latency. Low-Rank Adap-\\ntation (LoRA) [250] learns low-rank decomposed matrices to\\nfreeze original weights. The learned weights are fused with the\\noriginal weights for inference, avoiding latency.\\nPrompt Tuning: Prompting is an e ffective way to adapt a\\npre-trained LLM for the downstream task. However, manual\\nprompts bring uncertainty in the model’s prediction, where a\\nchange in a single word drops the performance [247]. Prompt\\ntuning alleviates this problem by fine-tuning only 0.001%-3%\\nadditional parameters [251]. It concatenates trainable prompt\\nparameters with the model embeddings [247, 40, 251]. Task-\\nspecific fixed discrete prompts are concatenated with input em-\\nbeddings in [40]. As discrete prompts bring instability, prompts\\nare encoded through a learnable mapping in P-Tuning [247],\\nnaming continuous prompts, which are appended with the dis-\\ncrete prompts. Only the prompt encoder is trainable in the\\nmodel. In an extension of P-Tuning, continuous prompts are\\nconcatenated with each layer of the network in [251]. Progres-\\nsive prompts [252] avoid catastrophic forgetting and transfer\\npreviously learned knowledge by sequentially adding trainable\\nprompt embeddings to the previously frozen task embeddings.\\nPrefix Tuning: A set of trainable task-specific prefix vectors\\nare appended to the frozen transformer layers in prefix tun-\\ning [41]. The prefix vectors are virtual tokens attended by the\\ncontext tokens on the right. In addition, adaptive prefix tun-\\ning [253] applies a gating mechanism to control the information\\nfrom the prefix and actual tokens.\\nBias Tuning: Fine-tuning only bias terms in small to medium\\ntraining data has been found e ffective in BitFit [254]. This\\nmethod achieves full fine-tuning performance for tasks with less\\ntraining data and comparable performance with more training\\ndata.\\n3.6.2. Quantization\\nLLMs require extensive computing and memory for infer-\\nence. Deploying a 175B parameter GPT-3 model needs at\\nleast five 80GB A100 GPUs and 350GB of memory to store in\\nFP16 format [44]. Such demanding requirements for deploying\\nLLMs make it harder for smaller organizations to utilize them.\\nModel compression is an effective solution but comes at the cost\\nof degraded performance, especially at large scales greater than\\n6B. These models exhibit very large magnitude outliers that do\\nnot exist in smaller models [255], making it challenging and re-\\nquiring specialized methods for quantizing LLMs [44, 256].\\nPost-Training Quantization: Minimal or no training is re-\\nquired in this type of quantization, without significantly com-\\npromising the model performance. LLM-8-bit [255] uses full-\\nprecision matrix multiplication for weights associated with out-\\nlier features and 8-bit for remaining features. The lower pre-\\ncision multiplication outputs are converted to FP-16 and con-\\ncatenated with others. The quantized models have homogenous\\nword embeddings, which may degrade their performance. To\\nfix this, token-level knowledge distillation is employed in [45]\\nalong with independent quantization scaling factors for each\\nmodule due to varying weight distribution. Feature distribu-\\ntions are asymmetric and appear in di fferent channels; outlier\\nsuppression [257] shifts and scales per-channel activation dis-\\ntributions for effective quantization. SmoothQuant [44] quan-\\ntizes activations and weights to INT8 format by smoothing\\nactivations and migrating the quantization di fficulty toward\\nweights. It multiplies the inverse of the smoothing factor with\\nweights, which introduces a few outliers in the weights but is\\neasier to quantify than unsmoothed activations. OPTQ [256]\\nuses the optimal brain compression (OBC) [258] algorithm to\\nquantize the model layer-by-layer and update weights to com-\\npensate for quantization error. To improve speed and per-\\nformance, OPTQ updates weights in arbitrary order, employs\\nlazy updates, and uses better Cholesky kernels. Outlier-aware\\nweight quantization (OWQ) [259] uses the OPTQ algorithm for\\nquantization but assigns higher precision to vulnerable weights,\\ncausing outliers and lower precision for others.\\nQuantization-Aware Training: To compensate for perfor-\\nmance degradation, a quantized model is fine-tuned in\\nquantization-aware training (QAT) [260, 261, 262]. Al-\\npha Tuning quantizes the model using binary coding quan-\\ntization (BCQ) [263] and fine-tunes only quantization scal-\\ning factors. This approach improves performance over\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 21, 'page_label': '22', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='parameter-efficient fine-tuning of the pre-trained model. Sim-\\nilarly, parameter-e fficient and quantization-aware adaptation\\n(PEQA) [264] reduces the precision of fully-connected layers\\nand fine-tunes only quantization scaling parameters. LLM-\\nQAT [262] generates training data from the pre-trained network\\nand trains a quantized student model with knowledge distilla-\\ntion. QLoRA [261] fine-tunes 4-bit quantized pre-trained LLM\\nwith LoRA [250] using a 4-bit normal float, which shows better\\nperformance over a 4-bit integer and float.\\n3.6.3. Pruning\\nPruning is an alternative approach to quantization to com-\\npress model size, thereby reducing LLMs deployment costs\\nsignificantly. Compared to task-agnostic pruning, task-specific\\npruning is easily achievable with good performance, where a\\nmodel is fine-tuned on the downstream task and pruned for\\nfaster inference. It is possible to prune LLMs for individual\\ntasks, but the cost of pruning and deploying task-specific mod-\\nels is high. To overcome this, many structured and unstructured\\npruning methods for LLMs have been proposed to maintain rea-\\nsonable performance across all tasks while shrinking the model\\nsize [265, 42, 266].\\nUnstructured Pruning: This kind of pruning removes less im-\\nportant weights without maintaining any structure. Existing\\nLLM pruning methods take advantage of the unique charac-\\nteristics of LLMs, uncommon for smaller models, where a\\nsmall subset of hidden states are activated with large magni-\\ntude [255]. Pruning by weights and activations (Wanda) [265]\\nprunes weights in every row based on importance, calculated\\nby multiplying the weights with the norm of input. The pruned\\nmodel does not require fine-tuning, thereby saving computa-\\ntional costs. Outlier weighed layerwise sparsity (OWL) [267]\\nextends Wanda with non-uniform layer pruning. It shows that\\nthe number of outliers varies for different layers; therefore, the\\nmodel should have variable pruning ratios for better perfor-\\nmance for every layer. Contrastive pruning (CAP) [43] itera-\\ntively prunes the model by training the sparse model using con-\\ntrastive loss between pre-trained, fine-tuned, and snapshots of\\nprevious sparse models to learn task-specific and task-agnostic\\nknowledge.\\nStructured Pruning: Here, the parameters are removed in\\ngroups, rows, columns, or matrices, which speeds up the\\ninference because of e ffective hardware tensor core utiliza-\\ntion [265]. LLM-Pruner [42] employs a 3-stage structured\\npruning strategy, identifying the groups of hidden states caus-\\ning each other to activate during the forward-pass, keeping im-\\nportant groups and removing less important ones, and fine-\\ntuning the pruned model with LoRA. Sparsity-induced mask\\nlearning (SIMPLE) [268] prunes the network using learnable\\nmasks. Similarly, another method prunes LLMs by learning\\nmasks and removing unimportant rank-1 components of the\\nfactorized weight matrix [266].\\n3.7. Multimodal LLMs\\nInspired by the success of LLMs in natural language process-\\ning applications, an increasing number of research works are\\nnow facilitating LLMs to perceive different modalities of infor-\\nmation like image [269, 270, 271], video [272, 273, 274], au-\\ndio [275, 274, 276], etc. Multimodal LLMs (MLLMs) present\\nsubstantial benefits compared to standard LLMs that process\\nonly text. By incorporating information from various modal-\\nities, MLLMs can achieve a deeper understanding of context,\\nleading to more intelligent responses infused with a variety of\\nexpressions. Importantly, MLLMs align closely with human\\nperceptual experiences, leveraging the synergistic nature of our\\nmultisensory inputs to form a comprehensive understanding of\\nthe world [276, 26]. Coupled with a user-friendly interface,\\nMLLMs can offer intuitive, flexible, and adaptable interactions,\\nallowing users to engage with intelligent assistants through a\\nspectrum of input methods. According to the ways of construct-\\ning models, current MLLMs can be generally divided into three\\nstreams: pre-training, fine-tuning, and prompting. In this sec-\\ntion, we will discuss more details of these main streams, as well\\nas the important application of MLLMs in visual reasoning.\\nPre-training: This stream of MLLMs intends to support differ-\\nent modalities using unified end-to-end models. For instance,\\nFlamingo [269] applies gated cross-attention to fuse vision and\\nlanguage modalities, which are collected from pre-trained and\\nfrozen visual encoder and LLM, respectively. Moreover, BLIP-\\n2 [270] proposes a two-stage strategy to pre-train a Querying\\nTransformer (Q-Former) for the alignment between vision and\\nlanguage modalities: in the first stage, vision-language repre-\\nsentation learning is bootstrapped from a frozen visual encoder;\\nand in the second stage, a frozen LLM bootstraps vision-to-\\nlanguage generative learning for zero-shot image-to-text gen-\\neration. Similarly, MiniGPT-4 [277] deploys pre-trained and\\nfrozen ViT [278], Q-Former and Vicuna LLM [159], only train-\\ning the linear projection layer for vision and language modali-\\nties alignment.\\nFine-tuning: Derived from instruction tuning [16] for NLP\\ntasks [20, 16, 97], researchers are fine-tune pre-trained LLMs\\nusing multimodal instructions. Following this method, LLMs\\ncan be easily and e ffectively extended as multimodal chat-\\nbots [277, 271, 29] and multimodal task solvers [279, 30, 280].\\nThe key issue of this stream of MLLMs is to collect multi-\\nmodal instruction-following data for fine-tuning [58]. To ad-\\ndress this issue, the solutions of benchmark adaptation [279,\\n281, 282], self-instruction [19, 31, 283], and hybrid composi-\\ntion [284, 280] are employed, respectively. To mitigate the gap\\nbetween the original language modality and additional modal-\\nities, the learnable interface is introduced to connect di ffer-\\nent modalities from frozen pre-trained models. Particularly,\\nthe learnable interface is expected to work in a parameter-\\nefficient tuning manner: e.g., LLaMA-Adapter [285] applies\\nan e fficient transformer-based adapter module for training,\\nand LaVIN [284] dynamically learns the multimodal feature\\nweights using a mixture-of-modality adapter. Di fferent from\\nthe learnable interface, the expert models can directly convert\\nmultimodalities into language: e.g., VideoChat-Text [272] in-\\ncorporates Whisper [286], a speech recognition expert model,\\nto generate the captions of given videos for the understanding\\nof following LLMs.\\nPrompting: Different from the fine-tuning technique that\\n22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 22, 'page_label': '23', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='directly updates the model parameters given task-specific\\ndatasets, the prompting technique provides certain context, ex-\\namples, or instructions to the model, fulfilling specialized tasks\\nwithout changing the model parameters. Since prompting can\\nsignificantly reduce the need for large-scale multimodal data,\\nthis technique is widely used to construct MLLMs. Particularly,\\nto solve multimodal Chain of Thought (CoT) problems [103],\\nLLMs are prompted to generate both the reasoning process and\\nthe answer given multimodal inputs [287]. On this front, differ-\\nent learning paradigms are exploited in practice: for example,\\nMultimodal-CoT [287] involves two stages of rationale genera-\\ntion and answer inference, where the input of the second stage\\nis a combination of the original input and the output of the first\\nstage; and CoT-PT [288] applies both prompt tuning and spe-\\ncific visual bias to generate a chain of reasoning implicitly. In\\naddition to CoT problems, LLMs can also be prompted with\\nmultimodal descriptions and tools, effectively dividing complex\\ntasks into sub-tasks [289, 290].\\nVisual Reasoning Application: Recent visual reasoning sys-\\ntems [291, 292, 216, 293] tend to apply LLMs for better visual\\ninformation analysis and visual-language integration. Di ffer-\\nent from previous works [294, 295] that rely on limited VQA\\ndatasets and small-scale neural networks, current LLM-aided\\nmethods offer benefits of stronger generalization ability, emer-\\ngent ability, and interactivity [58]. To realize visual reasoning\\nwith the help of LLMs, prompting and fine-tuning techniques\\ncan also be utilized: for example, PointClip V2 [292] applies\\nLLMs to generate 3D-specific prompts, which are encoded as\\ntextual features and then combined with visual features for\\n3D recognition; and GPT4Tools [31] employs LoRA [250] to\\nfine-tune LLMs following tool-related instructions. Serving\\nas a controller [293], decision maker [296], or semantics re-\\nfiner [291, 297], LLMs significantly facilitates the progress of\\nvisual reasoning research.\\n3.8. Summary and Discussion\\n3.8.1. Architecture\\nDue to the gigantic scale of LLMs, minor changes in archi-\\ntecture and training strategies have a big impact on performance\\nand stability. Here, we summarize key architectural modules\\nused in various LLMs, leading to better performance, reduced\\ntraining time and memory, and better training stability.\\nLayer Normalization: The performance and training stability\\nof LLMs are affected significantly by layer normalization. Pre-\\nnorm, that is normalizing inputs rather than outputs, is more\\ncommon among LLMs stabilizing the training [6, 127, 108].\\nBLOOM [13] and AlexaTM [122] utilize an additional layer\\nnormalization before embedding layer to stabilize the training\\nof large-scale models, while the model’s zero-shot generaliza-\\ntion ability can be negatively impacted [13]. However, another\\nstudy [33] finds that pre-norm degrades fine-tuned model per-\\nformance as compared to post-norm, and there are no stability\\nbenefits of pre-norm beyond the 100B scale. Therefore, GLM-\\n130B [33] used deep-norm which is a variant of post-norm for\\nbetter downstream task performance after fine-tuning.\\nPositional Encoding: Like other building blocks of the model,\\npositional encoding also a ffects the performance and training\\nstability of LLMs. BLOOM [13] finds ALiBi outperforms\\nlearned and rotary positional encodings. Contrary to this,\\nGLM-130B [33] identifies rotary positional encoding as being\\nbetter than ALiBi. So, there is no conclusion in the literature\\nabout positional encodings yet.\\nParallel Attention: In this type of attention, feed-forward and\\nattention layers are parallel to each other rather than sequen-\\ntial in a transformer block. It has been shown to reduce train-\\ning time by 15%. There is no evidence of performance drop\\ndue to this change in the literature and it is used by the models\\nPaLM [15], GPT-NeoX [118], and CodeGen [140].\\nMulti-Query Attention It has shared key and value attention\\nheads in a transformer block while query attention heads are\\nprojected as usual. This reduces memory usage and speeds up\\nsampling in autoregressive decoding. No performance degrada-\\ntion has been observed with this change and it makes the train-\\ning efficient allowing larger batch sizes. Multi-query attention\\nis used in [15, 142].\\nMixture of Experts: This type of architecture enables eas-\\nily scaling models to trillions of parameters [92, 91]. Only a\\nfew experts are activated during the computation making them\\ncompute-efficient. The performance of MoE models is better\\nthan dense models for the same amount of data and requires less\\ncomputation during fine-tuning to achieve performance similar\\nto dense models as discussed in [91]. MoE architectures are\\nless prone to catastrophic forgetting, therefore are more suited\\nfor continual learning [92]. Extracting smaller sub-models for\\ndownstream tasks is possible without losing any performance,\\nmaking MoE architecture hardware-friendly [92].\\nSparse vs Dense Activated: GPT-3 [6] uses sparse transform-\\ners [67] whereas GLaM [91] and PanGu-P [92] use MoE [121]\\narchitectures to lower computational costs and increase the\\nmodel size and capacity. According to the literature, sparse\\nmodules do not degrade the model’s performance [67]. How-\\never, more experiments are required to verify this statement.\\n3.8.2. Training Strategies\\nTraining models at a huge scale require tricks to reduce train-\\ning costs, avoid loss divergence, and achieve better perfor-\\nmance. We summarize and discuss some of these key tricks\\nused in different LLMs.\\nMixed Precision: It is a famous method for LLMs to reduce\\nmemory usage and improve training e fficiency. In mixed pre-\\ncision, forward and backward passes are performed in FP16\\nformat whereas optimizer states and master weights are kept\\nin FP32 format [120]. A drawback associated with this for-\\nmat change is training instability due to a smaller value range\\nresulting in loss spikes [33]. An alternative to FP16 is BF16\\nwhich has a comparatively larger range and performs precision-\\nsensitive operations like gradient accumulation and softmax in\\nFP32 [13]. BF16 has better performance and training stability\\nbut uses more memory and is supported on specific hardware,\\nfor example, A100 GPUs. Therefore, its adoption in LLMs is\\nlimited.\\nTraining Instability: Loss divergence or spiking is a common\\nissue in LLMs that occurs multiple times during training. This\\n23'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 23, 'page_label': '24', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='happens in the presence of gradient clipping [15]. To mitigate\\nthis problem, many approaches suggest restarting training from\\nan earlier checkpoint [15, 33, 91], skipping 200-500 earlier\\ndata batches at the point of divergence in [15] and re-shu ffling\\nbatches in [91]. The embedding layer gradient shrink proves to\\nfurther stabilize the training as its gradient norm is significantly\\nlarger than the other layers [33]. Another suggestion to improve\\ntraining stability for larger models is not to use biases in dense\\nand norm layers as in [15].\\nWeight Initialization: It plays a significant role in model con-\\nvergence and training stability. GPT-NeoX [118] initializes\\nfeed-forward layers before residuals with 2\\nL\\n√\\nd as in [153] and\\nother layers with the small initialization scheme [298]. This\\navoids activations growing exponentially with increasing depth.\\nMT-NLG [117] found higher variance for weight initialization\\nleads to unstable training, hence validating small initialization\\nscheme [298]. Various models perform random weight initial-\\nization which can cause bad initialization, Galactica [148] sug-\\ngests a longer warmup to negate the effect.\\nLearning Rate: A suitable learning rate is important for sta-\\nble training. It is suggested to use a lower value [13, 15, 124]\\nwith warmup and decay (cosine or linear). Usually, the learn-\\ning rate is within the range 1 e−4 to 8e−4. Moreover, MT-NLG\\n(530B) [117] and GPT-NeoX (20B) [118] suggest interpolat-\\ning learning rates based on the model size using the GPT-3 [6]\\nmodels ranging between 13B and 175B. This avoids tuning the\\nlearning rate hyperparameter.\\nTraining Parallelism: 3D parallelism, a combination of data,\\npipeline, and tensor parallelism, is the most utilized training\\nparallelism approach in LLMs [33, 15, 14, 13, 117, 115, 112].\\nIn addition to 3D parallelism, BLOOM [13] uses a zero op-\\ntimizer [37] to shard optimizer states. PanGu- α [108] and\\nPanGu-Σ [92] go beyond 3D parallelism and apply 5D paral-\\nlelism which additionally contains optimizer parallelism and\\nrematerialization.\\nMode Switching: It adds task-related tokens at the beginning\\nof the text during training. These tokens refer to the natural\\nlanguage understanding and natural language generation tasks\\nwhich are shown to improve downstream task performance\\nin [125, 124, 122]. During fine-tuning and inference, tokens\\nare appended based on the downstream tasks.\\nControllable Text Generation: Generating credible and con-\\ntrolled text from a pre-trained model is challenging. GPT-3 [6]\\nand other LLMs use in-context learning to control generated\\ntext. While in-context learning helps in controlling the gener-\\nated text, ERNIE 3.0 Titan [35] suggests using adversarial loss\\nto rank its generated text for credibility and soft prompts such as\\ngenre, topic, keywords, sentiment, and length for better control\\non generated text.\\n3.8.3. Supervised Models vs Generalized Models\\nAlthough generalized models are capable of performing di-\\nverse tasks with good performance they have not yet outper-\\nformed models trained in supervised settings. The supervised\\ntrained models are still state-of-the-art in various NLP tasks by\\na large margin as shown in [6, 15, 18].\\n3.8.4. Zero-Shot vs Few-Shot\\nLLMs perform well in zero-shot and few-shot settings. But\\nthe performance di fference between zero-shot and few-shot is\\nlarge for pre-trained models [6, 15], naming LLMs as meta-\\nlearners [6]. LLMs zero-shot evaluations underperform unsu-\\npervised methods in neural machine translation [6]. The liter-\\nature shows pre-training is not enough for good zero-shot per-\\nformance [15, 16]. To improve the zero-shot performance the\\nliterature suggests using instruction fine-tuning that improves\\nthe zero-shot performance significantly and outperforms base-\\nlines. Instruction fine-tuning has also been shown to improve\\nzero-shot generalization to unseen tasks. Another model, Flan-\\nPaLM [16], unlocks zero-shot reasoning with CoT training.\\n3.8.5. Encoder vs Decoder vs Encoder-Decoder\\nTraditionally, these architectures perform well for di fferent\\ntasks, for example, encoder-only for NLU tasks, decoder-only\\nfor NLG, and encoder-decoder for sequence2sequence model-\\ning. Encoder-only models are famous for smaller models such\\nas Bert [7], RoBERTa [299], etc., whereas LLMs are either\\ndecoder-only [6, 118, 13] or encoder-decoder [10, 11, 122].\\nWhile decoder-only models are good at NLG tasks, various\\nLLMs, PaLM [15], OPT [14], GPT-3 [6], BLOOM [13],\\nLLaMA [156], are decoder-only models with significant per-\\nformance gains on both NLU and NLG tasks. In contradic-\\ntion to this, T5 [10] and UL2 [125] identify encoder-decoder\\nmodels out-performing decoder-only models. In another study,\\nPaLM [15] finds increasing the size of decoder-only models\\ncan reduce the performance gap between decoder-only and\\nencoder-decoder architectures.\\nAlthough decoder-only architectures have become a trend for\\nLLMs, many recently proposed approaches [125, 122] use\\nmode-switching tokens in text with encoder-decoder architec-\\ntures to enable task-specific modes. Similarly, CodeT5 + [34]\\nuses an encoder-decoder architecture with multiple training ob-\\njectives for different tasks, activating the encoder, decoder, or\\nboth according to the tasks. These variations in architecture\\nand training objectives allow a model to perform well in differ-\\nent settings. Because of this dynamic configuration, the future\\nof LLMs can be attributed to encoder-decoder architectures.\\n4. Model Configurations\\nWe provide different statistics of pre-trained and instruction-\\ntuned models in this section. This includes information such as\\npublication venue, license type, model creators, steps trained,\\nparallelism, etc in Table 3 and Table 4. Architecture details\\nof pre-trained LLMs are available in Table 5. Providing these\\ndetails for instruction-tuned models is unnecessary because it\\nfine-tunes pre-trained models for instruction datasets. Hence,\\narchitectural details are the same as the baselines. Moreover,\\noptimization settings for various LLMs are available in Table 6\\nand Table 7. We do not include details on precision, warmup,\\nand weight decay in Table 7. These details are not as important\\nas others to mention for instruction-tuned models, and are not\\nprovided by the papers.\\n24'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 24, 'page_label': '25', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content=\"Table 3: Summary of pre-trained LLMs ( >10B). Only the LLMs discussed individually in the previous sections are summarized. “Data /Tokens” is the model’s\\npre-training data, which is either the number of tokens or data size. “Data Cleaning” indicates whether data cleaning is performed or not. This includes heuristics\\n(Heur), deduplication (Dedup), quality filtering (QF), and privacy filtering (PF), “Cost” is the calculated training cost obtained by multiplying the GPUs /TPUs\\nhourly rate with the number of GPUs and the training time. The actual cost may vary due to many reasons such as using in-house GPUs or getting a discounted rate,\\nre-training, number of employees working on the problem, etc. “Training Parallelism” indicates distributed training using data parallelism (D), tensor parallelism\\n(T), pipeline parallelism (P), context parallelism (C), model parallelism (M), optimizer parallelism (OP), and rematerialization (R), where for “Library” column,\\n“DS” is a short form for Deep Speed. In column “Commercial Use”, we assumed a model is for non-commercial purposes if its license is unavailable.\\nModels PublicationVenue LicenseType ModelCreatorsPurposeNo. ofParamsCommercialUse StepsTrainedData/TokensDataCleaning No. ofProcessing UnitsProcessingUnit TypeTrainingTimeCalculatedTrain. CostTrainingParallelism Library\\nT5 [10] JMLR'20Apache-2.0GoogleGeneral11B ✓ 1M 1T Heur+Dedup 1024 TPU v3 - - D+M Mesh TensorFlowGPT-3 [6] NeurIPS'20 - OpenAIGeneral175B × - 300BDedup+QF - V100 - - M -mT5 [11] NAACL'21Apache-2.0GoogleGeneral13B ✓ 1M 1T - - - - - - -PanGu-α[108] arXiv'21 Apache-2.0HuaweiGeneral200B ✓ 260k1.1TBHeur+Dedup 2048 Ascend 910- - D+OP+P+O+R MindSporeCPM-2 [12] AI Open'21MIT TsinghuaGeneral198B ✓ 1M 2.6TB Dedup - - - - D+M JAXFormerCodex [141] arXiv'21 - OpenAICoding12B × - 100B Heur - - - - - -ERNIE 3.0 [110]arXiv'21 - BaiduGeneral10B × 120k∗ 375BHeur+Dedup 384 V100 - - M∗ PaddlePaddleJurassic-1 [112]White-Paper'21Apache-2.0AI21 General178B ✓ - 300B - 800 GPU - - D+M+P Megatron+DSHyperCLOV A [114]EMNLP'21 - NaverGeneral82B × - 300BClf+Dedup+PF 1024 A100 321h 1.32 Mil M MegatronYuan 1.0 [115]arXiv'21 Apache-2.0- General245B ✓ 26k∗ 180BHeur+Clf+Dedup 2128 GPU - - D+T+P -Gopher [116]arXiv'21 - GoogleGeneral280B × - 300BQF+Dedup 4096 TPU v3920h13.19 MilD+M JAX+HaikuERNIE 3.0 Titan [35]arXiv'21 - BaiduGeneral260B × - 300BHeur+Dedup - Ascend 910- - D+M+P+D* PaddlePaddleGPT-NeoX-20B [118]BigScience'22Apache-2.0EleutherAIGeneral20B ✓ 150k825GB None 96 40G A100- - M Megatron+DS+PyTorchOPT [14] arXiv'22 MIT Meta General175B ✓ 150k180B Dedup 992 80G A100- - D+T MegatronBLOOM [13]arXiv'22 RAIL-1.0BigScienceGeneral176B ✓ - 366BDedup+PR 384 80G A1002520h3.87 MilD+T+P Megatron+DSGalactica [148]arXiv'22 Apache-2.0Meta Science120B × 225k106B Dedup 128 80GB A100- - - MetaseqGLaM [91] ICML'22 - GoogleGeneral1.2T × 600k∗ 600B Clf 1024 TPU v4 - - M GSPMDLaMDA [150]arXiv'22 - GoogleDialog137B × 3M 2.81T Filtered 1024 TPU v31384h4.96 MilD+M LingvoMT-NLG [117]arXiv'22 Apache-v2.0MS.+NvidiaGeneral530B × - 270B - 4480 80G A100- - D+T+P Megatron+DSAlphaCode [142]Science'22Apache-v2.0GoogleCoding41B ✓ 205k967BHeur+Dedup - TPU v4 - - M JAX+HaikuChinchilla [96]arXiv'22 - GoogleGeneral70B × - 1.4T QF+Dedup - TPUv4 - - - JAX+HaikuPaLM [15] arXiv'22 - GoogleGeneral540B × 255k780B Heur 6144 TPU v4 - - D+M JAX+T5XAlexaTM [122]arXiv'22 Apache v2.0AmazonGeneral20B × 500k1.1T Filtered 128 A100 2880h1.47 Mil M DSU-PaLM [124]arXiv'22 - GoogleGeneral540B × 20k - - 512 TPU v4120h 0.25 Mil - -UL2 [125] ICLR'23 Apache-2.0GoogleGeneral20B ✓ 2M 1T - 512 TPU v4 - - M JAX+T5XGLM [33] ICLR'23 Apache-2.0MultipleGeneral130B × - 400B - 768 40G A1001440h3.37 Mil M -CodeGen [140]ICLR'23 Apache-2.0SalesforceCoding16B ✓ 650k577BHeur+Dedup - TPU v4 - - D+M JAXFormerLLaMA [127]arXiv'23 - Meta General65B × 350k1.4TClf+Heur+Dedup 2048 80G A100504h 4.12 MilD+M xFormersPanGuΣ[92] arXiv'23 - HuaweiGeneral1.085T × - 329B - 512 Ascend 9102400h - D+OP+P+O+R MindSporeBloombergGPT [151]arXiv23 - BloombergFinance50B × 139k569B Dedup 512 40G A1001272h1.97 Mil M PyTorchXuan Yuan 2.0 [152]arXiv23 RAIL-1.0Du XiaomanFinance176B ✓ - 366B Filtered - 80GB A100- - P DSCodeT5+[34] arXiv'23 BSD-3SalesforceCoding16B ✓ 110k51.5B Dedup 16 40G A100- - - DSStarCoder [147]arXiv'23OpenRAIL-MBigCodeCoding15.5B ✓ 250k 1T Dedup+QF+PF 512 80G A100624h 1.28 MilD+T+P Megatron-LMLLaMA-2 [21]arXiv'23 LLaMA-2.0Meta General70B ✓ 500k 2TMinimal Filtering- 80G A1001.7Mh - - -PaLM-2 [123]arXiv'23 - GoogleGeneral- × - - Ddedup+PF+QF - - - - - -LLaMA-3.1 [130]arXiv'24 LLaMA-3.0Meta General405B ✓ 1.2M15T Dedup+QF 16k 80G H10030.84Mh- D+T+P+C PyTorchMixtral 8x22B [131]web'24 Apache-2.0Mistral AIGeneral141B ✓ - - - - - - - - -Snowflake Arctic [132]web'24 Apache-2.0SnowflakeGeneral480B ✓ - 3.5T - - - - T+P DSNemotron-4 340B [137]web'24 Nvidia NvidiaGeneral340B ✓ - 9T - 6144 80G H100- - D+T+P -DeepSeek [138]arXiv'24 MIT DeepSeekGeneral67B ✓ - 2T Dedup+QF - - 300.6Kh- D+T+P DSDeepSeek-v2 [139]arXiv'24 MIT DeepSeekGeneral67B ✓ - 8.1T QF - H800172.8Kh- D+P HAI-LLM\\nTable 4: Summary of instruction tuned LLMs (>10B). All abbreviations are the same as Table 3. Entries in “Data/Tokens” starting with “S-” represent the number\\nof training samples.\\nModels PublicationVenue LicenseType ModelCreatorsPurposeNo. ofParamsCommercialUse Pre-trainedModelsStepsTrainedData/Tokens No. ofProcessing UnitsProcessingUnit TypeTrain.TimeCalculatedTrain. CostTrain.ParallelismLibrary\\nWebGPT [166]arXiv'21 - OpenAI General175B × GPT-3 - - - - - - - -T0 [17] ICLR'22Apache-2.0BigScienceGeneral11B ✓ T5 - 250B 512 TPU v3270h 0.48 Mil - -Tk-Instruct [18]EMNLP'22MIT AI2+ General11B ✓ T5 1000 - 256 TPU v34h 0.0036 Mil - Google T5OPT-IML [97]arXiv'22 - Meta General175B × OPT 8k 2B 128 40G A100- - D+T MegatronFlan-U-PaLM [16]ICLR'22Apache-2.0Google General540B ✓ U-PaLM30k - 512 TPU v4 - - - JAX+T5XmT0 [154] ACL'23Apache-2.0HuggingFace+ General13B ✓ mT5 - - - - - - - -Sparrow [167]arXiv'22 - Google Dialog70B × Chinchilla- - 64 TPU v3 - - M -WizardCoder [164]arXiv'23Apache-2.0HK Bapt.Coding15B × StarCoder200 S-78k - - - - - -Alpaca [158]Github'23Apache-2.0StanfordGeneral13B ✓ LLaMA3-EpochS-52k 8 80G A1003h 600 FSDP PyTorchVicuna [159]Github'23Apache-2.0LMSYS General13B ✓ LLaMA3-EpochS-125k - - - - FSDP PyTorchLIMA [185] arXiv'23 - Meta+ General65B - LLaMA15-EpochS-1000 - - - - - -Koala [300] Github'23Apache-2.0UC-BerkleyGeneral13B × LLaMA2-EpochS-472k 8 A100 6h 100 - JAX/FLAX\\n5. Datasets and Evaluation\\nGenerating training and evaluation datasets is expensive be-\\ncause of the large-scale data demand of LLMs. Hence, datasets\\nfor training and benchmarking these models are topics of key\\nimportance. A summary of datasets commonly used by LLMs\\nis provided next.\\n5.1. Training Datasets\\nThe performance of LLMs largely depends on the training\\ndata’s quality, size, and diversity. Preparing training datasets\\nof high quality at a large scale is laborious. Researchers have\\nsuggested various pre-training and fine-tuning datasets to en-\\nhance LLMs capabilities. We summarize these e fforts in Ta-\\nble 8. While numerous training datasets are available in the\\nliterature, we cover the most widely used ones in our summary.\\n25\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 25, 'page_label': '26', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='Table 5: Architecture details of LLMs. Here, “PE” is the positional embedding, “nL” is the number of layers, “nH” is the number of attention heads, “HS” is the\\nsize of hidden states.\\nModels Type TrainingObjective Attention Vocab Tokenizer Norm PE ActivationBias nL nH HS\\nT5 (11B) Enc-Dec Span CorruptionStandard 32k SentencePiecePre-RMSRelative ReLU × 24 128 1024GPT3 (175B) Causal-Dec Next TokenDense+Sparse - - Layer Learned GeLU ✓ 96 96 12288mT5 (13B) Enc-Dec Span CorruptionStandard 250k SentencePiecePre-RMSRelative ReLU - - - -PanGu-α(200B) Causal-Dec Next Token Standard 40k BPE Layer - - - 64 128 16384CPM-2 (198B) Enc-Dec Span CorruptionStandard 250k SentencePiecePre-RMSRelative ReLU - 24 64 -Codex (12B) Causal-Dec Next Token Standard - BPE+ Pre-LayerLearned GeLU - 96 96 12288ERNIE 3.0 (10B) Causal-Dec Next Token Standard - WordPiece Post-LayerRelative GeLU - 48 64 4096Jurassic-1 (178B) Causal-Dec Next Token Standard 256k SentencePiece∗ Pre-LayerLearned GeLU ✓ 76 96 13824HyperCLOV A (82B)Causal-Dec Next TokenDense+Sparse - BPE* Pre-LayerLearned GeLU - 64 80 10240Yuan 1.0 (245B) Causal-Dec Next Token Standard - - - - - - 76 - 16384Gopher (280B) Causal-Dec Next Token Standard 32k SentencePiecePre-RMSRelative GeLU ✓ 80 128 16384ERNIE 3.0 Titan (260B)Causal-Dec Next Token Standard - WordPiece Post-LayerRelative GeLU - 48 192 12288GPT-NeoX-20B Causal-Dec Next Token Parallel 50k BPE Layer Rotary GeLU ✓ 44 64 -OPT (175B) Causal-Dec Next Token Standard - BPE - - ReLU ✓ 96 96 -BLOOM (176B) Causal-Dec Next Token Standard 250k BPE Layer ALiBi GeLU ✓ 70 112 14336Galactica (120B) Causal-Dec Next Token Standard 50k BPE+custom Layer Learned GeLU × 96 80 10240GLaM (1.2T) MoE-Dec Next Token Standard 256k SentencePiece Layer Relative GeLU ✓ 64 128 32768LaMDA (137B) Causal-Dec Next Token Standard 32k BPE Layer RelativeGeGLU - 64 128 8192MT-NLG (530B) Causal-Dec Next Token Standard 50k BPE Pre-LayerLearned GeLU ✓ 105 128 20480AlphaCode (41B) Enc-Dec Next Token Multi-query 8k SentencePiece - - - - 64 128 6144Chinchilla (70B) Causal-Dec Next Token Standard 32k SentencePiece-NFKCPre-RMSRelative GeLU ✓ 80 64 8192PaLM (540B) Causal-Dec Next TokenParallel+Multi-query256k SentencePiece Layer RoPE SwiGLU × 118 48 18432AlexaTM (20B) Enc-Dec Denoising Standard 150k SentencePiecePre-LayerLearned GeLU ✓ 78 32 4096Sparrow (70B) Causal-Dec Pref.&Rule RM - 32k SentencePiece-NFKCPre-RMSRelative GeLU ✓ 16∗ 64 8192U-PaLM (540B) Non-Causal-Dec MoDParallel+Multi-query256k SentencePiece Layer RoPE SwiGLU × 118 48 18432UL2 (20B) Enc-Dec MoD Standard 32k SentencePiece - - - - 64 16 4096GLM (130B) Non-Causal-DecAR Blank InfillingStandard 130k SentencePiece Deep RoPE GeGLU ✓ 70 96 12288CodeGen (16B) Causal-Dec Next Token Parallel - BPE Layer RoPE - - 34 24 -LLaMA (65B) Causal-Dec Next Token Standard 32k BPE Pre-RMSRoPE SwiGLU - 80 64 8192PanGu-Σ(1085B) Causal-Dec Next Token Standard - BPE Fused Layer- FastGeLU - 40 40 5120BloombergGPT (50B)Causal-Dec Next Token Standard 131k Unigram Layer ALiBi GeLU ✓ 70 40 7680Xuan Yuan 2.0 (176B)Causal-Dec Next Token Self 250k BPE Layer ALiBi GeLU ✓ 70 112 14336CodeT5+(16B) Enc-Dec SC+NT+Cont.+Match Standard - Code-Specific - - - - - - -StarCoder (15.5B) Causal-Dec FIM Multi-query 49k BPE - Learned - - 40 48 6144LLaMA-2 (70B) Causal-Dec Next TokenGrouped-query32k BPE Pre-RMSRoPE SwiGLUE - - - -PaLM-2 - MoD Parallel - - - - - - - - -LLaMA-3.1 (405B)Causal-Dec Next TokenGrouped-query128k BPE Pre-RMSRoPE SwiGLU - 126 128 16384Nemotron-4 (340B)Causal-Dec Next Token Standard 256k SentencePiece - RoPE ReLU × 96 96 18432DeepSeek (67B) Causal-Dec Next TokenGrouped-query100k BBPE Pre-RMSRoPE SwiGLU - 95 64 8192DeepSeek-v2 (67B)MoE-Dec Next TokenMulti-Head Latent100k BBPE Pre-RMSRoPE SwiGLU - 60 128 5120\\n5.2. Evaluation Datasets and Tasks\\nThe evaluation of LLMs is important in gauging their profi-\\nciency and limitations. This process measures the model’s abil-\\nity to comprehend, generate, and interact with human language\\nacross a spectrum of tasks. Evaluating a language model (LM)\\nis divided into two broader categories: 1) natural language un-\\nderstanding (NLU) and 2) natural language generation (NLG).\\nIt is emphasized that tasks in NLU and NLG are softly catego-\\nrized and are often used interchangeably in the literature.\\nNatural Language Understanding: It measures the language\\nunderstanding capacity of LMs. It encompasses multiple tasks,\\nincluding sentiment analysis, text classification, natural lan-\\nguage inference (NLI), question answering (QA), common-\\nsense reasoning (CR), mathematical reasoning (MR), reading\\ncomprehension (RC), etc.\\nNatural Language Generation: It assesses the language gener-\\nation capabilities of LLMs by understanding the provided input\\ncontext. It includes tasks such as summarization, sentence com-\\npletion, machine translation (MT), dialogue generation, etc.\\nNumerous datasets are proposed for each task, evaluating\\nLLMs against different characteristics. To provide an overview\\nof evaluation datasets, we briefly discuss a few famous datasets\\nwithin each category and offer a comprehensive list of datasets\\nin Table 9. Moreover, we show a detailed overview of the train-\\ning datasets and evaluation tasks and benchmarks used by vari-\\nous pre-trained LLMs in Table 10 and fine-tuned LLMs in Ta-\\nble 11. We also compare the top-performing LLMs in various\\nNLP tasks in Table 12.\\n5.2.1. Multi-task\\nMMLU [307]: A benchmark that measures the knowledge\\nacquired by models during pretraining and evaluates models in\\nzero-shot and few-shot settings across 57 subjects, testing both\\nworld knowledge and problem-solving ability.\\nSuperGLUE [2]: A more challenging and diverse successor\\nto the GLUE [309] benchmark, SuperGLUE includes a variety\\nof language understanding tasks, such as question answering,\\nnatural language inference, and co-reference resolution. It is\\ndesigned to provide a rigorous test of language understanding\\nand requires significant progress in areas like sample-e fficient,\\ntransfer, multi-task, and unsupervised or self-supervised learn-\\ning.\\nBIG-bench [308]: The BIG-bench (Behavior of Intelligent\\nGenerative Models Benchmark) is a large-scale benchmark de-\\nsigned to test the abilities of LLMs across a wide range of\\ntasks, including reasoning, creativity, ethics, and understanding\\nof specific domains.\\nGLUE [309]: The General Language Understanding Evalua-\\ntion (GLUE) benchmark is a collection of resources for train-\\ning, evaluating, and analyzing natural language understanding\\n26'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 26, 'page_label': '27', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='Table 6: Summary of optimization settings used for pre-trained LLMs. The values for weight decay, gradient clipping, and dropout are 0.1, 1.0, and 0.1, respectively,\\nfor most of the LLMs.\\nSequence LR Optimizers Precision WeightGrad\\nModels Batch Size Length LR Warmup Decay AdaFactorAdamAdamWFP16BF16MixedDecay Clip Dropout\\nT5 (11B) 211 512 0.01 × inverse square root✓ - - - - - ✓\\nGPT3 (175B) 32K - 6e-5 ✓ cosine ✓ ✓ ✓ ✓ -\\nmT5 (13B) 1024 1024 0.01 - inverse square root✓ - - - - - ✓\\nPanGu-α(200B) - 1024 2e-5 - - - - - - ✓ - - - -\\nCPM-2 (198B) 1024 1024 0.001 - - ✓ - - - - - ✓\\nCodex (12B) - - 6e-5 ✓ cosine ✓ ✓ ✓ - -\\nERNIE 3.0 (12B) 6144 512 1e-4 ✓ linear ✓ - - - ✓ - -\\nJurassic-1 (178B) 3.2M 2048 6e-5 ✓ cosine ✓ ✓ ✓ ✓ -\\nHyperCLOV A (82B) 1024 - 6e-5 - cosine ✓ - - - ✓ - -\\nYuan 1.0 (245B) <10M 2048 1.6e-4 ✓ cosine decay to 10% ✓ - - - ✓ - -\\nGopher (280B) 3M 2048 4e-5 ✓ cosine decay to 10% ✓ ✓ - ✓ -\\nERNIE 3.0 Titan (260B) - 512 1e-4 ✓ linear ✓ ✓ ✓ ✓ -\\nGPT-NeoX-20B 1538 2048 0.97e-5 ✓ cosine ✓ ✓ ✓ ✓ ×\\nOPT (175B) 2M 2048 1.2e-4 - linear ✓ ✓ ✓ ✓ ✓\\nBLOOM (176B) 2048 2048 6e-5 ✓ cosine ✓ ✓ ✓ ✓ ×\\nGalactica (120B) 2M 2048 7e-6 ✓ linear decay to 10% ✓ - - - ✓ ✓ ✓\\nGLaM (1.2T) 1M 1024 0.01 - inverse square root✓ FP32+✓ - ✓ ×\\nLaMDA (137B) 256K - - - - - - - - - - - - -\\nMT-NLG (530B) 1920 2048 5e-5 ✓ cosine decay to 10% ✓ ✓ ✓ ✓ -\\nAlphaCode (41B) 2048 1536+768 1e-4 ✓ cosine decay to 10% ✓ ✓ ✓ ✓ -\\nChinchilla (70B) 1.5M 2048 1e-4 ✓ cosine decay to 10% ✓ ✓ - - -\\nPaLM (540B) 2048 2048 0.01 - inverse square root✓ - - - ✓ ✓ ×\\nAlexaTM (20B) 2M 1024 1e-4 - linear decay to 5% ✓ ✓ ✓ - ✓\\nU-PaLM (540B) 32 2048 1e-4 - cosine ✓ - - - - - -\\nUL2 (20B) 1024 1024 - - inverse square root- - - - - - × - -\\nGLM (130B) 4224 2048 8e-5 ✓ cosine ✓ ✓ ✓ ✓ ✓\\nCodeGen (16B) 2M 2048 5e-5 ✓ cosine ✓ - - - ✓ ✓ -\\nLLaMA (65B) 4M Tokens 2048 1.5e-4 ✓ cosine decay to 10% ✓ - - - ✓ ✓ -\\nPanGu-Σ(1.085T) 512 1024 2e-5 ✓ - ✓ ✓ - - -\\nBloombergGPT (50B) 2048 2048 6e-5 ✓ cosine ✓ ✓ ✓ ✓ ×\\nXuan Yuan 2.0 (176B) 2048 2048 6e-5 ✓ cosine ✓ ✓ ✓ ✓ -\\nCodeT5+(16B) 2048 1024 2e-4 - linear ✓ ✓ ✓ - -\\nStarCoder (15.5B) 512 8k 3e-4 ✓ cosine ✓ ✓ ✓ - -\\nLLaMA-2 (70B) 4M Tokens 4k 1.5e-4 ✓ cosine ✓ ✓ ✓ ✓ -\\nLLaMA-3.1 (405B) 16M 8192 8e-5 ✓ linear+cosine ✓ ✓ - - -\\nNemotron-4 (340B) 2304 4096 - - linear - - - ✓ - - ×\\nDeepSeek (67B) 4608 4096 3.2e-4 ✓ cosine ✓ ✓ ✓ ✓ -\\nDeepSeek-v2 (67B) 9216 4k 2.4e-4 ✓ step-decay ✓ - - - ✓ ✓ -\\nTable 7: Summary of optimization settings used for instruction-tuned LLMs. Values for gradient clipping and dropout are the same as the pre-trained models, while\\nno model uses weight decay for instruction tuning.\\nSequence Optimizers Grad\\nModels Batch Size Length LR Warmup LR_Decay AdaFactorAdam AdamW Clip Dropout\\nWebGPT (175B) BC:512, RM:32 - 6e-5 - - ✓ - -\\nT0 (11B) 1024 1280 1e-3 - - ✓ - ✓\\nTk-Instruct (11B) 1024 - 1e-5 - constant - - - - -\\nOPT-IML (175B) 128 2048 5e-5 × linear ✓ ✓ ✓\\nFlan-U-PaLM (540B) 32 - 1e-3 - constant ✓ - ✓\\nSparrow (70B) RM: 8+16, RL:16 - 2e-6 ✓ cosine decay to 10% ✓ ✓ ×\\nWizardCoder (15B) 512 2048 2e-5 ✓ cosine - - - - -\\nAlpaca (13B) 128 512 1e-5 ✓ cosine - - ✓ ✓ ×\\nVicuna (13B) 128 -2048 2e-5 ✓ cosine ✓ - ×\\nLIMA (65B) 32 2048 1e-5 × linear ✓ - ✓\\nsystems. It includes a variety of tasks that test a wide range of\\nlinguistic phenomena, making it a comprehensive tool for eval-\\nuating language understanding in AI.\\n5.2.2. Language Understanding\\nWinoGrande [354]: A large-scale dataset inspired by the orig-\\ninal Winograd [357] Schema Challenge tests models on their\\nability to resolve pronoun ambiguity and encourages the devel-\\nopment of models that understand the broad context in natural\\nlanguage text.\\nCoQA [316]: A conversational question-answering dataset,\\nCoQA challenges models with questions that rely on conver-\\nsation history and require free-form text answers. Its diverse\\ncontent from seven domains makes it a rigorous test for mod-\\nels’ ability to handle a wide range of topics and conversational\\ncontexts.\\nWiC [317]: This dataset assesses a model’s ability to dis-\\ncern word meanings based on context, aiding in tasks related\\n27'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 27, 'page_label': '28', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='Table 8: Details of various well-known pre-training and fine-tuning datasets. Here, alignment means aligning with human preferences.\\nDataset Type Size/Samples Tasks Source Creation Comments\\nC4 [10] Pretrain 806GB - Common Crawl AutomatedA clean, multilingual dataset with billions\\nof tokens\\nmC4 [11] Pretrain 38.49TB - Common Crawl AutomatedA multilingual extension of the C4\\ndataset, mC4 identifies over 100 lan-\\nguages using cld3 from 71 monthly web\\nscrapes of Common Crawl.\\nPILE [301] Pretrain 825GB -\\nCommon Crawl, PubMed Central,\\nOpenWebText2, ArXiv, GitHub,\\nBooks3, and others\\nAutomatedA massive dataset comprised of 22 con-\\nstituent sub-datasets\\nROOTs [302] Pretrain 1.61TB - 498 Hugging Face datasetsAutomated46 natural and 13 programming lan-\\nguages\\nMassiveText [116] Pretrain 10.5TB - MassiveWeb, Books, News,\\nWikipedia, Github, C4 Automated99% of the data is in English\\nWikipedia [303] Pretrain - - Wikipedia AutomatedDump of wikipedia\\nRedPajama [304] Pretrain 5TB - CommonCrawl, C4, Wikipedia,\\nGithub, Books, StackExchangeAutomatedOpen-source replica of LLaMA dataset\\nPushShift.io Reddit Pretrain 21.1GB - Reddit AutomatedSubmissions and comments on Reddit\\nfrom 2005 to 2019\\nBigPython [140] Pretrain 5.5TB Coding GitHub Automated-\\nPool of Prompt (P3) [17] Instructions 12M 62 PromptSource Manual A Subset of PromptSource, created from\\n177 datasets including summarization,\\nQA, classification, etc.\\nxP3 [154] Instructions 81M 71 P3+Multilingual datasets Manual Extending P3 to total 46 languages\\nSuper-NaturalInstructions (SNI) [18]Instructions 12.4M 1616 Multiple datasets Manual Extending P3 with additional multi-\\nlingual datasets, total 46 languages\\nFlan [16] Instructions 15M 1836 Muffin+T0-SF+NIV2 Manual Total 60 languages\\nOPT-IML [97] Instructions 18.1M 1667 - Manual -\\nSelf-Instruct [19] Instructions 82k 175 - AutomatedGenerated 52k instructions with 82k sam-\\nples from 175 seed tasks using GPT-3\\nAlpaca [158] Instructions 52k - - AutomatedEmployed self-instruct method to gener-\\nate data from text-davinci-003\\nVicuna [159] Instructions 125k - ShareGPT AutomatedConversations shared by users on\\nShareGPT using public APIs\\nLLaMA-GPT-4 [160] Instructions 52k - Alpaca AutomatedRecreated Alpaca dataset with GPT-4 in\\nEnglish and Chinese\\nUnnatural Instructions [305] Instructions 68k - 15-Seeds (SNI) Automated-\\nLIMA [185] Instructions 1k - Multiple datasets Manual Carefully created samples to test perfor-\\nmance with fine-tuning on less data\\nAnthropic-HH-RLHF [306] Alignment 142k - - Manual\\nAnthropic-HH-RLHF-2 [178]Alignment 39k - - Manual\\nto Word Sense Disambiguation.\\nWikitext103 [318]: With over 100 million tokens from\\nWikipedia’s top articles, this dataset is a rich resource for tasks\\nthat require understanding long-term dependencies, such as lan-\\nguage modeling and translation.\\nPG19 [319]: This is a digital library of diverse books from\\nProject Gutenberg. It is specifically designed to facilitate re-\\nsearch in unsupervised learning and language modeling, with a\\nspecial focus on long-form content.\\nC4 [10]: A clean, multilingual dataset, C4 offers billions of to-\\nkens from web-crawled data. It is a comprehensive resource for\\ntraining advanced Transformer models on various languages.\\nLCQMC [320]: The Large-scale Chinese Question Matching\\nCorpus (LCQMC) is a dataset for evaluating the performance\\nof models in semantic matching tasks. It contains pairs of ques-\\ntions in Chinese and their matching status, making it a valuable\\nresource for research in Chinese language understanding.\\n5.2.3. Story Cloze and Sentence Completion\\nStoryCloze [334]: It introduces a new “StoryCloze Test”, a\\ncommonsense reasoning framework for evaluating story under-\\nstanding, generation, and script learning. It considers a model’s\\nability to understand and generate coherent and sensible stories.\\nLAMBADA [335]: This dataset evaluates contextual text un-\\nderstanding through a word prediction task. Models must pre-\\ndict the last word of a passage, which is easy for humans when\\ngiven the whole passage, but not when given only the last sen-\\ntence.\\n5.2.4. Physical Knowledge and World Understanding\\nPIQA [340]: A dataset that probes the physical knowledge of\\nmodels, aiming to understand how well they are learning about\\nthe real world.\\nTriviaQA [341]: A dataset that tests models on reading com-\\nprehension and open domain question answering (QA) tasks,\\nwith a focus on Information Retrieval (IR)-style QA.\\nARC [342]: A larger version of the ARC-Challenge, this\\ndataset contains both easy and challenging grade-school level,\\nmultiple-choice science questions. It is a comprehensive test of\\na model’s ability to understand and answer complex questions.\\nARC-Easy [342]: A subset of the ARC dataset, ARC-\\nEasy, contains questions that are answered correctly by either\\na retrieval-based algorithm or a word co-occurrence algorithm.\\n28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 28, 'page_label': '29', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='Table 9: Categorized evaluation datasets used in evaluating LLMs.\\nType Datasets/Benchmarks\\nMulti-Task MMLU [307], SuperGLUE [2], BIG-bench [308], GLUE [309], BBH [308], CUGE [310], Zero-\\nCLUE [311], FewCLUE [312], Blended Skill Talk [313], HELM [314], KLUE-STS [315]\\nLanguage Understanding CoQA [316], WiC [317], Wikitext103 [318], PG19 [319], LCQMC [320], QQP [321], WinoGender [322],\\nCB [323], FinRE [324], SanWen [325], AFQMC [311], BQ Corpus [326], CNSS [327], CKBQA 13 [328],\\nCLUENER [311], Weibo [329], AQuA [330], OntoNotes [331], HeadQA [332], Twitter Dataset [333]\\nStory Cloze and\\nSentence Completion StoryCloze [334], LAMBADA [335], LCSTS [336], AdGen [337], E2E [338], CHID [339], CHID-\\nFC [312]\\nPhysical Knowledge and\\nWorld Understanding PIQA [340], TriviaQA [341], ARC [342], ARC-Easy [342], ARC-Challenge [342], PROST [343], Open-\\nBookQA [344], WebNLG [345], DogWhistle Insider & Outsider [346]\\nContextual Language\\nUnderstanding\\nRACE [347], RACE-Middle [347], RACE-High [347], QuAC [348], StrategyQA [349], Quiz Bowl [350],\\ncMedQA [351],cMedQA2 [352], MATINF-QA [353]\\nCommonsense Reasoning WinoGrande [354], HellaSwag [355], COPA [356], WSC [357], CSQA [358], SIQA [359], C3 [360],\\nCLUEWSC2020 [311], CLUEWSC [311], CLUEWSC-FC [312], ReCoRD [361]\\nReading Comprehension SQuAD [362], BoolQ [363], SQUADv2 [364], DROP [365], RTE [366], WebQA [367], CMRC2017 [368],\\nCMRC2018 [369], CMRC2019 [370], COTE-BD [371], COTE-DP [371], COTE-MFW [371], Mul-\\ntiRC [372], Natural Questions [373], CNSE [327], DRCD [374], DuReader [375], Dureaderrobust [376],\\nDuReader-QG [375], SciQ [377], Sogou-log [378], Dureaderrobust-QG [376], QA4MRE [379], KorQuAD\\n1.0 [380], CAIL2018-Task1 & Task2 [381]\\nMathematical Reasoning MATH [382], Math23k [383], GSM8K [384], MathQA [385], MGSM [386], MultiArith [387], AS-\\nDiv [388], MAWPS [389], SV AMP [390]\\nProblem Solving HumanEval [141], DS-1000 [391], MBPP [392], APPS [382], CodeContests [142]\\nNatural Language Inference\\n& Logical Reasoning ANLI [393], MNLI-m [394], MNLI-mm [394],QNLI [362], WNLI [357], OCNLI [311], CMNLI [311],\\nANLI R1 [393], ANLI R2 [393], ANLI R3 [393], HANS [395], OCNLI-FC [312], LogiQA [396], Strate-\\ngyQA [349]\\nCross-Lingual Understanding MLQA [397], XNLI [398], PAWS-X [399], XSum [400], XCOPA [401], XWinograd [402], TyDiQA-\\nGoldP [403], MLSum [404]\\nTruthfulness and Fact CheckingTruthfulQA [405], MultiFC [406], Fact Checking on Fever [407]\\nBiases and Ethics in AI ETHOS [408], StereoSet [409], BBQ [410], Winobias [411], CrowS-Pairs [412]\\nToxicity RealToxicityPrompts [413], CivilComments toxicity classification [414]\\nLanguage Translation WMT [415], WMT20 [416], WMT20-enzh [416], EPRSTMT [312], CCPM [417]\\nScientific Knowledge AminoProbe [148], BioLAMA [148], Chemical Reactions [148], Galaxy Clusters [148], Mineral\\nGroups [148]\\nDialogue Wizard of Wikipedia [418], Empathetic Dialogues [419], DPC-generated [96] dialogues, ConvAI2 [420],\\nKdConv [421]\\nTopic Classification TNEWS-FC [312], YNAT [315], KLUE-TC [315], CSL [311], CSL-FC [312], IFLYTEK [422]\\nIt is a great starting point for models beginning to explore ad-\\nvanced question-answering.\\nARC-Challenge [342]: A rigorous question-answering\\ndataset, ARC-Challenge includes complex, grade-school level\\nquestions that demand reasoning beyond simple retrieval, test-\\ning the true comprehension capabilities of models.\\n5.2.5. Contextual Language Understanding\\nRACE [347]: The RACE dataset is a reading comprehension\\ndataset collected from English examinations in China, which\\nbenchmarks AI models for understanding and answering ques-\\ntions on long and complex passages, simulating the challenge\\nof a real-world examination.\\nRACE-Middle [347]: Another subset of the RACE [347]\\ndataset, RACE-Middle, contains middle school-level English\\nexam questions. It offers a slightly less challenging but academ-\\nically oriented evaluation of a model’s comprehension skills.\\nRACE-High [347]: A subset of the RACE [347] dataset,\\nRACE-High consists of high school-level English exam ques-\\ntions. It is designed to evaluate the comprehension ability of\\nmodels in a more academic and challenging context.\\nQuAC [348]: This dataset simulates an information-seeking\\ndialog between students and teachers using hidden Wikipedia\\ntext. It introduces unique challenges not found in machine com-\\nprehension datasets, making it a valuable resource for advanc-\\ning dialog systems.\\n5.2.6. Commonsense Reasoning\\nHellaSwag [355]: A dataset that challenges models to pick the\\nbest ending to a context uses Adversarial Filtering to create a\\n‘Goldilocks’ zone of complexity, where generated text is absurd\\nto humans but often misclassified by models.\\nCOPA [401]: This dataset evaluates a model’s progress in\\nopen-domain commonsense causal reasoning. Each question\\ncomprises a premise and two alternatives, and the model must\\nselect the more plausible alternative, testing a model’s ability to\\nunderstand and reason about cause and effect.\\nWSC [357]: The Winograd Schema Challenge (WSC) is a\\n29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 29, 'page_label': '30', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='Table 10: An illustration of training datasets and evaluation tasks employed by pre-trained LLMs. Here, “QA” is question-answering, “Clf” is classification, “NLI”\\nis natural language inference, “MT” is machine translation, “RC” is reading comprehension, “CR” is commonsense reasoning, “MR” is mathematical reasoning,\\n“Mem.” is memorization.\\nBenchmark\\nModels Training Dataset BIG-\\nbenchMMLUSuper\\nGLUEQA Clf NLI MT Cloze/\\nCompletionRC CR MR Coding\\nTruthful/\\nBias/\\nToxicity/\\nMem.\\nT5 C4 [10] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓\\nGPT-3 Common Crawl, WebText, Books Cor-\\npora, Wikipedia\\n✓ ✓ ✓ ✓ ✓ ✓\\nmT5 mC4 [11] ✓ ✓ ✓\\nPanGu-α 1.1TB Chinese Text Corpus ✓ ✓ ✓ ✓ ✓\\nCPM-2 WuDaoCorpus [109] ✓ ✓\\nCodex 54 million public repositories from Github ✓\\nERNIE-3.0 Chinese text corpora, Baidu Search, Web\\ntext, QA-long, QA-short, Poetry and Cou-\\nplet Domain-specific data from medical,\\nlaw, and financial area Baidu knowledge\\ngraph with more than 50 million facts\\n✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓\\nJurassic-1 Wikipedia, OWT, Books, C4, Pile [301],\\narXiv, GitHub\\n✓ ✓ ✓ ✓\\nHyperCLOV AKorean blogs, Community sites, News,\\nKiN Korean Wikipedia, Wikipedia (En-\\nglish and Japanese), Modu-Corpus: Mes-\\nsenger, News, Spoken and written lan-\\nguage corpus, Web corpus\\n✓\\nYuan 1.0 Common Crawl, SogouT, Sogou News,\\nBaidu Baike, Wikipedia, Books\\n✓ ✓ ✓ ✓\\nGopher subsets of MassiveWeb Books, C4, News,\\nGitHub and Wikipedia samples from Mas-\\nsiveText\\n✓ ✓ ✓ ✓ ✓ ✓ ✓\\nERNIE-3.0 TITANSame as ERNIE 3.0 and ERNIE 3.0 ad-\\nversarial dataset, ERNIE 3.0 controllable\\ndataset\\n✓ ✓ ✓ ✓ ✓\\nGPT-NeoX-20BPile [301] ✓ ✓ ✓ ✓ ✓ ✓\\nOPT RoBERTa [299], Pile [301], PushShift.io\\nReddit [423]\\n✓ ✓ ✓ ✓\\nBLOOM ROOTs [13] ✓ ✓ ✓ ✓ ✓ ✓\\nGalactica arXiv, PMC, Semantic Scholar, Wikipedia,\\nStackExchange, LibreText, Open Text-\\nbooks, RefSeq Genome, OEIS, LIPID\\nMAPS, NASAExoplanet, Common Crawl,\\nScientificCC, AcademicCC, GitHub repos-\\nitories Khan Problems, GSM8K, OneS-\\nmallStep\\n✓ ✓ ✓ ✓ ✓\\nGLaM Filtered Webpages, Social media conversa-\\ntions Wikipedia, Forums, Books, News\\n✓ ✓ ✓ ✓ ✓\\nLaMDA Infiniset : Public documents, Dialogs, Ut-\\nterances\\n✓\\nMT-NLG Two snapshots of Common Crawl and\\nBooks3, OpenWebText2, Stack Exchange,\\nPubMed Abstracts, Wikipedia, PG-19\\n[242], BookCorpus2, NIH ExPorter, Pile,\\nCC-Stories, RealNews\\n✓ ✓ ✓ ✓ ✓\\nAlphaCode Selected GitHub repositories, CodeCon-\\ntests: Codeforces, Description2Code, Co-\\ndeNet\\n✓\\nChinchilla MassiveWeb, MassiveText Books, C4,\\nNews, GitHub, Wikipedia\\n✓ ✓ ✓ ✓ ✓ ✓\\nPaLM webpages, books, Wikipedia, news, arti-\\ncles, source code, social media conversa-\\ntions\\n✓ ✓ ✓ ✓ ✓ ✓\\nAlexaTM Wikipedia, mC4 ✓ ✓ ✓ ✓ ✓\\nU-PaLM Same as PaLM ✓ ✓ ✓ ✓ ✓ ✓ ✓\\nUL2 - ✓ ✓ ✓ ✓ ✓ ✓\\nGLM-130B - ✓ ✓ ✓\\nCodeGen Pile, BigQuery, BigPython ✓\\nLLaMA CommonCrawl, C4, Github, Wikipedia,\\nBooks, arXiv, StackExchange\\n✓ ✓ ✓ ✓ ✓ ✓ ✓\\nPanGu-Σ WuDaoCorpora, CLUE, Pile, C4, Python\\ncode\\n✓ ✓ ✓ ✓ ✓ ✓\\nBloombergGPTinPile, Pile, C4, Wikipedia ✓ ✓ ✓ ✓ ✓ ✓ ✓\\nCodeT5+ CodeSearchNet, Github Code ✓ ✓\\nStarCoder The Stack v1.2 ✓ ✓ ✓ ✓\\nLLaMA-2 ✓ ✓ ✓ ✓ ✓ ✓ ✓\\nPaLM-2 Web documents, Code, Books, Maths,\\nConversation\\n✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓\\n30'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 30, 'page_label': '31', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='Table 11: An illustration of training datasets and evaluation benchmarks used in fine-tuned LLMs. “SNI” is a short of Super-NaturalInsturctions.\\nModels Training Dataset BIG-\\nbench MMLUBBH RAFTFLANSNI PromptSourceTyDiQAHumanEvalMBPP\\nTruthful/\\nBias/\\nToxicity\\nT0 Pool of Prompts ✓\\nWebGPT ELI5 [424], ELI5 fact-\\ncheck [166], TriviaQA [341],\\nARC-Challenge [342], ARC-\\nEasy [342], Hand-written data,\\nDemonstrations of humans, Com-\\nparisons between model-generated\\nanswers\\n✓\\nTk-INSTRUCTSNI [18] ✓\\nmT0 xP3 [154]\\nOPT-IML PromptSource [17], FLAN [16],\\nSNI [425], UnifiedSKG [426],\\nCrossFit [427], ExMix [428],\\nT5 [10], Reasoning\\n✓ ✓ ✓ ✓ ✓ ✓\\nFlan Muffin, T0-SF, NIv2, CoT ✓ ✓ ✓\\nWizardCoderCode Alpaca ✓ ✓\\nreading comprehension task in which a system must resolve\\nreferences in a text, often requiring world knowledge and rea-\\nsoning about the text.\\nCSQA [358]: The CommonsenseQA is a question-answering\\ndataset that requires commonsense knowledge to evaluate the\\nability of AI models to understand and answer questions.\\n5.2.7. Reading Comprehension\\nBoolQ [363]: A dataset derived from Google search queries,\\nBoolQ challenges models to answer binary (yes /no) questions.\\nThe questions are naturally occurring and are paired with a\\nparagraph from a Wikipedia article containing the answer. It\\nis a test of reading comprehension and reasoning.\\nSQUADv2 [364]: The Stanford Question Answering Dataset\\n(SQuAD) [362] is a collection of questions posed by crowd\\nworkers on a set of Wikipedia articles, where the answer to ev-\\nery question is a segment of text from the corresponding reading\\npassage. SQuADv2 combines the original SQuAD1.1 dataset\\nwith over 50,000 unanswerable questions. The aim is to evalu-\\nate a model’s ability to understand and answer questions based\\non a given context and to determine when a question is unan-\\nswerable.\\nDROP [365]: DROP, or Discrete Reasoning Over the con-\\ntent of Paragraphs, is designed to test a model’s ability to un-\\nderstand a wide variety of reading phenomena. It encourages\\ncomprehensive and reliable evaluation of reading comprehen-\\nsion capabilities.\\nRTE [366]: The Recognizing Textual Entailment (RTE)\\ndatasets come from a series of annual competitions on textual\\nentailment, predicting whether a given sentence logically fol-\\nlows from another and evaluating a model’s understanding of\\nlogical relationships in a text.\\nWebQA [367]: A dataset for open-domain question answering,\\nWebQA offers a large collection of web-based question-answer\\npairs. It is designed to assess the ability of AI models to under-\\nstand and answer questions based on web content.\\nCMRC2018 [369]: This dataset is a test of Chinese language\\nmodels’ ability to reason comprehensively and is designed with\\na challenging span-extraction format that pushes the boundaries\\nof machine performance.\\n5.2.8. Mathematical Reasoning\\nMATH [382]: This dataset is a platform for evaluating the\\nmathematical problem-solving abilities of AI models. It con-\\ntains a diverse set of math problems, ranging from arithmetic\\nto calculus, and is designed to test the model’s ability to under-\\nstand and solve complex mathematical problems.\\nMath23k [383]: This one challenges a model’s ability to un-\\nderstand and solve mathematical word problems. It contains\\n23,000 Chinese arithmetic word problems that require models\\nto perform reasoning and computation based on the problem\\ndescription.\\nGSM8K [384]: A dataset of diverse grade school math word\\nproblems, testing a model’s ability to perform multi-step math-\\nematical reasoning.\\n5.2.9. Problem Solving and Logical Reasoning\\nANLI [393]: A large-scale dataset designed to test the robust-\\nness of machine learning models in Natural Language Inference\\n(NLI) is created through an iterative, adversarial process where\\nhumans try to generate examples that models cannot correctly\\nclassify.\\nHumanEval [141]: A dataset for evaluating the problem-\\nsolving ability of AI models, which includes a diverse set of\\ntasks that require various cognitive abilities, making it a com-\\nprehensive tool for assessing general intelligence in AI.\\nStrategyQA [349]: A question-answering dataset that re-\\nquires reasoning over multiple pieces of evidence to evaluate\\nthe strategic reasoning ability of AI models, pushing the bound-\\naries of what machines can understand and answer.\\n5.2.10. Cross-Lingual Understanding\\nXNLI [398]: A cross-lingual benchmark, XNLI extends the\\nMultiNLI [429] corpus to 15 languages, including low-resource\\nones like Urdu. It tests models on cross-lingual sentence under-\\nstanding, with 112,500 annotated pairs across three categories:\\nentailment, contradiction, and neutral.\\nPAWS-X [399]:PAWS-X, or Cross-lingual Paraphrase Adver-\\nsaries from Word Scrambling, is a multilingual version of the\\n31'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 31, 'page_label': '32', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='PAWS [430] dataset for paraphrase identification. It includes\\nexamples in seven languages and is designed to evaluate the\\nperformance of cross-lingual paraphrase identification models.\\n5.2.11. Truthfulness\\nTruthful-QA [405]: A unique benchmark that measures a\\nlanguage model’s truthfulness when generating answers. The\\ndataset includes questions across various categories like health,\\nlaw, and politics, some designed to test the model against com-\\nmon human misconceptions.\\n5.2.12. Biases and Ethics in AI\\nETHOS [408]: ETHOS is a hate speech detection dataset\\nbuilt from YouTube and Reddit comments. It is a tool in the\\nfight against online hate speech, offering binary and multi-label\\nvariants for robust content moderation.\\nStereoSet [409]: StereoSet is a comprehensive dataset de-\\nsigned to measure and evaluate the presence of stereotypical\\nbiases in language models. It focuses on four key domains:\\ngender, profession, race, and religion. Contrasting stereotypi-\\ncal bias against language modeling ability provides a valuable\\ntool for understanding and mitigating biases in large language\\nmodels.\\n6. Applications\\nApplying Large Language Models (LLMs) to a variety of\\ndownstream tasks has become a popular trend in both AI-\\nrelated research communities and industries, with many emerg-\\ning uses being discovered and explored daily. LLMs, which are\\ncapable of understanding and generating human-like text, have\\nfound meaningful applications across a variety of fields. This\\nsection provides an overview of LLM applications in medicine,\\neducation, science, mathematics, law, finance, robotics, and\\ncoding. While each of these domains pose different challenges,\\nLLMs open up opportunities to make significant contributions\\nto these domains through their generalizability.\\nGeneral Purpose: LLMs are being widely considered as\\ngeneral-purpose tools for a wide variety of tasks [431]. This\\nis due to their inherent ability to understand, generate, and\\nmanipulate human-like text in a contextually relevant man-\\nner. This allows them to perform tasks ranging from simple\\nlanguage translation and question-answering to more complex\\ntasks like summarization, text generation, and even program-\\nming help [432]. The utility of LLMs is further enhanced by\\ntheir ability to adapt to the specific style and tone of the text\\nthey are processing, making the outputs more user-friendly and\\ncontext-aware. In everyday applications, LLMs can be used\\nas personal assistants, helping users draft emails or schedule\\nappointments [433]; they can also be deployed in customer ser-\\nvice to handle common questions or applied to generate content\\nfor digital platforms like websites by creating human-like text\\nbased on given prompts [434]. Moreover, LLMs play a cru-\\ncial role in data analysis, where they can filter large volumes of\\ntext data, summarize key points, and find patterns that would\\ntake humans much longer to identify [435]. Despite their wide-\\nranging applications, it is essential to remember that LLMs,\\nsimilar to any AI system, are only as good as the data they have\\nbeen trained on.\\nMedicine: The application of LLMs in the field of medicine is\\nreshaping healthcare delivery and research. For example, LLMs\\nare increasingly used in clinical decision support systems to\\nprovide physicians with evidence-based treatment recommen-\\ndations [436, 437, 438]. By analyzing patient data and medical\\nliterature, they can help identify potential diagnoses, suggest\\nappropriate tests, and recommend optimal treatment strategies.\\nMoreover, LLMs can also enhance patient interactions with\\nhealthcare systems; e.g., they can be used in chatbot applica-\\ntions [439, 440, 441] to answer patient queries about symptoms\\nor medications, schedule appointments, and even provide es-\\nsential health advice. For medical research, LLMs are used to\\nextract and filter information from a considerable amount of\\nmedical literature, identify relevant studies, summarize find-\\nings, and even predict future research trends [442, 443, 444].\\nFor medical education, LLMs can help create training mate-\\nrials, generate exam questions, provide detailed explanations\\nof complex medical topics, and o ffer personalized feedback to\\nstudents [445, 446, 447, 448]. They can also simulate patient\\ninteractions, enabling students to practice and improve their\\nclinical skills. At a broader level, LLMs can assist in public\\nhealth initiatives by analyzing media data to detect disease out-\\nbreaks, monitor public sentiment towards health policies, and\\ndisseminate health information in a clear and understandable\\nmanner [449]. LLMs can be employed to support public health\\ninitiatives, addressing related issues such as data privacy, the\\nnecessity for explainability, and the potential risk of propagat-\\ning biases [450, 451].\\nEducation: The integration of LLMs into the educational sec-\\ntor offers opportunities to enhance learning experiences, teacher\\nsupport, and educational content development. For students, by\\nanalyzing their learning styles, performance, and preferences,\\nLLMs can provide customized study materials and practice\\nquestions to develop personalized learning experiences [452].\\nFor teachers, LLMs can help to create lesson plans and grade\\nassignments and generate diverse and inclusive educational\\ncontent, significantly saving more time for teaching and student\\ninteraction [453, 454]. In language learning, LLMs serve as\\nadvanced conversational partners capable of simulating conver-\\nsations in multiple languages, correcting grammar, enhancing\\nvocabulary, and aiding pronunciation for the needs of fluency\\nin practice [455]. Furthermore, LLMs improve accessibility\\nin education by providing support for students with disabili-\\nties. They can generate real-time transcriptions for the hear-\\ning impaired, offer reading assistance for the visually impaired,\\nand simplify complex texts for those with learning disabili-\\nties [451]. As LLMs continue to evolve, their applications in\\neducation can benefit more students and teachers from different\\nperspectives in practice.\\nScience: Similar to medical applications, LLMs can expedite\\nthe research process by quickly analyzing and summarizing sci-\\nentific literature. By briefing comprehensible and accessible re-\\nsearch summaries, LLMs can assist researchers in staying up-\\nto-date with the latest findings, even in fields outside their area\\nof expertise [456, 457]. In addition, LLMs can aid scientists\\n32'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 32, 'page_label': '33', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='Table 12: Performance comparison of top performing LLMs across various NLU and NLG tasks. Here, “N-Shots” indicate the number of example prompts provided\\nto the model during the evaluation, representing its capability in few-shot or zero-shot learning settings, “f” represents the fine-tuned version, and “B” represents the\\nbenchmark.\\nTask Dataset/Benchmark Top-1 Top-2 Top-3\\nModel (Size)Score (N-shots)Model (Size)Score (N-shots) Model (Size) Score (N-shots)\\nMulti-Task BIG-bench (B) Chinchilla (70B)65.1 (5-shot) Gopher (280B)53.97 (5-shot) PaLM (540B) 53.7 (5-shot)\\nMMLU (B) GPT-4 (-) 86.4 (5-shot) Gemini (Ultra)83.7 (5-shot)Flan-PaLM-2(f) (Large) 81.2 (5-shot)\\nLanguage UnderstandingSuperGLUE (B) ERNIE 3.0 (12B) 90.6 (-) PaLM(f) (540B) 90.4 (-) T5 (11B) 88.9 (-)\\nStory Comprehension and\\nGeneration\\nHellaSwag GPT-4 (-) 95.3 (10-shot)Gemini (Ultra)87.8 (10-shot) PaLM-2 (Large) 86.8 (one shot)\\nStoryCloze GPT3 (175B)87.7 (few shot)PaLM-2 (Large)87.4 (one shot) OPT (175B) 79.82 (-)\\nPhysical Knowledge and\\nWorld Understanding\\nPIQA PaLM-2 (Large)85.0 (one shot)LLaMa (65B)82.8 (zero shot) MT-NLG (530B)81.99 (zero shot)\\nTriviaQA PaLM-2 (Large)86.1 (one shot)LLaMA-2 (70B)85.0 (one shot) PaLM (540B) 81.4 (one shot)\\nContextual Language\\nUnderstanding LAMBADA PaLM (540B)89.7 (few shot)MT-NLG (530B)87.15 (few shot) PaLM-2 (Large) 86.9 (one shot)\\nCommonsense ReasoningWinoGrande GPT-4 (-) 87.5 (5-shot)PaLM-2 (Large)83.0 (one shot) PaLM (540B) 81.1 (zero shot)\\nSIQA LLaMA (65B)52.3 (zero shot)Chinchilla (70B)51.3 (zero shot) Gopher (280B) 50.6 (zero shot)\\nReading ComprehensionBoolQ PaLM(f) (540B) 92.2 (-) T5 (11B) 91.2 (-) PaLM-2 (Large) 90.9 (one shot)\\nTruthfulness Truthful-QA LLaMA (65B) 57 (-)\\nMathematical ReasoningMATH Gemini (Ultra)53.2 (4-shot)PaLM-2 (Large)34.3 (4-shot) LLaMa-2 (65B) 13.5 (4-shot)\\nGSM8K GPT-4 (-) 92.0 (5-shot)PaLM-2 (Large)80.7 (8-shot) U-PaLM (540B) 58.5 (-)\\nProblem Solving and\\nLogical Reasoning HumanEval Gemini(f) (Ultra) 74.4 (zero shot) GPT-4 (-) 67.0 (zero shot)Code Llama (34B)48.8 (zero shot)\\nin formulating new hypotheses and research questions since\\ntheir ability to process large-scale datasets allows them to un-\\nveil insights that might not be immediately apparent to human\\nresearchers [458]. Moreover, for scientific writing, LLMs can\\nhelp researchers draft documents, suggest improvements, and\\nensure adherence to specific formatting guidelines [459, 460].\\nThis not only saves time but also improves the clarity of scien-\\ntific communication, enabling interdisciplinary teams to work\\ntogether more effectively.\\nMaths: In addition to providing mathematical research and\\neducation support, LLMs can assist in solving mathematical\\nproblems by giving step-by-step explanations and guiding users\\nthrough complex proofs and calculations. They can help iden-\\ntify errors in reasoning or computation and suggest corrections,\\nserving as an invaluable tool for both learning and verification\\npurposes [461, 462]. LLMs can be employed to check the valid-\\nity of mathematical proofs, o ffering a preliminary filter before\\nhuman review. While they are not a substitute for the meticu-\\nlous work of mathematicians, they can help simplify the process\\nof proof verification [463, 464]. Moreover, LLMs enhance ac-\\ncessibility to mathematics by translating complex concepts and\\nfindings into understandable language for non-specialists [465],\\nwhere the gap between theoretical mathematics and applied\\ncontexts such as physics, engineering, and economics can be\\nbridged.\\nLaw: LLMs can assist with the thematic analysis of legal doc-\\numents, including generating initial coding for datasets, iden-\\ntifying themes, and classifying data according to these themes.\\nThis collaborative e ffort between legal experts and LLMs has\\nproved to be e ffective in analyzing legal texts such as court\\nopinions on theft, improving both the e fficiency and quality of\\nthe research [466]. Additionally, LLMs have been evaluated for\\ntheir ability to generate explanations of legal terms, focusing\\non improving factual accuracy and relevance by incorporating\\nsentences from case law. By feeding relevant case law into the\\nLLM, the augmented models can generate higher-quality expla-\\nnations with less factually incorrect information [467]. More-\\nover, LLMs can be trained with specialized domain knowledge\\nto perform legal reasoning tasks [468] and answer legal ques-\\ntions [469].\\nFinance: LLMs like BloombergGPT [151], trained on exten-\\nsive proprietary financial datasets, exhibit superior performance\\non financial tasks. This indicates the value of domain-specific\\ntraining in creating LLMs that can more accurately understand\\nand process industry-specific language and concepts. The intro-\\nduction of FinGPT [470] as an open-source model offers trans-\\nparent and accessible resources to develop novel applications\\nsuch as robo-advising, algorithmic trading, and low-code so-\\nlutions, ultimately expanding the capabilities of financial ser-\\nvices. Both BloombergGPT and FinGPT show the adaptabil-\\nity of LLMs to the financial domain, with the former showing\\nthe power of custom datasets and the latter emphasizing a data-\\ncentric approach and low-rank adaptation techniques for cus-\\ntomization. Moreover, LLMs demonstrate an ability to break\\ndown complex financial tasks into actionable plans, enabling\\nend-to-end solutions that were previously unfeasible with a sin-\\ngle model [471].\\nRobotics: In robotics research, LLMs have promising appli-\\ncations, such as enhancing human-robot interaction [28, 472,\\n473, 474], task planning [237], motion planning [246], nav-\\nigation [246, 475], object manipulation [236], personalized\\nrobots [476], etc. LLMs enable robots to understand the en-\\nvironment effectively and generate plans to complete tasks col-\\nlaboratively [240, 26]. They can facilitate continuous learning\\nby allowing robots to access and integrate information from a\\nwide range of sources, helping robots acquire new skills, adapt\\nto changes, and refine their paths [224, 233, 234].\\n7. Challenges and Future Directions\\nLLMs such as GPT-4 and its predecessors have significantly\\nadvanced natural language processing. Nevertheless, they also\\nbring along a set of challenges. The computational cost, ad-\\nversarial robustness, and interpretability are among the tech-\\nnical challenges that are intrinsic to these models. Further-\\nmore, as these models are scaled up to handle more complex\\n33'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 33, 'page_label': '34', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='tasks or to operate in more complex or dynamic environments,\\nnew challenges in scalability, privacy, and real-time processing\\nemerge. On the frontier of foundational research, integrating\\nmulti-modality and the effectiveness of transfer learning are be-\\ning keenly explored. Additionally, the continuous learning as-\\npect of these models, which aims to have models that can adapt\\nto new information over time, presents a fresh set of challenges.\\nThese challenges not only underscore the technical intricacies\\ninvolved but also highlight the broader impact and the future\\ntrajectory of LLMs in real-world applications. The following\\nsections delve into these challenges, shedding light on the on-\\ngoing and potential efforts to address them.\\nComputational Cost: Training LLMs require extensive compu-\\ntational resources, which increases production costs and raises\\nenvironmental concerns due to substantial energy consump-\\ntion during large-scale training. Improved performance occurs\\nas computational resources increase, but the rate of improve-\\nment gradually decreases when both the model and dataset\\nsize remain fixed, following the power law of diminishing re-\\nturns [477].\\nBias and Fairness: LLMs can inherit and amplify societal bi-\\nases in their training data. These biases can manifest in the\\nmodel’s outputs, leading to potential ethical and fairness is-\\nsues [478].\\nOverfitting: Although LLMs possess substantial learning ca-\\npabilities, they are susceptible to overfitting noisy and peculiar\\npatterns within their extensive training data. Consequently, this\\nmay cause them to generate illogical responses [479]. The de-\\nbate about Memorization vs. Generalization in LLMs is about\\nfinding the right balance. Memorization allows the model to\\nremember specific details from its training data, ensuring it can\\nprovide accurate answers to precise questions. However, gen-\\neralization enables the model to make inferences and produce\\nresponses for inputs it has not seen before, which is essential\\nfor handling various real-world tasks. Striking the right bal-\\nance is the challenge: too much memorization can lead to over-\\nfitting, making the model inflexible and struggling with new\\ninputs [480].\\nEconomic and Research Inequality: The high cost of train-\\ning and deploying LLMs may make their development concen-\\ntrated within well-funded organizations, potentially worsening\\neconomic and research inequalities in AI [481].\\nReasoning and Planning: Some reasoning and planning tasks,\\neven as seemingly simple as common-sense planning, which\\nhumans find easy, remain well beyond the current capabilities\\nof LLMs evaluated using an assessment framework. This is not\\nentirely unexpected, considering that LLMs primarily generate\\ntext completions based on likelihood and offer no solid guaran-\\ntees in terms of reasoning abilities [482].\\nHallucinations: LLMs exhibit “hallucinations\", where they\\ngenerate responses that, while sounding plausible, are incorrect\\nor do not align with the provided information [483]. Hallucina-\\ntions can be categorized into three categories.\\n• Input-conflicting hallucination, wherein LLMs produce\\ncontent that diverges from the input given by users.\\n• Context-conflicting hallucination, where LLMs generate\\ncontent that contradicts information they have generated\\nearlier.\\n• Fact-conflicting hallucination involves LLM’s generation\\nof content that does not align with established world\\nknowledge.\\nPrompt Engineering: Prompts serve as inputs to LLMs, and\\ntheir syntax and semantics play a crucial role in determining\\nthe model’s output. The prompt variations, sometimes counter-\\nintuitive to humans, can result in significant changes in model\\noutput and are addressed through prompt engineering, which\\ninvolves designing natural language queries to guide LLMs\\nresponses effectively [484, 32].\\nLimited Knowledge: Information acquired during pretraining\\nis limited and may become obsolete after some time. Re-\\ntraining the model using updated data is costly. To generate\\nfactually accurate responses, people use a retrieval augmen-\\ntation pipeline [198]. However, pre-trained models are not\\ntrained with retrieval augmentation generation (RAG) [6, 21];\\nhence, adapting the training pipeline is necessary [193, 25].\\nSafety and Controllability: Using LLMs comes with the risk\\nof generating harmful, misleading, or inappropriate content,\\nwhether by accident or when given specific prompts. Ensuring\\nthese models are safely utilized is a significant concern [485].\\nSecurity and Privacy: LLMs are prone to leaking personal\\ninformation and generating false, unethical, misaligned re-\\nsponses. Researchers have explored various security attacks,\\ni.e., backdoor attacks, jailbreaking, prompt injection, and data\\npoisoning, that lead to breaking LLMs security. Therefore,\\ndeveloping better defense mechanisms is essential to ensure\\nLLMs are safe, reliable, and trustworthy for complex AI\\napplications [486].\\nMulti-Modality: Multi-modal learning, where LLMs are\\ntrained on diverse data like text, images, and videos, aims to\\ncreate models with richer understanding but faces challenges\\nin data alignment, fusion strategies, and higher computational\\ndemands.\\nCatastrophic Forgetting: LLMs are often pre-trained on\\nlarge datasets and then fine-tuned on domain-specific data,\\nreducing training resources. However, they face issues like\\ndomain adaptation and catastrophic forgetting, which hinder\\nthe retention of original knowledge when learning new tasks.\\nAdversarial Robustness: Large Language Models (LLMs)\\nhave shown great capabilities in various tasks but are vul-\\nnerable to adversarial attacks, where slight, deliberate input\\nalterations can mislead them. Especially with models like\\nBERT, adversarial fine-tuning can enhance robustness, al-\\nthough it sometimes compromises generalization [487]. As\\nLLMs integrate more into complex systems, examining their\\nsecurity properties becomes crucial, given the emerging field\\nof adversarial attacks on LLMs within trustworthy ML [488].\\nThis vulnerability is notable in safety-critical domains, ne-\\ncessitating robust adversarial evaluation tools to ensure LLM\\nreliability [489].\\nInterpretability and Explainability: The “black-box” nature\\nof LLMs poses challenges in understanding their decision-\\nmaking, which is crucial for broader acceptance and trust,\\n34'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 34, 'page_label': '35', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='especially in sensitive domains. Despite their advanced\\ncapabilities, the lack of insight into their operation limits their\\neffectiveness and trustworthiness [490, 491]. E fforts are being\\nmade to make LLMs more explainable to promote user trust\\nand to ensure responsible AI usage. Understanding the logic\\nbehind LLMs’ responses is essential for fostering trust and\\nensuring they align with human values and legal standards.\\nPrivacy Concerns: Privacy concerns in Large Language\\nModels (LLMs) have escalated with their growth in complexity\\nand size, particularly around data sharing and potential misuse.\\nThere is a risk of malicious content creation, filter bypass,\\nand data privacy issues, especially in e-commerce, where\\nprotecting customer privacy is crucial. If models are trained\\non private data, additional concerns arise if such models are\\nmade publicly available. LLMs tend to memorize phrases from\\ntheir training sets, which an adversary could exploit to extract\\nsensitive data, posing a threat to personal privacy [492, 493].\\nReal-Time Processing: Real-time processing in Large Lan-\\nguage Models (LLMs) is pivotal for various applications,\\nespecially with the rising popularity of mobile AI applications\\nand concerns regarding information security and privacy.\\nHowever, LLMs often have hundreds of layers and millions\\nof parameters, which impede real-time processing due to the\\nhigh computational demands and limited weight storage on\\nhardware platforms, particularly in edge computing environ-\\nments [494]. While certain e fforts like MobileBERT aim\\nto reduce memory requirements, they still face substantial\\nexecution overhead due to the large number of model layers,\\nleading to high inference latency.\\nLong-Term Dependencies: Large Language Models have\\nshown considerable progress in understanding and generating\\ntext, yet they often struggle with preserving context and\\nhandling long-term dependencies, particularly in complex,\\nmulti-turn conversations or long documents. This limitation\\ncan lead to incoherent or irrelevant responses.\\nHardware Acceleration: The growth of LLMs presents signif-\\nicant hardware challenges due to the increasing computational\\nand memory demands associated with training and deploying\\nthese models. GPUs have played a crucial role in meeting the\\nhardware requirements for training LLMs, with the networking\\nindustry also evolving to optimize hardware for training\\nworkloads. However, the growing size of LLMs, which has\\nbeen outpacing hardware progress, makes model inference in-\\ncreasingly costly. Model quantization is a promising approach\\nto bridge the widening gap between LLM size and hardware\\ncapacity [495]. Although specialized hardware acceleration\\nlike GPUs or TPUs can significantly reduce the computational\\ncost, making real-time applications more feasible, they may not\\nfully resolve all limitations, necessitating further advancements\\nin hardware technology.\\nRegulatory and Ethical Frameworks:The rapid advancements\\nin artificial intelligence have given rise to sophisticated Large\\nLanguage Models (LLMs) like OpenAI’s GPT-4 [157] and\\nGoogle’s Bard. These developments underscore the imperative\\nfor regulatory oversight to manage the ethical and social\\nchallenges accompanying LLMs’ widespread use [496]. For\\ninstance, LLMs can generate content that can be used posi-\\ntively or negatively, emphasizing the need for proactive ethical\\nframeworks and policy measures to guide their responsible\\nuse and assign accountability for their outputs [497]. Auditing\\nis identified as a promising governance mechanism to ensure\\nthat AI systems, including LLMs, are designed and deployed\\nethically, legally, and technically robust [498].\\n8. Conclusion\\nThis article has comprehensively reviewed the develop-\\nments in LLMs. It contributes to summarizing significant\\nfindings of LLMs in the existing literature and provides a\\ndetailed analysis of the design aspects, including architec-\\ntures, datasets, and training pipelines. We identified crucial\\narchitectural components and training strategies employed by\\ndifferent LLMs. These aspects are presented as summaries\\nand discussions throughout the article. Moreover, we have\\ndiscussed the performance di fferences of LLMs in zero-shot\\nand few-shot settings, explored the impact of fine-tuning, and\\ncompared supervised and generalized models and encoder vs.\\ndecoder vs. encoder-decoder architectures. A comprehensive\\nreview of multi-modal LLMs, retrieval augmented LLMs,\\nLLMs-powered agents, e fficient LLMs, datasets, evaluation,\\napplications, and challenges is also provided. This article is\\nanticipated to serve as a valuable resource for researchers,\\noffering insights into the recent advancements in LLMs and\\nproviding fundamental concepts and details to develop better\\nLLMs.\\nAcknowledgement: The author /s would like to acknowl-\\nedge the support received from Saudi Data and AI Authority\\n(SDAIA) and King Fahd University of Petroleum and Miner-\\nals (KFUPM) under SDAIA-KFUPM Joint Research Center for\\nArtificial Intelligence Grant No. JRC-AI-RFP-11.\\nReferences\\n[1] A. Chernyavskiy, D. Ilvovsky, P. Nakov, Transformers:“the end of his-\\ntory” for natural language processing?, in: Machine Learning and\\nKnowledge Discovery in Databases. Research Track: European Con-\\nference, ECML PKDD 2021, Bilbao, Spain, September 13–17, 2021,\\nProceedings, Part III 21, Springer, 2021, pp. 677–693. 1\\n[2] A. Wang, Y . Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill,\\nO. Levy, S. Bowman, Superglue: A stickier benchmark for general-\\npurpose language understanding systems, Advances in neural informa-\\ntion processing systems 32 (2019). 1, 26, 29\\n[3] D. Adiwardana, M.-T. Luong, D. R. So, J. Hall, N. Fiedel, R. Thoppilan,\\nZ. Yang, A. Kulshreshtha, G. Nemade, Y . Lu, et al., Towards a human-\\nlike open-domain chatbot, arXiv preprint arXiv:2001.09977 (2020). 1\\n[4] B. A. y Arcas, Do large language models understand us?, Daedalus\\n151 (2) (2022) 183–197. 2\\n[5] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al.,\\nLanguage models are unsupervised multitask learners, OpenAI blog\\n1 (8) (2019) 9. 2, 7\\n[6] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., Language models\\nare few-shot learners, Advances in neural information processing sys-\\ntems 33 (2020) 1877–1901. 2, 6, 7, 8, 9, 16, 18, 23, 24, 25, 34\\n[7] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training\\nof deep bidirectional transformers for language understanding, arXiv\\npreprint arXiv:1810.04805 (2018). 2, 18, 24\\n35'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 35, 'page_label': '36', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='[8] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\\nL. Zettlemoyer, Deep contextualized word representations, in: NAACL-\\nHLT, Association for Computational Linguistics, 2018, pp. 2227–2237.\\n2\\n[9] M. Lewis, Y . Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy,\\nV . Stoyanov, L. Zettlemoyer, Bart: Denoising sequence-to-sequence pre-\\ntraining for natural language generation, translation, and comprehen-\\nsion, arXiv preprint arXiv:1910.13461 (2019). 2\\n[10] C. Ra ffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\\nY . Zhou, W. Li, P. J. Liu, Exploring the limits of transfer learning with\\na unified text-to-text transformer, The Journal of Machine Learning Re-\\nsearch 21 (1) (2020) 5485–5551. 2, 7, 8, 18, 19, 24, 25, 28, 30, 31\\n[11] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant,\\nA. Barua, C. Ra ffel, mt5: A massively multilingual pre-trained text-to-\\ntext transformer, arXiv preprint arXiv:2010.11934 (2020). 2, 7, 8, 24,\\n25, 28, 30\\n[12] Z. Zhang, Y . Gu, X. Han, S. Chen, C. Xiao, Z. Sun, Y . Yao, F. Qi,\\nJ. Guan, P. Ke, et al., Cpm-2: Large-scale cost-effective pre-trained lan-\\nguage models, AI Open 2 (2021) 216–224. 2, 8, 25\\n[13] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili ´c, D. Hesslow,\\nR. Castagné, A. S. Luccioni, F. Yvon, M. Gallé, et al., Bloom: A 176b-\\nparameter open-access multilingual language model, arXiv preprint\\narXiv:2211.05100 (2022). 2, 4, 9, 11, 23, 24, 25, 30\\n[14] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan,\\nM. Diab, X. Li, X. V . Lin, et al., Opt: Open pre-trained transformer\\nlanguage models, arXiv preprint arXiv:2205.01068 (2022). 2, 9, 11, 24,\\n25\\n[15] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts,\\nP. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al., Palm: Scal-\\ning language modeling with pathways, arXiv preprint arXiv:2204.02311\\n(2022). 2, 6, 9, 11, 23, 24, 25\\n[16] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y . Tay, W. Fedus, E. Li,\\nX. Wang, M. Dehghani, S. Brahma, et al., Scaling instruction-finetuned\\nlanguage models, arXiv preprint arXiv:2210.11416 (2022). 2, 7, 11, 16,\\n17, 22, 24, 25, 28, 31\\n[17] V . Sanh, A. Webson, C. Ra ffel, S. H. Bach, L. Sutawika, Z. Alyafeai,\\nA. Cha ffin, A. Stiegler, T. L. Scao, A. Raja, et al., Multitask\\nprompted training enables zero-shot task generalization, arXiv preprint\\narXiv:2110.08207 (2021). 2, 11, 16, 25, 28, 31\\n[18] Y . Wang, S. Mishra, P. Alipoormolabashi, Y . Kordi, A. Mirzaei,\\nA. Naik, A. Ashok, A. S. Dhanasekaran, A. Arunkumar, D. Stap, et al.,\\nSuper-naturalinstructions: Generalization via declarative instructions on\\n1600+ nlp tasks, in: Proceedings of the 2022 Conference on Empirical\\nMethods in Natural Language Processing, 2022, pp. 5085–5109. 2, 7,\\n11, 16, 17, 24, 25, 28, 31\\n[19] Y . Wang, Y . Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, H. Ha-\\njishirzi, Self-instruct: Aligning language model with self generated in-\\nstructions, arXiv preprint arXiv:2212.10560 (2022). 2, 16, 19, 22, 28\\n[20] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\\nC. Zhang, S. Agarwal, K. Slama, A. Ray, et al., Training language mod-\\nels to follow instructions with human feedback, Advances in Neural In-\\nformation Processing Systems 35 (2022) 27730–27744. 2, 7, 11, 16,\\n22\\n[21] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei,\\nN. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al., Llama 2: Open\\nfoundation and fine-tuned chat models, arXiv preprint arXiv:2307.09288\\n(2023). 2, 7, 10, 16, 25, 34\\n[22] J. Wei, Y . Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yo-\\ngatama, M. Bosma, D. Zhou, D. Metzler, et al., Emergent abilities of\\nlarge language models, arXiv preprint arXiv:2206.07682 (2022). 2\\n[23] T. Webb, K. J. Holyoak, H. Lu, Emergent analogical reasoning in large\\nlanguage models, Nature Human Behaviour 7 (9) (2023) 1526–1541. 2\\n[24] D. A. Boiko, R. MacKnight, G. Gomes, Emergent autonomous sci-\\nentific research capabilities of large language models, arXiv preprint\\narXiv:2304.05332 (2023). 2\\n[25] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick,\\nJ. Dwivedi-Yu, A. Joulin, S. Riedel, E. Grave, Few-shot learning with\\nretrieval augmented language models, arXiv preprint arXiv:2208.03299\\n(2022). 2, 18, 19, 34\\n[26] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter,\\nA. Wahid, J. Tompson, Q. Vuong, T. Yu, et al., Palm-e: An embodied\\nmultimodal language model, arXiv preprint arXiv:2303.03378 (2023).\\n2, 20, 22, 33\\n[27] A. Parisi, Y . Zhao, N. Fiedel, Talm: Tool augmented language models,\\narXiv preprint arXiv:2205.12255 (2022). 2, 19, 20\\n[28] B. Zhang, H. Soh, Large language models as zero-shot human models\\nfor human-robot interaction, arXiv preprint arXiv:2303.03548 (2023). 2,\\n33\\n[29] Q. Ye, H. Xu, G. Xu, J. Ye, M. Yan, Y . Zhou, J. Wang, A. Hu, P. Shi,\\nY . Shi, et al., mplug-owl: Modularization empowers large language\\nmodels with multimodality, arXiv preprint arXiv:2304.14178 (2023). 2,\\n22\\n[30] W. Wang, Z. Chen, X. Chen, J. Wu, X. Zhu, G. Zeng, P. Luo,\\nT. Lu, J. Zhou, Y . Qiao, et al., Visionllm: Large language model\\nis also an open-ended decoder for vision-centric tasks, arXiv preprint\\narXiv:2305.11175 (2023). 2, 22\\n[31] R. Yang, L. Song, Y . Li, S. Zhao, Y . Ge, X. Li, Y . Shan, Gpt4tools:\\nTeaching large language model to use tools via self-instruction, arXiv\\npreprint arXiv:2305.18752 (2023). 2, 19, 22, 23\\n[32] E. Saravia, Prompt Engineering Guide, https: //github.com/dair-\\nai/Prompt-Engineering-Guide (12 2022). 2, 7, 18, 34\\n[33] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y . Xu,\\nW. Zheng, X. Xia, et al., Glm-130b: An open bilingual pre-trained\\nmodel, arXiv preprint arXiv:2210.02414 (2022). 2, 10, 23, 24, 25\\n[34] Y . Wang, H. Le, A. D. Gotmare, N. D. Bui, J. Li, S. C. Hoi, Codet5 +:\\nOpen code large language models for code understanding and genera-\\ntion, arXiv preprint arXiv:2305.07922 (2023). 2, 11, 24, 25\\n[35] S. Wang, Y . Sun, Y . Xiang, Z. Wu, S. Ding, W. Gong, S. Feng, J. Shang,\\nY . Zhao, C. Pang, et al., Ernie 3.0 titan: Exploring larger-scale knowl-\\nedge enhanced pre-training for language understanding and generation,\\narXiv preprint arXiv:2112.12731 (2021). 2, 8, 24, 25\\n[36] J. Rasley, S. Rajbhandari, O. Ruwase, Y . He, Deepspeed: System op-\\ntimizations enable training deep learning models with over 100 billion\\nparameters, in: Proceedings of the 26th ACM SIGKDD International\\nConference on Knowledge Discovery & Data Mining, 2020, pp. 3505–\\n3506. 2, 5\\n[37] S. Rajbhandari, J. Rasley, O. Ruwase, Y . He, Zero: Memory optimiza-\\ntions toward training trillion parameter models, in: SC20: International\\nConference for High Performance Computing, Networking, Storage and\\nAnalysis, IEEE, 2020, pp. 1–16. 2, 4, 24\\n[38] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, G. Neubig, Towards\\na unified view of parameter-e fficient transfer learning, arXiv preprint\\narXiv:2110.04366 (2021). 2, 20, 21\\n[39] Z. Hu, Y . Lan, L. Wang, W. Xu, E.-P. Lim, R. K.-W. Lee, L. Bing, S. Po-\\nria, Llm-adapters: An adapter family for parameter-e fficient fine-tuning\\nof large language models, arXiv preprint arXiv:2304.01933 (2023). 2,\\n20\\n[40] B. Lester, R. Al-Rfou, N. Constant, The power of scale for parameter-\\nefficient prompt tuning, arXiv preprint arXiv:2104.08691 (2021). 2, 8,\\n20, 21\\n[41] X. L. Li, P. Liang, Prefix-tuning: Optimizing continuous prompts for\\ngeneration, arXiv preprint arXiv:2101.00190 (2021). 2, 20, 21\\n[42] X. Ma, G. Fang, X. Wang, Llm-pruner: On the structural pruning of\\nlarge language models, arXiv preprint arXiv:2305.11627 (2023). 2, 22\\n[43] R. Xu, F. Luo, C. Wang, B. Chang, J. Huang, S. Huang, F. Huang,\\nFrom dense to sparse: Contrastive pruning for better pre-trained lan-\\nguage model compression, in: Proceedings of the AAAI Conference on\\nArtificial Intelligence, V ol. 36, 2022, pp. 11547–11555. 2, 22\\n[44] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, S. Han, Smoothquant:\\nAccurate and e fficient post-training quantization for large language\\nmodels, in: ICML, V ol. 202 of Proceedings of Machine Learning Re-\\nsearch, PMLR, 2023, pp. 38087–38099. 2, 21\\n[45] C. Tao, L. Hou, W. Zhang, L. Shang, X. Jiang, Q. Liu, P. Luo, N. Wong,\\nCompression of generative pre-trained language models via quantiza-\\ntion, arXiv preprint arXiv:2203.10705 (2022). 2, 21\\n[46] A. Pal, D. Karkhanis, M. Roberts, S. Dooley, A. Sundararajan, S. Naidu,\\nGiraffe: Adventures in expanding context lengths in llms, arXiv preprint\\narXiv:2308.10882 (2023). 2, 17\\n[47] B. Peng, J. Quesnelle, H. Fan, E. Shippole, Yarn: E fficient con-\\ntext window extension of large language models, arXiv preprint\\narXiv:2309.00071 (2023). 2, 17\\n[48] M. Guo, J. Ainslie, D. Uthus, S. Ontanon, J. Ni, Y .-H. Sung, Y . Yang,\\n36'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 36, 'page_label': '37', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='Longt5: E fficient text-to-text transformer for long sequences, arXiv\\npreprint arXiv:2112.07916 (2021). 2, 18\\n[49] S. Chen, S. Wong, L. Chen, Y . Tian, Extending context window\\nof large language models via positional interpolation, arXiv preprint\\narXiv:2306.15595 (2023). 2, 17\\n[50] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y . Hou, Y . Min, B. Zhang,\\nJ. Zhang, Z. Dong, et al., A survey of large language models, arXiv\\npreprint arXiv:2303.18223 (2023). 2, 3, 7\\n[51] U. Naseem, I. Razzak, S. K. Khan, M. Prasad, A comprehensive sur-\\nvey on word representation models: From classical to state-of-the-art\\nword representation language models, Transactions on Asian and Low-\\nResource Language Information Processing 20 (5) (2021) 1–35. 2, 3\\n[52] B. Min, H. Ross, E. Sulem, A. P. B. Veyseh, T. H. Nguyen, O. Sainz,\\nE. Agirre, I. Heinz, D. Roth, Recent advances in natural language pro-\\ncessing via large pre-trained language models: A survey, arXiv preprint\\narXiv:2111.01243 (2021). 2, 3\\n[53] C. Zhou, Q. Li, C. Li, J. Yu, Y . Liu, G. Wang, K. Zhang, C. Ji, Q. Yan,\\nL. He, et al., A comprehensive survey on pretrained foundation models:\\nA history from bert to chatgpt, arXiv preprint arXiv:2302.09419 (2023).\\n2, 3\\n[54] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun,\\nJ. Xu, Z. Sui, A survey for in-context learning, arXiv preprint\\narXiv:2301.00234 (2022). 2, 7, 18\\n[55] J. Huang, K. C.-C. Chang, Towards reasoning in large language models:\\nA survey, arXiv preprint arXiv:2212.10403 (2022). 2, 7, 18\\n[56] Y . Wang, W. Zhong, L. Li, F. Mi, X. Zeng, W. Huang, L. Shang, X. Jiang,\\nQ. Liu, Aligning large language models with human: A survey, arXiv\\npreprint arXiv:2307.12966 (2023). 2\\n[57] X. Zhu, J. Li, Y . Liu, C. Ma, W. Wang, A survey on model compression\\nfor large language models, arXiv preprint arXiv:2308.07633 (2023). 2\\n[58] S. Yin, C. Fu, S. Zhao, K. Li, X. Sun, T. Xu, E. Chen, A survey on multi-\\nmodal large language models, arXiv preprint arXiv:2306.13549 (2023).\\n2, 22, 23\\n[59] J. J. Webster, C. Kit, Tokenization as the initial phase in nlp, in: COL-\\nING 1992 volume 4: The 14th international conference on computa-\\ntional linguistics, 1992. 4\\n[60] T. Kudo, Subword regularization: Improving neural network translation\\nmodels with multiple subword candidates, in: Proceedings of the 56th\\nAnnual Meeting of the Association for Computational Linguistics (V ol-\\nume 1: Long Papers), 2018, pp. 66–75. 4\\n[61] R. Sennrich, B. Haddow, A. Birch, Neural machine translation of rare\\nwords with subword units, in: Proceedings of the 54th Annual Meet-\\ning of the Association for Computational Linguistics (V olume 1: Long\\nPapers), 2016, pp. 1715–1725. 4\\n[62] M. Schuster, K. Nakajima, Japanese and korean voice search, in: 2012\\nIEEE international conference on acoustics, speech and signal process-\\ning (ICASSP), IEEE, 2012, pp. 5149–5152. 4\\n[63] S. J. Mielke, Z. Alyafeai, E. Salesky, C. Ra ffel, M. Dey, M. Gallé,\\nA. Raja, C. Si, W. Y . Lee, B. Sagot, et al., Between words and char-\\nacters: A brief history of open-vocabulary modeling and tokenization in\\nnlp, arXiv preprint arXiv:2112.10508 (2021). 4\\n[64] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\\nŁ. Kaiser, I. Polosukhin, Attention is all you need, Advances in neural\\ninformation processing systems 30 (2017). 4, 7\\n[65] O. Press, N. Smith, M. Lewis, Train short, test long: Attention with\\nlinear biases enables input length extrapolation, in: International Con-\\nference on Learning Representations, 2022.\\nURL https://openreview.net/forum?id=R8sQPpGCv0 4, 17\\n[66] J. Su, Y . Lu, S. Pan, A. Murtadha, B. Wen, Y . Liu, Roformer: En-\\nhanced transformer with rotary position embedding, arXiv preprint\\narXiv:2104.09864 (2021). 4, 9, 17\\n[67] R. Child, S. Gray, A. Radford, I. Sutskever, Generating long sequences\\nwith sparse transformers, arXiv preprint arXiv:1904.10509 (2019). 4, 7,\\n23\\n[68] T. Dao, D. Fu, S. Ermon, A. Rudra, C. Ré, Flashattention: Fast and\\nmemory-efficient exact attention with io-awareness, Advances in Neural\\nInformation Processing Systems 35 (2022) 16344–16359. 4\\n[69] K. Hornik, M. Stinchcombe, H. White, Multilayer feedforward networks\\nare universal approximators, Neural networks 2 (5) (1989) 359–366. 4\\n[70] V . Nair, G. E. Hinton, Rectified linear units improve restricted boltz-\\nmann machines, in: Proceedings of the 27th international conference on\\nmachine learning (ICML-10), 2010, pp. 807–814. 4\\n[71] D. Hendrycks, K. Gimpel, Gaussian error linear units (gelus), arXiv\\npreprint arXiv:1606.08415 (2016). 4\\n[72] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov,\\nDropout: a simple way to prevent neural networks from overfitting, The\\njournal of machine learning research 15 (1) (2014) 1929–1958. 4\\n[73] D. Krueger, T. Maharaj, J. Kramár, M. Pezeshki, N. Ballas, N. R.\\nKe, A. Goyal, Y . Bengio, A. Courville, C. Pal, Zoneout: Regular-\\nizing rnns by randomly preserving hidden activations, arXiv preprint\\narXiv:1606.01305 (2016). 4\\n[74] N. Shazeer, Glu variants improve transformer, arXiv preprint\\narXiv:2002.05202 (2020). 4\\n[75] Y . N. Dauphin, A. Fan, M. Auli, D. Grangier, Language modeling with\\ngated convolutional networks, in: International conference on machine\\nlearning, PMLR, 2017, pp. 933–941. 4\\n[76] J. L. Ba, J. R. Kiros, G. E. Hinton, Layer normalization, arXiv preprint\\narXiv:1607.06450 (2016). 4\\n[77] B. Zhang, R. Sennrich, Root mean square layer normalization, Advances\\nin Neural Information Processing Systems 32 (2019). 4\\n[78] A. Baevski, M. Auli, Adaptive input representations for neural language\\nmodeling, arXiv preprint arXiv:1809.10853 (2018). 4\\n[79] H. Wang, S. Ma, L. Dong, S. Huang, D. Zhang, F. Wei, Deepnet: Scaling\\ntransformers to 1,000 layers, arXiv preprint arXiv:2203.00555 (2022). 4\\n[80] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, B. Catanzaro,\\nMegatron-lm: Training multi-billion parameter language models using\\nmodel parallelism, arXiv preprint arXiv:1909.08053 (2019). 4, 5\\n[81] \"bmtrain: E fficient training for big models.\".\\nURL https://github.com/OpenBMB/BMTrain 4, 5\\n[82] T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cis-\\ntac, T. Rault, R. Louf, M. Funtowicz, et al., Transformers: State-of-the-\\nart natural language processing, in: Proceedings of the 2020 conference\\non empirical methods in natural language processing: system demon-\\nstrations, 2020, pp. 38–45. 5\\n[83] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclau-\\nrin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, et al.,\\nJax: composable transformations of python + numpy programs (2018).\\n5\\n[84] S. Li, J. Fang, Z. Bian, H. Liu, Y . Liu, H. Huang, B. Wang, Y . You,\\nColossal-ai: A unified deep learning system for large-scale parallel train-\\ning, arXiv preprint arXiv:2110.14883 (2021). 5\\n[85] J. He, J. Qiu, A. Zeng, Z. Yang, J. Zhai, J. Tang, Fastmoe: A\\nfast mixture-of-expert training system, arXiv preprint arXiv:2103.13262\\n(2021). 5\\n[86] L. Huawei Technologies Co., Huawei mindspore ai development frame-\\nwork, in: Artificial Intelligence Technology, Springer, 2022, pp. 137–\\n162. 5\\n[87] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al., Pytorch: An imper-\\native style, high-performance deep learning library, Advances in neural\\ninformation processing systems 32 (2019). 5\\n[88] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,\\nS. Ghemawat, G. Irving, M. Isard, et al., Tensorflow: a system for large-\\nscale machine learning., in: Osdi, V ol. 16, Savannah, GA, USA, 2016,\\npp. 265–283. 5\\n[89] T. Chen, M. Li, Y . Li, M. Lin, N. Wang, M. Wang, T. Xiao,\\nB. Xu, C. Zhang, Z. Zhang, Mxnet: A flexible and e fficient machine\\nlearning library for heterogeneous distributed systems, arXiv preprint\\narXiv:1512.01274 (2015). 5\\n[90] W. Fedus, B. Zoph, N. Shazeer, Switch transformers: Scaling to tril-\\nlion parameter models with simple and efficient sparsity, The Journal of\\nMachine Learning Research 23 (1) (2022) 5232–5270. 5, 9\\n[91] N. Du, Y . Huang, A. M. Dai, S. Tong, D. Lepikhin, Y . Xu, M. Krikun,\\nY . Zhou, A. W. Yu, O. Firat, et al., Glam: Efficient scaling of language\\nmodels with mixture-of-experts, in: International Conference on Ma-\\nchine Learning, PMLR, 2022, pp. 5547–5569. 5, 9, 23, 24, 25\\n[92] X. Ren, P. Zhou, X. Meng, X. Huang, Y . Wang, W. Wang, P. Li,\\nX. Zhang, A. Podolskiy, G. Arshinov, et al., Pangu- P: Towards trillion\\nparameter language model with sparse heterogeneous computing, arXiv\\npreprint arXiv:2303.10845 (2023). 5, 10, 16, 23, 24, 25\\n[93] T. Wang, A. Roberts, D. Hesslow, T. Le Scao, H. W. Chung, I. Beltagy,\\nJ. Launay, C. Ra ffel, What language model architecture and pretrain-\\n37'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 37, 'page_label': '38', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='ing objective works best for zero-shot generalization?, in: International\\nConference on Machine Learning, PMLR, 2022, pp. 22964–22984. 5\\n[94] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y . Wang, J. Gao, M. Zhou,\\nH.-W. Hon, Unified language model pre-training for natural language\\nunderstanding and generation, Advances in neural information process-\\ning systems 32 (2019). 6\\n[95] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,\\nS. Gray, A. Radford, J. Wu, D. Amodei, Scaling laws for neural language\\nmodels, arXiv preprint arXiv:2001.08361 (2020). 6\\n[96] J. Ho ffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai,\\nE. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark,\\net al., Training compute-optimal large language models, arXiv preprint\\narXiv:2203.15556 (2022). 6, 9, 25, 29\\n[97] S. Iyer, X. V . Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shuster,\\nT. Wang, Q. Liu, P. S. Koura, et al., Opt-iml: Scaling language model in-\\nstruction meta learning through the lens of generalization, arXiv preprint\\narXiv:2212.12017 (2022). 7, 11, 16, 17, 22, 25, 28\\n[98] Z. Sun, Y . Shen, Q. Zhou, H. Zhang, Z. Chen, D. Cox, Y . Yang, C. Gan,\\nPrinciple-driven self-alignment of language models from scratch with\\nminimal human supervision, arXiv preprint arXiv:2305.03047 (2023).\\n7, 17\\n[99] A. Askell, Y . Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones,\\nN. Joseph, B. Mann, N. DasSarma, et al., A general language assistant\\nas a laboratory for alignment, arXiv preprint arXiv:2112.00861 (2021).\\n7\\n[100] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei,\\nP. Christiano, G. Irving, Fine-tuning language models from human pref-\\nerences, arXiv preprint arXiv:1909.08593 (2019). 7\\n[101] S. Kim, S. J. Joo, D. Kim, J. Jang, S. Ye, J. Shin, M. Seo, The cot collec-\\ntion: Improving zero-shot and few-shot learning of language models via\\nchain-of-thought fine-tuning, arXiv preprint arXiv:2305.14045 (2023).\\n7, 16\\n[102] Q. Liu, F. Zhou, Z. Jiang, L. Dou, M. Lin, From zero to hero: Exam-\\nining the power of symbolic tasks in instruction tuning, arXiv preprint\\narXiv:2304.07995 (2023). 7, 16\\n[103] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V . Le,\\nD. Zhou, et al., Chain-of-thought prompting elicits reasoning in large\\nlanguage models, Advances in Neural Information Processing Systems\\n35 (2022) 24824–24837. 7, 20, 23\\n[104] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowd-\\nhery, D. Zhou, Self-consistency improves chain of thought reasoning in\\nlanguage models, arXiv preprint arXiv:2203.11171 (2022). 7, 20\\n[105] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y . Cao, K. Narasimhan,\\nTree of thoughts: Deliberate problem solving with large language mod-\\nels, arXiv preprint arXiv:2305.10601 (2023). 7, 20\\n[106] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe,\\nA. Gesmundo, M. Attariyan, S. Gelly, Parameter-efficient transfer learn-\\ning for nlp, in: International Conference on Machine Learning, PMLR,\\n2019, pp. 2790–2799. 7, 20\\n[107] S. McCandlish, J. Kaplan, D. Amodei, O. D. Team, An empirical model\\nof large-batch training, arXiv preprint arXiv:1812.06162 (2018). 7\\n[108] W. Zeng, X. Ren, T. Su, H. Wang, Y . Liao, Z. Wang, X. Jiang, Z. Yang,\\nK. Wang, X. Zhang, et al., Pangu- α : Large-scale autoregressive pre-\\ntrained chinese language models with auto-parallel computation, arXiv\\npreprint arXiv:2104.12369 (2021). 8, 23, 24, 25\\n[109] S. Yuan, H. Zhao, Z. Du, M. Ding, X. Liu, Y . Cen, X. Zou, Z. Yang,\\nJ. Tang, Wudaocorpora: A super large-scale chinese corpora for pre-\\ntraining language models, AI Open 2 (2021) 65–68. 8, 30\\n[110] Y . Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang, J. Liu, X. Chen,\\nY . Zhao, Y . Lu, et al., Ernie 3.0: Large-scale knowledge enhanced\\npre-training for language understanding and generation, arXiv preprint\\narXiv:2107.02137 (2021). 8, 25\\n[111] Z. Dai, Z. Yang, Y . Yang, J. Carbonell, Q. V . Le, R. Salakhutdinov,\\nTransformer-xl: Attentive language models beyond a fixed-length con-\\ntext, arXiv preprint arXiv:1901.02860 (2019). 8\\n[112] O. Lieber, O. Sharir, B. Lenz, Y . Shoham, Jurassic-1: Technical details\\nand evaluation, White Paper. AI21 Labs 1 (2021). 8, 24, 25\\n[113] Y . Levine, N. Wies, O. Sharir, H. Bata, A. Shashua, Limits to depth ef-\\nficiencies of self-attention, Advances in Neural Information Processing\\nSystems 33 (2020) 22640–22651. 8, 11\\n[114] B. Kim, H. Kim, S.-W. Lee, G. Lee, D. Kwak, D. H. Jeon, S. Park,\\nS. Kim, S. Kim, D. Seo, et al., What changes can large-scale language\\nmodels bring? intensive study on hyperclova: Billions-scale korean\\ngenerative pretrained transformers, arXiv preprint arXiv:2109.04650\\n(2021). 8, 25\\n[115] S. Wu, X. Zhao, T. Yu, R. Zhang, C. Shen, H. Liu, F. Li, H. Zhu, J. Luo,\\nL. Xu, et al., Yuan 1.0: Large-scale pre-trained language model in zero-\\nshot and few-shot learning, arXiv preprint arXiv:2110.04725 (2021). 8,\\n24, 25\\n[116] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Ho ffmann, F. Song,\\nJ. Aslanides, S. Henderson, R. Ring, S. Young, et al., Scaling lan-\\nguage models: Methods, analysis & insights from training gopher, arXiv\\npreprint arXiv:2112.11446 (2021). 8, 9, 25, 28\\n[117] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari,\\nJ. Casper, Z. Liu, S. Prabhumoye, G. Zerveas, V . Korthikanti, et al.,\\nUsing deepspeed and megatron to train megatron-turing nlg 530b, a\\nlarge-scale generative language model, arXiv preprint arXiv:2201.11990\\n(2022). 8, 9, 24, 25\\n[118] S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Golding,\\nH. He, C. Leahy, K. McDonell, J. Phang, et al., Gpt-neox-20b: An open-\\nsource autoregressive language model, arXiv preprint arXiv:2204.06745\\n(2022). 9, 23, 24, 25\\n[119] W. Ben, K. Aran, Gpt-j-6b: A 6 billion parameter autoregressive lan-\\nguage model (2021). 9\\n[120] P. Micikevicius, S. Narang, J. Alben, G. Diamos, E. Elsen, D. Garcia,\\nB. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh, et al., Mixed pre-\\ncision training, arXiv preprint arXiv:1710.03740 (2017). 9, 23\\n[121] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hin-\\nton, J. Dean, Outrageously large neural networks: The sparsely-gated\\nmixture-of-experts layer, arXiv preprint arXiv:1701.06538 (2017). 9, 23\\n[122] S. Soltan, S. Ananthakrishnan, J. FitzGerald, R. Gupta, W. Hamza,\\nH. Khan, C. Peris, S. Rawls, A. Rosenbaum, A. Rumshisky, et al., Alex-\\natm 20b: Few-shot learning using a large-scale multilingual seq2seq\\nmodel, arXiv preprint arXiv:2208.01448 (2022). 9, 23, 24, 25\\n[123] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos,\\nS. Shakeri, E. Taropa, P. Bailey, Z. Chen, et al., Palm 2 technical report,\\narXiv preprint arXiv:2305.10403 (2023). 9, 25\\n[124] Y . Tay, J. Wei, H. W. Chung, V . Q. Tran, D. R. So, S. Shakeri, X. Garcia,\\nH. S. Zheng, J. Rao, A. Chowdhery, et al., Transcending scaling laws\\nwith 0.1% extra compute, arXiv preprint arXiv:2210.11399 (2022). 9,\\n24, 25\\n[125] Y . Tay, M. Dehghani, V . Q. Tran, X. Garcia, J. Wei, X. Wang, H. W.\\nChung, D. Bahri, T. Schuster, S. Zheng, et al., Ul2: Unifying lan-\\nguage learning paradigms, in: The Eleventh International Conference\\non Learning Representations, 2022. 9, 10, 24, 25\\n[126] Z. Du, Y . Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, J. Tang, Glm: Gen-\\neral language model pretraining with autoregressive blank infilling, in:\\nProceedings of the 60th Annual Meeting of the Association for Compu-\\ntational Linguistics (V olume 1: Long Papers), 2022, pp. 320–335. 10\\n[127] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\\nT. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al.,\\nLlama: Open and e fficient foundation language models, arXiv preprint\\narXiv:2302.13971 (2023). 10, 23, 25\\n[128] M. N. Rabe, C. Staats, Self-attention does not need o(n2) memory, arXiv\\npreprint arXiv:2112.05682 (2021). 10\\n[129] V . A. Korthikanti, J. Casper, S. Lym, L. McAfee, M. Andersch,\\nM. Shoeybi, B. Catanzaro, Reducing activation recomputation in large\\ntransformer models, Proceedings of Machine Learning and Systems 5\\n(2023). 10\\n[130] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman,\\nA. Mathur, A. Schelten, A. Yang, A. Fan, et al., The llama 3 herd of\\nmodels, arXiv preprint arXiv:2407.21783 (2024). 10, 25\\n[131] https://mistral.ai/news/mixtral-8x22b/. 10, 25\\n[132] https://github.com/Snowflake-Labs/snowflake-arctic. 10,\\n25\\n[133] https://github.com/xai-org/grok-1. 10\\n[134] https://x.ai/blog/grok-1.5. 10\\n[135] G. Team, R. Anil, S. Borgeaud, Y . Wu, J.-B. Alayrac, J. Yu, R. Soricut,\\nJ. Schalkwyk, A. M. Dai, A. Hauth, et al., Gemini: a family of highly\\ncapable multimodal models, arXiv preprint arXiv:2312.11805 (2023).\\n10\\n[136] M. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J.-b.\\n38'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 38, 'page_label': '39', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='Alayrac, R. Soricut, A. Lazaridou, O. Firat, J. Schrittwieser, et al., Gem-\\nini 1.5: Unlocking multimodal understanding across millions of tokens\\nof context, arXiv preprint arXiv:2403.05530 (2024). 10\\n[137] B. Adler, N. Agarwal, A. Aithal, D. H. Anh, P. Bhattacharya, A. Brun-\\ndyn, J. Casper, B. Catanzaro, S. Clay, J. Cohen, et al., Nemotron-4 340b\\ntechnical report, arXiv preprint arXiv:2406.11704 (2024). 10, 25\\n[138] X. Bi, D. Chen, G. Chen, S. Chen, D. Dai, C. Deng, H. Ding, K. Dong,\\nQ. Du, Z. Fu, et al., Deepseek llm: Scaling open-source language models\\nwith longtermism, arXiv preprint arXiv:2401.02954 (2024). 10, 25\\n[139] DeepSeek-AI, A. Liu, B. Feng, B. Wang, B. Wang, B. Liu, C. Zhao,\\nC. Deng, C. Ruan, D. Dai, D. Guo, D. Yang, D. Chen, D. Ji, E. Li,\\nF. Lin, F. Luo, G. Hao, G. Chen, G. Li, H. Zhang, H. Xu, H. Yang,\\nH. Zhang, H. Ding, H. Xin, H. Gao, H. Li, H. Qu, J. L. Cai, J. Liang,\\nJ. Guo, J. Ni, J. Li, J. Chen, J. Yuan, J. Qiu, J. Song, K. Dong, K. Gao,\\nK. Guan, L. Wang, L. Zhang, L. Xu, L. Xia, L. Zhao, L. Zhang, M. Li,\\nM. Wang, M. Zhang, M. Zhang, M. Tang, M. Li, N. Tian, P. Huang,\\nP. Wang, P. Zhang, Q. Zhu, Q. Chen, Q. Du, R. J. Chen, R. L. Jin, R. Ge,\\nR. Pan, R. Xu, R. Chen, S. S. Li, S. Lu, S. Zhou, S. Chen, S. Wu, S. Ye,\\nS. Ma, S. Wang, S. Zhou, S. Yu, S. Zhou, S. Zheng, T. Wang, T. Pei,\\nT. Yuan, T. Sun, W. L. Xiao, W. Zeng, W. An, W. Liu, W. Liang, W. Gao,\\nW. Zhang, X. Q. Li, X. Jin, X. Wang, X. Bi, X. Liu, X. Wang, X. Shen,\\nX. Chen, X. Chen, X. Nie, X. Sun, Deepseek-v2: A strong, economical,\\nand efficient mixture-of-experts language model, CoRR abs/2405.04434\\n(2024). 10, 25\\n[140] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y . Zhou, S. Savarese,\\nC. Xiong, Codegen: An open large language model for code with multi-\\nturn program synthesis, arXiv preprint arXiv:2203.13474 (2022). 11,\\n23, 25, 28\\n[141] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Ed-\\nwards, Y . Burda, N. Joseph, G. Brockman, et al., Evaluating large lan-\\nguage models trained on code, arXiv preprint arXiv:2107.03374 (2021).\\n11, 25, 29, 31\\n[142] Y . Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond,\\nT. Eccles, J. Keeling, F. Gimeno, A. Dal Lago, et al., Competition-level\\ncode generation with alphacode, Science 378 (6624) (2022) 1092–1097.\\n11, 23, 25, 29\\n[143] N. Shazeer, Fast transformer decoding: One write-head is all you need,\\narXiv preprint arXiv:1911.02150 (2019). 11\\n[144] R. Y . Pang, H. He, Text generation by learning from demonstrations,\\narXiv preprint arXiv:2009.07839 (2020). 11\\n[145] R. Dabre, A. Fujita, Softmax tempering for training neural machine\\ntranslation models, arXiv preprint arXiv:2009.09372 (2020). 11\\n[146] Y . Wang, W. Wang, S. Joty, S. C. Hoi, Codet5: Identifier-aware unified\\npre-trained encoder-decoder models for code understanding and genera-\\ntion, arXiv preprint arXiv:2109.00859 (2021). 11\\n[147] R. Li, L. B. Allal, Y . Zi, N. Muennigho ff, D. Kocetkov, C. Mou,\\nM. Marone, C. Akiki, J. Li, J. Chim, et al., Starcoder: may the source be\\nwith you!, arXiv preprint arXiv:2305.06161 (2023). 11, 25\\n[148] R. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn, E. Saravia,\\nA. Poulton, V . Kerkez, R. Stojnic, Galactica: A large language model for\\nscience, arXiv preprint arXiv:2211.09085 (2022). 11, 24, 25, 29\\n[149] FairScale authors, Fairscale: A general purpose modular pytorch library\\nfor high performance and large scale training, https://github.com/\\nfacebookresearch/fairscale (2021). 11\\n[150] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T.\\nCheng, A. Jin, T. Bos, L. Baker, Y . Du, et al., Lamda: Language models\\nfor dialog applications, arXiv preprint arXiv:2201.08239 (2022). 11, 25\\n[151] S. Wu, O. Irsoy, S. Lu, V . Dabravolski, M. Dredze, S. Gehrmann,\\nP. Kambadur, D. Rosenberg, G. Mann, Bloomberggpt: A large language\\nmodel for finance, arXiv preprint arXiv:2303.17564 (2023). 11, 25, 33\\n[152] X. Zhang, Q. Yang, D. Xu, Xuanyuan 2.0: A large chinese finan-\\ncial chat model with hundreds of billions parameters, arXiv preprint\\narXiv:2305.12002 (2023). 11, 17, 25\\n[153] W. Ben, Mesh-transformer-jax: Model-parallel implementation of trans-\\nformer language model with jax (2021). 12, 24\\n[154] N. Muennigho ff, T. Wang, L. Sutawika, A. Roberts, S. Biderman,\\nT. L. Scao, M. S. Bari, S. Shen, Z.-X. Yong, H. Schoelkopf, et al.,\\nCrosslingual generalization through multitask finetuning, arXiv preprint\\narXiv:2211.01786 (2022). 16, 25, 28, 31\\n[155] D. Yin, X. Liu, F. Yin, M. Zhong, H. Bansal, J. Han, K.-W. Chang,\\nDynosaur: A dynamic growth paradigm for instruction-tuning data cu-\\nration, arXiv preprint arXiv:2305.14327 (2023). 16\\n[156] P. Gao, J. Han, R. Zhang, Z. Lin, S. Geng, A. Zhou, W. Zhang, P. Lu,\\nC. He, X. Yue, et al., Llama-adapter v2: Parameter-e fficient visual in-\\nstruction model, arXiv preprint arXiv:2304.15010 (2023). 16, 24\\n[157] Openai. gpt-4 technical report (2023). 16, 35\\n[158] R. Taori, I. Gulrajani, T. Zhang, Y . Dubois, X. Li, C. Guestrin, P. Liang,\\nT. B. Hashimoto, Stanford alpaca: An instruction-following llama\\nmodel, https://github.com/tatsu-lab/stanford_alpaca\\n(2023). 16, 25, 28\\n[159] W.-L. Chiang, Z. Li, Z. Lin, Y . Sheng, Z. Wu, H. Zhang, L. Zheng,\\nS. Zhuang, Y . Zhuang, J. E. Gonzalez, I. Stoica, E. P. Xing, Vicuna: An\\nopen-source chatbot impressing gpt-4 with 90%* chatgpt quality (March\\n2023).\\nURL https://lmsys.org/blog/2023-03-30-vicuna/ 16, 22, 25,\\n28\\n[160] B. Peng, C. Li, P. He, M. Galley, J. Gao, Instruction tuning with gpt-4,\\narXiv preprint arXiv:2304.03277 (2023). 16, 28\\n[161] T. Liu, B. K. H. Low, Goat: Fine-tuned llama outperforms gpt-4 on\\narithmetic tasks, arXiv preprint arXiv:2305.14201 (2023). 16\\n[162] H. Wang, C. Liu, N. Xi, Z. Qiang, S. Zhao, B. Qin, T. Liu, Huatuo:\\nTuning llama model with chinese medical knowledge, arXiv preprint\\narXiv:2304.06975 (2023). 16\\n[163] C. Xu, Q. Sun, K. Zheng, X. Geng, P. Zhao, J. Feng, C. Tao, D. Jiang,\\nWizardlm: Empowering large language models to follow complex in-\\nstructions, arXiv preprint arXiv:2304.12244 (2023). 16\\n[164] Z. Luo, C. Xu, P. Zhao, Q. Sun, X. Geng, W. Hu, C. Tao, J. Ma, Q. Lin,\\nD. Jiang, Wizardcoder: Empowering code large language models with\\nevol-instruct, arXiv preprint arXiv:2306.08568 (2023). 16, 25\\n[165] J. Menick, M. Trebacz, V . Mikulik, J. Aslanides, F. Song, M. Chadwick,\\nM. Glaese, S. Young, L. Campbell-Gillingham, G. Irving, et al., Teach-\\ning language models to support answers with verified quotes, arXiv\\npreprint arXiv:2203.11147 (2022). 17\\n[166] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim,\\nC. Hesse, S. Jain, V . Kosaraju, W. Saunders, et al., Webgpt: Browser-\\nassisted question-answering with human feedback, arXiv preprint\\narXiv:2112.09332 (2021). 17, 19, 20, 25, 31\\n[167] A. Glaese, N. McAleese, M. Tr˛ ebacz, J. Aslanides, V . Firoiu, T. Ewalds,\\nM. Rauh, L. Weidinger, M. Chadwick, P. Thacker, et al., Improving\\nalignment of dialogue agents via targeted human judgements, arXiv\\npreprint arXiv:2209.14375 (2022). 17, 20, 25\\n[168] R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, C. Finn,\\nDirect preference optimization: Your language model is secretly a re-\\nward model, arXiv preprint arXiv:2305.18290 (2023). 17\\n[169] H. Dong, W. Xiong, D. Goyal, R. Pan, S. Diao, J. Zhang, K. Shum,\\nT. Zhang, Raft: Reward ranked finetuning for generative foundation\\nmodel alignment, arXiv preprint arXiv:2304.06767 (2023). 17\\n[170] Z. Yuan, H. Yuan, C. Tan, W. Wang, S. Huang, F. Huang, Rrhf: Rank\\nresponses to align language models with human feedback without tears,\\narXiv preprint arXiv:2304.05302 (2023). 17\\n[171] F. Song, B. Yu, M. Li, H. Yu, F. Huang, Y . Li, H. Wang, Preference rank-\\ning optimization for human alignment, arXiv preprint arXiv:2306.17492\\n(2023). 17\\n[172] H. Liu, C. Sferrazza, P. Abbeel, Languages are rewards: Hindsight fine-\\ntuning using human feedback, arXiv preprint arXiv:2302.02676 (2023).\\n17\\n[173] Y . Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen,\\nA. Goldie, A. Mirhoseini, C. McKinnon, et al., Constitutional ai: Harm-\\nlessness from ai feedback, arXiv preprint arXiv:2212.08073 (2022). 17\\n[174] Y . Dubois, X. Li, R. Taori, T. Zhang, I. Gulrajani, J. Ba, C. Guestrin,\\nP. Liang, T. B. Hashimoto, Alpacafarm: A simulation frame-\\nwork for methods that learn from human feedback, arXiv preprint\\narXiv:2305.14387 (2023). 17\\n[175] C. Si, Z. Gan, Z. Yang, S. Wang, J. Wang, J. Boyd-Graber, L. Wang,\\nPrompting gpt-3 to be reliable, arXiv preprint arXiv:2210.09150 (2022).\\n17\\n[176] D. Ganguli, A. Askell, N. Schiefer, T. Liao, K. Lukoši ¯ut˙e, A. Chen,\\nA. Goldie, A. Mirhoseini, C. Olsson, D. Hernandez, et al., The capac-\\nity for moral self-correction in large language models, arXiv preprint\\narXiv:2302.07459 (2023). 17\\n[177] A. Wei, N. Haghtalab, J. Steinhardt, Jailbroken: How does llm safety\\ntraining fail?, arXiv preprint arXiv:2307.02483 (2023). 17\\n39'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 39, 'page_label': '40', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='[178] D. Ganguli, L. Lovitt, J. Kernion, A. Askell, Y . Bai, S. Kadavath,\\nB. Mann, E. Perez, N. Schiefer, K. Ndousse, et al., Red teaming lan-\\nguage models to reduce harms: Methods, scaling behaviors, and lessons\\nlearned, arXiv preprint arXiv:2209.07858 (2022). 17, 28\\n[179] S. Casper, J. Lin, J. Kwon, G. Culp, D. Hadfield-Menell, Explore, estab-\\nlish, exploit: Red teaming language models from scratch, arXiv preprint\\narXiv:2306.09442 (2023). 17\\n[180] E. Perez, S. Huang, F. Song, T. Cai, R. Ring, J. Aslanides, A. Glaese,\\nN. McAleese, G. Irving, Red teaming language models with language\\nmodels, arXiv preprint arXiv:2202.03286 (2022). 17\\n[181] T. Scialom, T. Chakrabarty, S. Muresan, Fine-tuned language models are\\ncontinual learners, in: Proceedings of the 2022 Conference on Empirical\\nMethods in Natural Language Processing, 2022, pp. 6107–6122. 17\\n[182] Z. Shi, A. Lipani, Don’t stop pretraining? make prompt-based fine-\\ntuning powerful learner, arXiv preprint arXiv:2305.01711 (2023). 17\\n[183] H. Gupta, S. A. Sawant, S. Mishra, M. Nakamura, A. Mitra, S. Mashetty,\\nC. Baral, Instruction tuned models are quick learners, arXiv preprint\\narXiv:2306.05539 (2023). 17\\n[184] H. Chen, Y . Zhang, Q. Zhang, H. Yang, X. Hu, X. Ma, Y . Yanggong,\\nJ. Zhao, Maybe only 0.5% data is needed: A preliminary exploration\\nof low training data instruction tuning, arXiv preprint arXiv:2305.09246\\n(2023). 17\\n[185] C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y . Mao, X. Ma, A. Efrat,\\nP. Yu, L. Yu, et al., Lima: Less is more for alignment, arXiv preprint\\narXiv:2305.11206 (2023). 17, 25, 28\\n[186] C. Han, Q. Wang, W. Xiong, Y . Chen, H. Ji, S. Wang, Lm-infinite: Sim-\\nple on-the-fly length generalization for large language models, arXiv\\npreprint arXiv:2308.16137 (2023). 17, 18\\n[187] J. Ainslie, T. Lei, M. de Jong, S. Ontañón, S. Brahma, Y . Zemlyan-\\nskiy, D. Uthus, M. Guo, J. Lee-Thorp, Y . Tay, et al., Colt5: Faster\\nlong-range transformers with conditional computation, arXiv preprint\\narXiv:2303.09752 (2023). 18\\n[188] J. Ding, S. Ma, L. Dong, X. Zhang, S. Huang, W. Wang, F. Wei,\\nLongnet: Scaling transformers to 1,000,000,000 tokens, arXiv preprint\\narXiv:2307.02486 (2023). 18\\n[189] Y . Chen, S. Qian, H. Tang, X. Lai, Z. Liu, S. Han, J. Jia, Longlora: Effi-\\ncient fine-tuning of long-context large language models, arXiv preprint\\narXiv:2309.12307 (2023). 18\\n[190] N. Ratner, Y . Levine, Y . Belinkov, O. Ram, I. Magar, O. Abend,\\nE. Karpas, A. Shashua, K. Leyton-Brown, Y . Shoham, Parallel context\\nwindows for large language models, in: Proceedings of the 61st Annual\\nMeeting of the Association for Computational Linguistics (V olume 1:\\nLong Papers), 2023, pp. 6383–6402. 18\\n[191] W. Wang, L. Dong, H. Cheng, X. Liu, X. Yan, J. Gao, F. Wei,\\nAugmenting language models with long-term memory, arXiv preprint\\narXiv:2306.07174 (2023). 18\\n[192] X. Xu, Z. Gou, W. Wu, Z.-Y . Niu, H. Wu, H. Wang, S. Wang, Long\\ntime no see! open-domain conversation with long-term persona memory,\\narXiv preprint arXiv:2203.05797 (2022). 18\\n[193] S. Borgeaud, A. Mensch, J. Ho ffmann, T. Cai, E. Rutherford, K. Milli-\\ncan, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark, et al.,\\nImproving language models by retrieving from trillions of tokens, in:\\nInternational conference on machine learning, PMLR, 2022, pp. 2206–\\n2240. 18, 19, 34\\n[194] W. Zhong, L. Guo, Q. Gao, Y . Wang, Memorybank: Enhanc-\\ning large language models with long-term memory, arXiv preprint\\narXiv:2305.10250 (2023). 18\\n[195] N. Shinn, F. Cassano, B. Labash, A. Gopinath, K. Narasimhan, S. Yao,\\nReflexion: Language agents with verbal reinforcement learning, arXiv\\npreprint arXiv:2303.11366 14 (2023). 18, 20\\n[196] C. Hu, J. Fu, C. Du, S. Luo, J. Zhao, H. Zhao, Chatdb: Augment-\\ning llms with databases as their symbolic memory, arXiv preprint\\narXiv:2306.03901 (2023). 18\\n[197] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y . Yang,\\nJ. Callan, G. Neubig, Active retrieval augmented generation, arXiv\\npreprint arXiv:2305.06983 (2023). 18\\n[198] O. Ram, Y . Levine, I. Dalmedigos, D. Muhlgay, A. Shashua, K. Leyton-\\nBrown, Y . Shoham, In-context retrieval-augmented language models,\\narXiv preprint arXiv:2302.00083 (2023). 18, 34\\n[199] X. Li, X. Qiu, Mot: Pre-thinking and recalling enable chatgpt to self-\\nimprove with memory-of-thoughts, arXiv preprint arXiv:2305.05181\\n(2023). 18\\n[200] D. Schuurmans, Memory augmented large language models are compu-\\ntationally universal, arXiv preprint arXiv:2301.04589 (2023). 18\\n[201] A. Modarressi, A. Imani, M. Fayyaz, H. Schütze, Ret-llm: Towards a\\ngeneral read-write memory for large language models, arXiv preprint\\narXiv:2305.14322 (2023). 18\\n[202] S. Robertson, H. Zaragoza, et al., The probabilistic relevance frame-\\nwork: Bm25 and beyond, Foundations and Trends® in Information Re-\\ntrieval 3 (4) (2009) 333–389. 18\\n[203] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, D. Zhou,\\nRationale-augmented ensembles in language models, arXiv preprint\\narXiv:2207.00747 (2022). 18\\n[204] F. Zhang, B. Chen, Y . Zhang, J. Liu, D. Zan, Y . Mao, J.-G. Lou, W. Chen,\\nRepocoder: Repository-level code completion through iterative retrieval\\nand generation, arXiv preprint arXiv:2303.12570 (2023). 18\\n[205] B. Wang, W. Ping, P. Xu, L. McAfee, Z. Liu, M. Shoeybi, Y . Dong,\\nO. Kuchaiev, B. Li, C. Xiao, et al., Shall we pretrain autoregressive\\nlanguage models with retrieval? a comprehensive study, arXiv preprint\\narXiv:2304.06762 (2023). 19\\n[206] L. Wang, N. Yang, F. Wei, Learning to retrieve in-context examples for\\nlarge language models, arXiv preprint arXiv:2307.07164 (2023). 19\\n[207] J. Liu, D. Shen, Y . Zhang, B. Dolan, L. Carin, W. Chen, What makes\\ngood in-context examples for gpt-3?, arXiv preprint arXiv:2101.06804\\n(2021). 19\\n[208] O. Rubin, J. Herzig, J. Berant, Learning to retrieve prompts for in-\\ncontext learning, arXiv preprint arXiv:2112.08633 (2021). 19\\n[209] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettle-\\nmoyer, W.-t. Yih, Replug: Retrieval-augmented black-box language\\nmodels, arXiv preprint arXiv:2301.12652 (2023). 19\\n[210] O. Rubin, J. Berant, Long-range language modeling with self-retrieval,\\narXiv preprint arXiv:2306.13421 (2023). 19\\n[211] K. Guu, K. Lee, Z. Tung, P. Pasupat, M. Chang, Retrieval augmented\\nlanguage model pre-training, in: International conference on machine\\nlearning, PMLR, 2020, pp. 3929–3938. 19\\n[212] S. Hofstätter, J. Chen, K. Raman, H. Zamani, Fid-light: E fficient and ef-\\nfective retrieval-augmented text generation, in: Proceedings of the 46th\\nInternational ACM SIGIR Conference on Research and Development in\\nInformation Retrieval, 2023, pp. 1437–1447. 19\\n[213] M. Komeili, K. Shuster, J. Weston, Internet-augmented dialogue gener-\\nation, arXiv preprint arXiv:2107.07566 (2021). 19\\n[214] A. Lazaridou, E. Gribovskaya, W. Stokowiec, N. Grigorev, Internet-\\naugmented language models through few-shot prompting for open-\\ndomain question answering, arXiv preprint arXiv:2203.05115 (2022).\\n19\\n[215] D. Gao, L. Ji, L. Zhou, K. Q. Lin, J. Chen, Z. Fan, M. Z. Shou, Assist-\\ngpt: A general multi-modal assistant that can plan, execute, inspect, and\\nlearn, arXiv preprint arXiv:2306.08640 (2023). 19\\n[216] P. Lu, B. Peng, H. Cheng, M. Galley, K.-W. Chang, Y . N. Wu, S.-C. Zhu,\\nJ. Gao, Chameleon: Plug-and-play compositional reasoning with large\\nlanguage models, arXiv preprint arXiv:2304.09842 (2023). 19, 20, 23\\n[217] B. Paranjape, S. Lundberg, S. Singh, H. Hajishirzi, L. Zettlemoyer, M. T.\\nRibeiro, Art: Automatic multi-step reasoning and tool-use for large lan-\\nguage models, arXiv preprint arXiv:2303.09014 (2023). 19\\n[218] C.-Y . Hsieh, S.-A. Chen, C.-L. Li, Y . Fujii, A. Ratner, C.-Y . Lee, R. Kr-\\nishna, T. Pfister, Tool documentation enables zero-shot tool-usage with\\nlarge language models, arXiv preprint arXiv:2308.00675 (2023). 19\\n[219] Y . Song, W. Xiong, D. Zhu, C. Li, K. Wang, Y . Tian, S. Li, Restgpt:\\nConnecting large language models with real-world applications via rest-\\nful apis, arXiv preprint arXiv:2306.06624 (2023). 19\\n[220] S. Hao, T. Liu, Z. Wang, Z. Hu, Toolkengpt: Augmenting frozen lan-\\nguage models with massive tools via tool embeddings, arXiv preprint\\narXiv:2305.11554 (2023). 19\\n[221] S. G. Patil, T. Zhang, X. Wang, J. E. Gonzalez, Gorilla: Large language\\nmodel connected with massive apis, arXiv preprint arXiv:2305.15334\\n(2023). 19\\n[222] Q. Xu, F. Hong, B. Li, C. Hu, Z. Chen, J. Zhang, On the tool manipu-\\nlation capability of open-source large language models, arXiv preprint\\narXiv:2305.16504 (2023). 19\\n[223] Y . Qin, S. Liang, Y . Ye, K. Zhu, L. Yan, Y . Lu, Y . Lin, X. Cong, X. Tang,\\nB. Qian, et al., Toolllm: Facilitating large language models to master\\n16000+ real-world apis, arXiv preprint arXiv:2307.16789 (2023). 19,\\n40'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 40, 'page_label': '41', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='20\\n[224] Y . Shen, K. Song, X. Tan, D. Li, W. Lu, Y . Zhuang, Hugginggpt: Solv-\\ning ai tasks with chatgpt and its friends in huggingface, arXiv preprint\\narXiv:2303.17580 (2023). 19, 20, 33\\n[225] Y . Liang, C. Wu, T. Song, W. Wu, Y . Xia, Y . Liu, Y . Ou, S. Lu, L. Ji,\\nS. Mao, et al., Taskmatrix. ai: Completing tasks by connecting foun-\\ndation models with millions of apis, arXiv preprint arXiv:2303.16434\\n(2023). 19\\n[226] D. Surís, S. Menon, C. V ondrick, Vipergpt: Visual inference via python\\nexecution for reasoning, arXiv preprint arXiv:2303.08128 (2023). 20\\n[227] A. Maedche, S. Morana, S. Schacht, D. Werth, J. Krumeich, Advanced\\nuser assistance systems, Business & Information Systems Engineering\\n58 (2016) 367–370. 20\\n[228] M. Campbell, A. J. Hoane Jr, F.-h. Hsu, Deep blue, Artificial intelligence\\n134 (1-2) (2002) 57–83. 20\\n[229] S. Hong, X. Zheng, J. Chen, Y . Cheng, J. Wang, C. Zhang, Z. Wang,\\nS. K. S. Yau, Z. Lin, L. Zhou, et al., Metagpt: Meta programming for\\nmulti-agent collaborative framework, arXiv preprint arXiv:2308.00352\\n(2023). 20\\n[230] Z. Xi, W. Chen, X. Guo, W. He, Y . Ding, B. Hong, M. Zhang, J. Wang,\\nS. Jin, E. Zhou, et al., The rise and potential of large language model\\nbased agents: A survey, arXiv preprint arXiv:2309.07864 (2023). 20\\n[231] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang,\\nX. Chen, Y . Lin, et al., A survey on large language model based au-\\ntonomous agents, arXiv preprint arXiv:2308.11432 (2023). 20\\n[232] W. Huang, P. Abbeel, D. Pathak, I. Mordatch, Language models as zero-\\nshot planners: Extracting actionable knowledge for embodied agents,\\nin: International Conference on Machine Learning, PMLR, 2022, pp.\\n9118–9147. 20\\n[233] S. Hao, Y . Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang, Z. Hu, Reason-\\ning with language model is planning with world model, arXiv preprint\\narXiv:2305.14992 (2023). 20, 33\\n[234] W. Yao, S. Heinecke, J. C. Niebles, Z. Liu, Y . Feng, L. Xue, R. Murthy,\\nZ. Chen, J. Zhang, D. Arpit, et al., Retroformer: Retrospective\\nlarge language agents with policy gradient optimization, arXiv preprint\\narXiv:2308.02151 (2023). 20, 33\\n[235] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng,\\nJ. Tompson, I. Mordatch, Y . Chebotar, P. Sermanet, T. Jackson,\\nN. Brown, L. Luu, S. Levine, K. Hausman, brian ichter, Inner mono-\\nlogue: Embodied reasoning through planning with language models, in:\\n6th Annual Conference on Robot Learning, 2022.\\nURL https://openreview.net/forum?id=3R3Pz5i0tye 20\\n[236] C. Jin, W. Tan, J. Yang, B. Liu, R. Song, L. Wang, J. Fu, Alphablock:\\nEmbodied finetuning for vision-language reasoning in robot manipula-\\ntion, arXiv preprint arXiv:2305.18898 (2023). 20, 33\\n[237] I. Singh, V . Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox,\\nJ. Thomason, A. Garg, Progprompt: Generating situated robot task plans\\nusing large language models, in: 2023 IEEE International Conference on\\nRobotics and Automation (ICRA), IEEE, 2023, pp. 11523–11530. 20,\\n33\\n[238] W. Yu, N. Gileadi, C. Fu, S. Kirmani, K.-H. Lee, M. G. Arenas, H.-T. L.\\nChiang, T. Erez, L. Hasenclever, J. Humplik, et al., Language to rewards\\nfor robotic skill synthesis, arXiv preprint arXiv:2306.08647 (2023). 20\\n[239] X. Tang, A. Zou, Z. Zhang, Y . Zhao, X. Zhang, A. Cohan, M. Gerstein,\\nMedagents: Large language models as collaborators for zero-shot med-\\nical reasoning, arXiv preprint arXiv:2311.10537 (2023). 20\\n[240] A. Brohan, Y . Chebotar, C. Finn, K. Hausman, A. Herzog, D. Ho,\\nJ. Ibarz, A. Irpan, E. Jang, R. Julian, et al., Do as i can, not as i say:\\nGrounding language in robotic a ffordances, in: Conference on Robot\\nLearning, PMLR, 2023, pp. 287–318. 20, 33\\n[241] H. Ha, P. Florence, S. Song, Scaling up and distilling down: Language-\\nguided robot skill acquisition, arXiv preprint arXiv:2307.14535 (2023).\\n20\\n[242] A. Rajvanshi, K. Sikka, X. Lin, B. Lee, H.-P. Chiu, A. Velasquez, Say-\\nnav: Grounding large language models for dynamic planning to navi-\\ngation in new environments, arXiv preprint arXiv:2309.04077 (2023).\\n20\\n[243] C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, Y . Su,\\nLlm-planner: Few-shot grounded planning for embodied agents with\\nlarge language models, arXiv preprint arXiv:2212.04088 (2022). 20\\n[244] V . S. Dorbala, J. F. Mullen Jr, D. Manocha, Can an embodied agent find\\nyour\" cat-shaped mug\"? llm-based zero-shot object navigation, arXiv\\npreprint arXiv:2303.03480 (2023). 20\\n[245] C. Huang, O. Mees, A. Zeng, W. Burgard, Visual language maps for\\nrobot navigation, in: 2023 IEEE International Conference on Robotics\\nand Automation (ICRA), IEEE, 2023, pp. 10608–10615. 20\\n[246] Y . Ding, X. Zhang, C. Paxton, S. Zhang, Task and motion planning\\nwith large language models for object rearrangement, arXiv preprint\\narXiv:2303.06247 (2023). 20, 33\\n[247] X. Liu, Y . Zheng, Z. Du, M. Ding, Y . Qian, Z. Yang, J. Tang, Gpt under-\\nstands, too, arXiv preprint arXiv:2103.10385 (2021). 20, 21\\n[248] G. Chen, F. Liu, Z. Meng, S. Liang, Revisiting parameter-e fficient tun-\\ning: Are we really there yet?, arXiv preprint arXiv:2202.07962 (2022).\\n20\\n[249] Y . Wang, S. Mukherjee, X. Liu, J. Gao, A. H. Awadallah, J. Gao,\\nAdamix: Mixture-of-adapter for parameter-efficient tuning of large lan-\\nguage models, arXiv preprint arXiv:2205.12410 1 (2) (2022) 4. 20\\n[250] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang,\\nW. Chen, Lora: Low-rank adaptation of large language models, arXiv\\npreprint arXiv:2106.09685 (2021). 21, 22, 23\\n[251] X. Liu, K. Ji, Y . Fu, W. Tam, Z. Du, Z. Yang, J. Tang, P-tuning: Prompt\\ntuning can be comparable to fine-tuning across scales and tasks, in: Pro-\\nceedings of the 60th Annual Meeting of the Association for Computa-\\ntional Linguistics (V olume 2: Short Papers), 2022, pp. 61–68. 21\\n[252] A. Razdaibiedina, Y . Mao, R. Hou, M. Khabsa, M. Lewis, A. Almahairi,\\nProgressive prompts: Continual learning for language models, arXiv\\npreprint arXiv:2301.12314 (2023). 21\\n[253] Z.-R. Zhang, C. Tan, H. Xu, C. Wang, J. Huang, S. Huang, To-\\nwards adaptive prefix tuning for parameter-e fficient language model\\nfine-tuning, arXiv preprint arXiv:2305.15212 (2023). 21\\n[254] E. B. Zaken, S. Ravfogel, Y . Goldberg, Bitfit: Simple parameter-\\nefficient fine-tuning for transformer-based masked language-models,\\narXiv preprint arXiv:2106.10199 (2021). 21\\n[255] T. Dettmers, M. Lewis, Y . Belkada, L. Zettlemoyer, Llm. int8 ():\\n8-bit matrix multiplication for transformers at scale, arXiv preprint\\narXiv:2208.07339 (2022). 21, 22\\n[256] E. Frantar, S. Ashkboos, T. Hoefler, D. Alistarh, Gptq: Accurate\\npost-training quantization for generative pre-trained transformers, arXiv\\npreprint arXiv:2210.17323 (2022). 21\\n[257] X. Wei, Y . Zhang, Y . Li, X. Zhang, R. Gong, J. Guo, X. Liu, Outlier sup-\\npression+: Accurate quantization of large language models by equiva-\\nlent and optimal shifting and scaling, arXiv preprint arXiv:2304.09145\\n(2023). 21\\n[258] E. Frantar, D. Alistarh, Optimal brain compression: A framework for\\naccurate post-training quantization and pruning, Advances in Neural In-\\nformation Processing Systems 35 (2022) 4475–4488. 21\\n[259] C. Lee, J. Jin, T. Kim, H. Kim, E. Park, Owq: Lessons learned from ac-\\ntivation outliers for weight quantization in large language models, arXiv\\npreprint arXiv:2306.02272 (2023). 21\\n[260] S. J. Kwon, J. Kim, J. Bae, K. M. Yoo, J.-H. Kim, B. Park, B. Kim, J.-\\nW. Ha, N. Sung, D. Lee, Alphatuning: Quantization-aware parameter-\\nefficient adaptation of large-scale pre-trained language models, arXiv\\npreprint arXiv:2210.03858 (2022). 21\\n[261] T. Dettmers, A. Pagnoni, A. Holtzman, L. Zettlemoyer, Qlora: E fficient\\nfinetuning of quantized llms, arXiv preprint arXiv:2305.14314 (2023).\\n21, 22\\n[262] Z. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock, Y . Mehdad, Y . Shi, R. Kr-\\nishnamoorthi, V . Chandra, Llm-qat: Data-free quantization aware train-\\ning for large language models, arXiv preprint arXiv:2305.17888 (2023).\\n21, 22\\n[263] Y . Guo, A. Yao, H. Zhao, Y . Chen, Network sketching: Exploiting bi-\\nnary structure in deep cnns, in: Proceedings of the IEEE Conference on\\nComputer Vision and Pattern Recognition, 2017, pp. 5955–5963. 21\\n[264] J. Kim, J. H. Lee, S. Kim, J. Park, K. M. Yoo, S. J. Kwon, D. Lee,\\nMemory-efficient fine-tuning of compressed large language models via\\nsub-4-bit integer quantization, arXiv preprint arXiv:2305.14152 (2023).\\n22\\n[265] M. Sun, Z. Liu, A. Bair, J. Z. Kolter, A simple and e ffective pruning\\napproach for large language models, arXiv preprint arXiv:2306.11695\\n(2023). 22\\n[266] Z. Wang, J. Wohlwend, T. Lei, Structured pruning of large language\\nmodels, arXiv preprint arXiv:1910.04732 (2019). 22\\n41'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 41, 'page_label': '42', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='[267] L. Yin, Y . Wu, Z. Zhang, C.-Y . Hsieh, Y . Wang, Y . Jia, M. Pechenizkiy,\\nY . Liang, Z. Wang, S. Liu, Outlier weighed layerwise sparsity (owl): A\\nmissing secret sauce for pruning llms to high sparsity, arXiv preprint\\narXiv:2310.05175 (2023). 22\\n[268] C. Tao, L. Hou, H. Bai, J. Wei, X. Jiang, Q. Liu, P. Luo, N. Wong,\\nStructured pruning for efficient generative pre-trained language models,\\nin: Findings of the Association for Computational Linguistics: ACL\\n2023, 2023, pp. 10880–10895. 22\\n[269] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y . Hasson, K. Lenc,\\nA. Mensch, K. Millican, M. Reynolds, et al., Flamingo: a visual lan-\\nguage model for few-shot learning, Advances in Neural Information Pro-\\ncessing Systems 35 (2022) 23716–23736. 22\\n[270] J. Li, D. Li, S. Savarese, S. Hoi, Blip-2: Bootstrapping language-image\\npre-training with frozen image encoders and large language models,\\narXiv preprint arXiv:2301.12597 (2023). 22\\n[271] H. Liu, C. Li, Q. Wu, Y . J. Lee, Visual instruction tuning, arXiv preprint\\narXiv:2304.08485 (2023). 22\\n[272] K. Li, Y . He, Y . Wang, Y . Li, W. Wang, P. Luo, Y . Wang, L. Wang,\\nY . Qiao, Videochat: Chat-centric video understanding, arXiv preprint\\narXiv:2305.06355 (2023). 22\\n[273] M. Maaz, H. Rasheed, S. Khan, F. S. Khan, Video-chatgpt: Towards de-\\ntailed video understanding via large vision and language models, arXiv\\npreprint arXiv:2306.05424 (2023). 22\\n[274] H. Zhang, X. Li, L. Bing, Video-llama: An instruction-tuned\\naudio-visual language model for video understanding, arXiv preprint\\narXiv:2306.02858 (2023). 22\\n[275] X. Mei, C. Meng, H. Liu, Q. Kong, T. Ko, C. Zhao, M. D. Plumbley,\\nY . Zou, W. Wang, Wavcaps: A chatgpt-assisted weakly-labelled au-\\ndio captioning dataset for audio-language multimodal research, arXiv\\npreprint arXiv:2303.17395 (2023). 22\\n[276] C. Lyu, M. Wu, L. Wang, X. Huang, B. Liu, Z. Du, S. Shi, Z. Tu, Macaw-\\nllm: Multi-modal language modeling with image, audio, video, and text\\nintegration, arXiv preprint arXiv:2306.09093 (2023). 22\\n[277] D. Zhu, J. Chen, X. Shen, X. Li, M. Elhoseiny, Minigpt-4: Enhancing\\nvision-language understanding with advanced large language models,\\narXiv preprint arXiv:2304.10592 (2023). 22\\n[278] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al.,\\nAn image is worth 16x16 words: Transformers for image recognition at\\nscale, arXiv preprint arXiv:2010.11929 (2020). 22\\n[279] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung,\\nS. Hoi, Instructblip: Towards general-purpose vision-language models\\nwith instruction tuning, arXiv preprint arXiv:2305.06500 (2023). 22\\n[280] Z. Xu, Y . Shen, L. Huang, Multiinstruct: Improving multi-modal zero-\\nshot learning via instruction tuning, arXiv preprint arXiv:2212.10773\\n(2022). 22\\n[281] Z. Zhao, L. Guo, T. Yue, S. Chen, S. Shao, X. Zhu, Z. Yuan, J. Liu,\\nChatbridge: Bridging modalities with large language model as a lan-\\nguage catalyst, arXiv preprint arXiv:2305.16103 (2023). 22\\n[282] L. Li, Y . Yin, S. Li, L. Chen, P. Wang, S. Ren, M. Li, Y . Yang, J. Xu,\\nX. Sun, et al., M3 it: A large-scale dataset towards multi-modal multi-\\nlingual instruction tuning, arXiv preprint arXiv:2306.04387 (2023). 22\\n[283] R. Pi, J. Gao, S. Diao, R. Pan, H. Dong, J. Zhang, L. Yao, J. Han,\\nH. Xu, L. K. T. Zhang, Detgpt: Detect what you need via reasoning,\\narXiv preprint arXiv:2305.14167 (2023). 22\\n[284] G. Luo, Y . Zhou, T. Ren, S. Chen, X. Sun, R. Ji, Cheap and quick:\\nEfficient vision-language instruction tuning for large language models,\\narXiv preprint arXiv:2305.15023 (2023). 22\\n[285] R. Zhang, J. Han, A. Zhou, X. Hu, S. Yan, P. Lu, H. Li, P. Gao, Y . Qiao,\\nLlama-adapter: E fficient fine-tuning of language models with zero-init\\nattention, arXiv preprint arXiv:2303.16199 (2023). 22\\n[286] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, I. Sutskever,\\nRobust speech recognition via large-scale weak supervision, in: Inter-\\nnational Conference on Machine Learning, PMLR, 2023, pp. 28492–\\n28518. 22\\n[287] Z. Zhang, A. Zhang, M. Li, H. Zhao, G. Karypis, A. Smola, Multi-\\nmodal chain-of-thought reasoning in language models, arXiv preprint\\narXiv:2302.00923 (2023). 23\\n[288] J. Ge, H. Luo, S. Qian, Y . Gan, J. Fu, S. Zhan, Chain of thought prompt\\ntuning in vision language models, arXiv preprint arXiv:2304.07919\\n(2023). 23\\n[289] C. Wu, S. Yin, W. Qi, X. Wang, Z. Tang, N. Duan, Visual chatgpt: Talk-\\ning, drawing and editing with visual foundation models, arXiv preprint\\narXiv:2303.04671 (2023). 23\\n[290] Z. Yang, L. Li, J. Wang, K. Lin, E. Azarnasab, F. Ahmed, Z. Liu, C. Liu,\\nM. Zeng, L. Wang, Mm-react: Prompting chatgpt for multimodal rea-\\nsoning and action, arXiv preprint arXiv:2303.11381 (2023). 23\\n[291] T. Wang, J. Zhang, J. Fei, Y . Ge, H. Zheng, Y . Tang, Z. Li, M. Gao,\\nS. Zhao, Y . Shan, et al., Caption anything: Interactive image descrip-\\ntion with diverse multimodal controls, arXiv preprint arXiv:2305.02677\\n(2023). 23\\n[292] X. Zhu, R. Zhang, B. He, Z. Zeng, S. Zhang, P. Gao, Pointclip v2:\\nAdapting clip for powerful 3d open-world learning, arXiv preprint\\narXiv:2211.11682 (2022). 23\\n[293] T. Gupta, A. Kembhavi, Visual programming: Compositional visual rea-\\nsoning without training, in: Proceedings of the IEEE /CVF Conference\\non Computer Vision and Pattern Recognition, 2023, pp. 14953–14962.\\n23\\n[294] P. Gao, Z. Jiang, H. You, P. Lu, S. C. Hoi, X. Wang, H. Li, Dynamic\\nfusion with intra-and inter-modality attention flow for visual question\\nanswering, in: Proceedings of the IEEE /CVF conference on computer\\nvision and pattern recognition, 2019, pp. 6639–6648. 23\\n[295] Z. Yu, J. Yu, Y . Cui, D. Tao, Q. Tian, Deep modular co-attention net-\\nworks for visual question answering, in: Proceedings of the IEEE /CVF\\nconference on computer vision and pattern recognition, 2019, pp. 6281–\\n6290. 23\\n[296] H. You, R. Sun, Z. Wang, L. Chen, G. Wang, H. A. Ayyubi, K.-\\nW. Chang, S.-F. Chang, Idealgpt: Iteratively decomposing vision\\nand language reasoning via large language models, arXiv preprint\\narXiv:2305.14985 (2023). 23\\n[297] R. Zhang, X. Hu, B. Li, S. Huang, H. Deng, Y . Qiao, P. Gao, H. Li,\\nPrompt, generate, then cache: Cascade of foundation models makes\\nstrong few-shot learners, in: Proceedings of the IEEE /CVF Conference\\non Computer Vision and Pattern Recognition, 2023, pp. 15211–15222.\\n23\\n[298] T. Q. Nguyen, J. Salazar, Transformers without tears: Improving the\\nnormalization of self-attention, CoRR abs/1910.05895 (2019). 24\\n[299] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\\nL. Zettlemoyer, V . Stoyanov, Roberta: A robustly optimized bert pre-\\ntraining approach, arXiv preprint arXiv:1907.11692 (2019). 24, 30\\n[300] X. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine,\\nD. Song, Koala: A dialogue model for academic research, Blog post\\n(April 2023).\\nURL https://bair.berkeley.edu/blog/2023/04/03/koala/\\n25\\n[301] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster,\\nJ. Phang, H. He, A. Thite, N. Nabeshima, et al., The pile: An\\n800gb dataset of diverse text for language modeling, arXiv preprint\\narXiv:2101.00027 (2020). 28, 30\\n[302] H. Laurençon, L. Saulnier, T. Wang, C. Akiki, A. Villanova del Moral,\\nT. Le Scao, L. V on Werra, C. Mou, E. González Ponferrada, H. Nguyen,\\net al., The bigscience roots corpus: A 1.6 tb composite multilingual\\ndataset, Advances in Neural Information Processing Systems 35 (2022)\\n31809–31826. 28\\n[303] Wikipedia.\\nURL https://en.wikipedia.org/wiki/Main_Page 28\\n[304] Together Computer, Redpajama: An open source recipe to reproduce\\nllama training dataset (Apr. 2023).\\nURL https://github.com/togethercomputer/\\nRedPajama-Data 28\\n[305] O. Honovich, T. Scialom, O. Levy, T. Schick, Unnatural instructions:\\nTuning language models with (almost) no human labor, arXiv preprint\\narXiv:2212.09689 (2022). 28\\n[306] Y . Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma,\\nD. Drain, S. Fort, D. Ganguli, T. Henighan, et al., Training a helpful and\\nharmless assistant with reinforcement learning from human feedback,\\narXiv preprint arXiv:2204.05862 (2022). 28\\n[307] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song,\\nJ. Steinhardt, Measuring massive multitask language understanding,\\narXiv preprint arXiv:2009.03300 (2020). 26, 29\\n[308] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch,\\nA. R. Brown, A. Santoro, A. Gupta, A. Garriga-Alonso, et al., Beyond\\n42'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 42, 'page_label': '43', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='the imitation game: Quantifying and extrapolating the capabilities of\\nlanguage models, arXiv preprint arXiv:2206.04615 (2022). 26, 29\\n[309] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, S. R. Bowman, Glue:\\nA multi-task benchmark and analysis platform for natural language un-\\nderstanding, arXiv preprint arXiv:1804.07461 (2018). 26, 29\\n[310] Y . Yao, Q. Dong, J. Guan, B. Cao, Z. Zhang, C. Xiao, X. Wang, F. Qi,\\nJ. Bao, J. Nie, et al., Cuge: A chinese language understanding and gen-\\neration evaluation benchmark, arXiv preprint arXiv:2112.13610 (2021).\\n29\\n[311] L. Xu, H. Hu, X. Zhang, L. Li, C. Cao, Y . Li, Y . Xu, K. Sun, D. Yu,\\nC. Yu, et al., Clue: A chinese language understanding evaluation bench-\\nmark, arXiv preprint arXiv:2004.05986 (2020). 29\\n[312] L. Xu, X. Lu, C. Yuan, X. Zhang, H. Xu, H. Yuan, G. Wei, X. Pan,\\nX. Tian, L. Qin, et al., Fewclue: A chinese few-shot learning evaluation\\nbenchmark, arXiv preprint arXiv:2107.07498 (2021). 29\\n[313] E. M. Smith, M. Williamson, K. Shuster, J. Weston, Y .-L. Boureau, Can\\nyou put it all together: Evaluating conversational agents’ ability to blend\\nskills, arXiv preprint arXiv:2004.08449 (2020). 29\\n[314] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga,\\nY . Zhang, D. Narayanan, Y . Wu, A. Kumar, et al., Holistic evaluation of\\nlanguage models, arXiv preprint arXiv:2211.09110 (2022). 29\\n[315] S. Park, J. Moon, S. Kim, W. I. Cho, J. Han, J. Park, C. Song, J. Kim,\\nY . Song, T. Oh, et al., Klue: Korean language understanding evaluation,\\narXiv preprint arXiv:2105.09680 (2021). 29\\n[316] S. Reddy, D. Chen, C. D. Manning, Coqa: A conversational question\\nanswering challenge, Transactions of the Association for Computational\\nLinguistics 7 (2019) 249–266. 27, 29\\n[317] M. T. Pilehvar, J. Camacho-Collados, Wic: 10,000 example\\npairs for evaluating context-sensitive representations, arXiv preprint\\narXiv:1808.09121 6 (2018). 27, 29\\n[318] S. Merity, C. Xiong, J. Bradbury, R. Socher, Pointer sentinel mixture\\nmodels, arXiv preprint arXiv:1609.07843 (2016). 28, 29\\n[319] J. W. Rae, A. Potapenko, S. M. Jayakumar, T. P. Lillicrap, Compres-\\nsive transformers for long-range sequence modelling, arXiv preprint\\narXiv:1911.05507 (2019). 28, 29\\n[320] X. Liu, Q. Chen, C. Deng, H. Zeng, J. Chen, D. Li, B. Tang, Lcqmc: A\\nlarge-scale chinese question matching corpus, in: Proceedings of the\\n27th international conference on computational linguistics, 2018, pp.\\n1952–1962. 28, 29\\n[321] S. Iyer, N. Dandekar, K. Csernai, First quora dataset re-\\nlease: Question pairs, https://quoradata.quora.com/\\nFirst-Quora-Dataset-Release-Question-Pairs . 29\\n[322] R. Rudinger, J. Naradowsky, B. Leonard, B. Van Durme, Gender bias in\\ncoreference resolution, arXiv preprint arXiv:1804.09301 (2018). 29\\n[323] M.-C. De Marne ffe, M. Simons, J. Tonhauser, The commitmentbank: In-\\nvestigating projection in naturally occurring discourse, in: proceedings\\nof Sinn und Bedeutung, V ol. 23, 2019, pp. 107–124. 29\\n[324] Z. Li, N. Ding, Z. Liu, H. Zheng, Y . Shen, Chinese relation extraction\\nwith multi-grained information and external linguistic knowledge, in:\\nProceedings of the 57th Annual Meeting of the Association for Compu-\\ntational Linguistics, 2019, pp. 4377–4386. 29\\n[325] J. Xu, J. Wen, X. Sun, Q. Su, A discourse-level named entity recognition\\nand relation extraction dataset for chinese literature text, arXiv preprint\\narXiv:1711.07010 (2017). 29\\n[326] J. Chen, Q. Chen, X. Liu, H. Yang, D. Lu, B. Tang, The bq corpus: A\\nlarge-scale domain-specific chinese corpus for sentence semantic equiv-\\nalence identification, in: Proceedings of the 2018 conference on empiri-\\ncal methods in natural language processing, 2018, pp. 4946–4951. 29\\n[327] B. Liu, D. Niu, H. Wei, J. Lin, Y . He, K. Lai, Y . Xu, Matching arti-\\ncle pairs with graphical decomposition and convolutions, arXiv preprint\\narXiv:1802.07459 (2018). 29\\n[328] P. Li, W. Li, Z. He, X. Wang, Y . Cao, J. Zhou, W. Xu, Dataset and neu-\\nral recurrent sequence labeling model for open-domain factoid question\\nanswering, arXiv preprint arXiv:1607.06275 (2016). 29\\n[329] N. Peng, M. Dredze, Named entity recognition for chinese social media\\nwith jointly trained embeddings, in: Proceedings of the 2015 conference\\non empirical methods in natural language processing, 2015, pp. 548–\\n554. 29\\n[330] W. Ling, D. Yogatama, C. Dyer, P. Blunsom, Program induction by ratio-\\nnale generation: Learning to solve and explain algebraic word problems,\\narXiv preprint arXiv:1705.04146 (2017). 29\\n[331] R. Weischedel, S. Pradhan, L. Ramshaw, M. Palmer, N. Xue, M. Mar-\\ncus, A. Taylor, C. Greenberg, E. Hovy, R. Belvin, et al., Ontonotes re-\\nlease 4.0, LDC2011T03, Philadelphia, Penn.: Linguistic Data Consor-\\ntium (2011). 29\\n[332] D. Vilares, C. Gómez-Rodríguez, Head-qa: A healthcare dataset for\\ncomplex reasoning, arXiv preprint arXiv:1906.04701 (2019). 29\\n[333] S. L. Blodgett, L. Green, B. O’Connor, Demographic dialectal variation\\nin social media: A case study of african-american english, arXiv preprint\\narXiv:1608.08868 (2016). 29\\n[334] N. Mostafazadeh, N. Chambers, X. He, D. Parikh, D. Batra, L. Van-\\nderwende, P. Kohli, J. Allen, A corpus and evaluation framework\\nfor deeper understanding of commonsense stories, arXiv preprint\\narXiv:1604.01696 (2016). 28, 29\\n[335] D. Paperno, G. Kruszewski, A. Lazaridou, Q. N. Pham, R. Bernardi,\\nS. Pezzelle, M. Baroni, G. Boleda, R. Fernández, The lambada dataset:\\nWord prediction requiring a broad discourse context, arXiv preprint\\narXiv:1606.06031 (2016). 28, 29\\n[336] B. Hu, Q. Chen, F. Zhu, Lcsts: A large scale chinese short text summa-\\nrization dataset, arXiv preprint arXiv:1506.05865 (2015). 29\\n[337] Z. Shao, M. Huang, J. Wen, W. Xu, X. Zhu, Long and diverse text gener-\\nation with planning-based hierarchical variational model, arXiv preprint\\narXiv:1908.06605 (2019). 29\\n[338] J. Novikova, O. Dušek, V . Rieser, The e2e dataset: New challenges for\\nend-to-end generation, arXiv preprint arXiv:1706.09254 (2017). 29\\n[339] C. Zheng, M. Huang, A. Sun, Chid: A large-scale chinese idiom dataset\\nfor cloze test, arXiv preprint arXiv:1906.01265 (2019). 29\\n[340] Y . Bisk, R. Zellers, J. Gao, Y . Choi, et al., Piqa: Reasoning about phys-\\nical commonsense in natural language, in: Proceedings of the AAAI\\nconference on artificial intelligence, V ol. 34, 2020, pp. 7432–7439. 28,\\n29\\n[341] M. Joshi, E. Choi, D. S. Weld, L. Zettlemoyer, Triviaqa: A large scale\\ndistantly supervised challenge dataset for reading comprehension, arXiv\\npreprint arXiv:1705.03551 (2017). 28, 29, 31\\n[342] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,\\nO. Tafjord, Think you have solved question answering? try arc, the ai2\\nreasoning challenge, arXiv preprint arXiv:1803.05457 (2018). 28, 29,\\n31\\n[343] S. Aroca-Ouellette, C. Paik, A. Roncone, K. Kann, Prost: Phys-\\nical reasoning of objects through space and time, arXiv preprint\\narXiv:2106.03634 (2021). 29\\n[344] T. Mihaylov, P. Clark, T. Khot, A. Sabharwal, Can a suit of armor con-\\nduct electricity? a new dataset for open book question answering, arXiv\\npreprint arXiv:1809.02789 (2018). 29\\n[345] T. C. Ferreira, C. Gardent, N. Ilinykh, C. Van Der Lee, S. Mille,\\nD. Moussallem, A. Shimorina, The 2020 bilingual, bi-directional\\nwebnlg+ shared task overview and evaluation results (webnlg + 2020),\\nin: Proceedings of the 3rd International Workshop on Natural Language\\nGeneration from the Semantic Web (WebNLG+), 2020. 29\\n[346] C. Xu, W. Zhou, T. Ge, K. Xu, J. McAuley, F. Wei, Blow the dog whistle:\\nA chinese dataset for cant understanding with common sense and world\\nknowledge, arXiv preprint arXiv:2104.02704 (2021). 29\\n[347] G. Lai, Q. Xie, H. Liu, Y . Yang, E. Hovy, Race: Large-scale\\nreading comprehension dataset from examinations, arXiv preprint\\narXiv:1704.04683 (2017). 29\\n[348] E. Choi, H. He, M. Iyyer, M. Yatskar, W.-t. Yih, Y . Choi, P. Liang,\\nL. Zettlemoyer, Quac: Question answering in context, arXiv preprint\\narXiv:1808.07036 (2018). 29\\n[349] M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth, J. Berant, Did aristo-\\ntle use a laptop? a question answering benchmark with implicit reason-\\ning strategies, Transactions of the Association for Computational Lin-\\nguistics 9 (2021) 346–361. 29, 31\\n[350] J. Boyd-Graber, B. Satino ff, H. He, H. Daumé III, Besting the quiz mas-\\nter: Crowdsourcing incremental classification games, in: Proceedings of\\nthe 2012 joint conference on empirical methods in natural language pro-\\ncessing and computational natural language learning, 2012, pp. 1290–\\n1301. 29\\n[351] S. Zhang, X. Zhang, H. Wang, J. Cheng, P. Li, Z. Ding, Chinese medical\\nquestion answer matching using end-to-end character-level multi-scale\\ncnns, Applied Sciences 7 (8) (2017) 767. 29\\n[352] S. Zhang, X. Zhang, H. Wang, L. Guo, S. Liu, Multi-scale attentive in-\\nteraction networks for chinese medical question answer selection, IEEE\\n43'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 43, 'page_label': '44', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='Access 6 (2018) 74061–74071. 29\\n[353] C. Xu, J. Pei, H. Wu, Y . Liu, C. Li, Matinf: A jointly labeled large-scale\\ndataset for classification, question answering and summarization, arXiv\\npreprint arXiv:2004.12302 (2020). 29\\n[354] K. Sakaguchi, R. L. Bras, C. Bhagavatula, Y . Choi, Winogrande: An\\nadversarial winograd schema challenge at scale, Communications of the\\nACM 64 (9) (2021) 99–106. 27, 29\\n[355] R. Zellers, A. Holtzman, Y . Bisk, A. Farhadi, Y . Choi, Hellaswag: Can a\\nmachine really finish your sentence?, arXiv preprint arXiv:1905.07830\\n(2019). 29\\n[356] M. Roemmele, C. A. Bejan, A. S. Gordon, Choice of plausible alter-\\nnatives: An evaluation of commonsense causal reasoning., in: AAAI\\nspring symposium: logical formalizations of commonsense reasoning,\\n2011, pp. 90–95. 29\\n[357] H. Levesque, E. Davis, L. Morgenstern, The winograd schema chal-\\nlenge, in: Thirteenth international conference on the principles of knowl-\\nedge representation and reasoning, 2012. 27, 29\\n[358] A. Talmor, J. Herzig, N. Lourie, J. Berant, Commonsenseqa: A question\\nanswering challenge targeting commonsense knowledge, arXiv preprint\\narXiv:1811.00937 (2018). 29, 31\\n[359] M. Sap, H. Rashkin, D. Chen, R. LeBras, Y . Choi, Socialiqa:\\nCommonsense reasoning about social interactions, arXiv preprint\\narXiv:1904.09728 (2019). 29\\n[360] K. Sun, D. Yu, D. Yu, C. Cardie, Investigating prior knowledge for chal-\\nlenging chinese machine reading comprehension, Transactions of the\\nAssociation for Computational Linguistics 8 (2020) 141–155. 29\\n[361] S. Zhang, X. Liu, J. Liu, J. Gao, K. Duh, B. Van Durme, Record: Bridg-\\ning the gap between human and machine commonsense reading compre-\\nhension, arXiv preprint arXiv:1810.12885 (2018). 29\\n[362] P. Rajpurkar, J. Zhang, K. Lopyrev, P. Liang, Squad: 100,000+ questions\\nfor machine comprehension of text, arXiv preprint arXiv:1606.05250\\n(2016). 29, 31\\n[363] C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins,\\nK. Toutanova, Boolq: Exploring the surprising di fficulty of natural\\nyes/no questions, arXiv preprint arXiv:1905.10044 (2019). 29, 31\\n[364] P. Rajpurkar, R. Jia, P. Liang, Know what you don’t know: Unanswer-\\nable questions for squad, arXiv preprint arXiv:1806.03822 (2018). 29,\\n31\\n[365] D. Dua, Y . Wang, P. Dasigi, G. Stanovsky, S. Singh, M. Gardner, Drop:\\nA reading comprehension benchmark requiring discrete reasoning over\\nparagraphs, arXiv preprint arXiv:1903.00161 (2019). 29, 31\\n[366] I. Dagan, O. Glickman, B. Magnini, The pascal recognising textual en-\\ntailment challenge, in: Machine learning challenges workshop, Springer,\\n2005, pp. 177–190. 29, 31\\n[367] Y . Chang, M. Narang, H. Suzuki, G. Cao, J. Gao, Y . Bisk, Webqa: Mul-\\ntihop and multimodal qa, in: Proceedings of the IEEE /CVF Conference\\non Computer Vision and Pattern Recognition, 2022, pp. 16495–16504.\\n29, 31\\n[368] Y . Cui, T. Liu, Z. Chen, W. Ma, S. Wang, G. Hu, Dataset for the first\\nevaluation on chinese machine reading comprehension, arXiv preprint\\narXiv:1709.08299 (2017). 29\\n[369] Y . Cui, T. Liu, W. Che, L. Xiao, Z. Chen, W. Ma, S. Wang, G. Hu,\\nA span-extraction dataset for chinese machine reading comprehension,\\narXiv preprint arXiv:1810.07366 (2018). 29, 31\\n[370] Y . Cui, T. Liu, Z. Yang, Z. Chen, W. Ma, W. Che, S. Wang, G. Hu,\\nA sentence cloze dataset for chinese machine reading comprehension,\\narXiv preprint arXiv:2004.03116 (2020). 29\\n[371] Y . Li, T. Liu, D. Li, Q. Li, J. Shi, Y . Wang, Character-based bilstm-crf\\nincorporating pos and dictionaries for chinese opinion target extraction,\\nin: Asian Conference on Machine Learning, PMLR, 2018, pp. 518–533.\\n29\\n[372] D. Khashabi, S. Chaturvedi, M. Roth, S. Upadhyay, D. Roth, Look-\\ning beyond the surface: A challenge set for reading comprehension\\nover multiple sentences, in: Proceedings of the 2018 Conference of the\\nNorth American Chapter of the Association for Computational Linguis-\\ntics: Human Language Technologies, V olume 1 (Long Papers), 2018,\\npp. 252–262. 29\\n[373] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Al-\\nberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, et al., Natural ques-\\ntions: a benchmark for question answering research, Transactions of the\\nAssociation for Computational Linguistics 7 (2019) 453–466. 29\\n[374] C. C. Shao, T. Liu, Y . Lai, Y . Tseng, S. Tsai, Drcd: A chinese ma-\\nchine reading comprehension dataset, arXiv preprint arXiv:1806.00920\\n(2018). 29\\n[375] W. He, K. Liu, J. Liu, Y . Lyu, S. Zhao, X. Xiao, Y . Liu, Y . Wang, H. Wu,\\nQ. She, et al., Dureader: a chinese machine reading comprehension\\ndataset from real-world applications, arXiv preprint arXiv:1711.05073\\n(2017). 29\\n[376] H. Tang, J. Liu, H. Li, Y . Hong, H. Wu, H. Wang, Dureaderrobust: A\\nchinese dataset towards evaluating the robustness of machine reading\\ncomprehension models, arXiv preprint arXiv:2004.11142 (2020). 29\\n[377] J. Welbl, N. F. Liu, M. Gardner, Crowdsourcing multiple choice science\\nquestions, arXiv preprint arXiv:1707.06209 (2017). 29\\n[378] C. Xiong, Z. Dai, J. Callan, Z. Liu, R. Power, End-to-end neural ad-hoc\\nranking with kernel pooling, in: Proceedings of the 40th International\\nACM SIGIR conference on research and development in information\\nretrieval, 2017, pp. 55–64. 29\\n[379] A. Peñas, E. Hovy, P. Forner, Á. Rodrigo, R. Sutcli ffe, R. Morante,\\nQa4mre 2011-2013: Overview of question answering for machine read-\\ning evaluation, in: Information Access Evaluation. Multilinguality, Mul-\\ntimodality, and Visualization: 4th International Conference of the CLEF\\nInitiative, CLEF 2013, Valencia, Spain, September 23-26, 2013. Pro-\\nceedings 4, Springer, 2013, pp. 303–320. 29\\n[380] S. Lim, M. Kim, J. Lee, Korquad1. 0: Korean qa dataset for machine\\nreading comprehension, arXiv preprint arXiv:1909.07005 (2019). 29\\n[381] C. Xiao, H. Zhong, Z. Guo, C. Tu, Z. Liu, M. Sun, Y . Feng, X. Han,\\nZ. Hu, H. Wang, et al., Cail2018: A large-scale legal dataset for judg-\\nment prediction, arXiv preprint arXiv:1807.02478 (2018). 29\\n[382] D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo,\\nC. Burns, S. Puranik, H. He, D. Song, et al., Measuring coding challenge\\ncompetence with apps, arXiv preprint arXiv:2105.09938 (2021). 29, 31\\n[383] Y . Wang, X. Liu, S. Shi, Deep neural solver for math word problems,\\nin: Proceedings of the 2017 conference on empirical methods in natural\\nlanguage processing, 2017, pp. 845–854. 29, 31\\n[384] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,\\nM. Plappert, J. Tworek, J. Hilton, R. Nakano, et al., Training verifiers\\nto solve math word problems, arXiv preprint arXiv:2110.14168 (2021).\\n29, 31\\n[385] J. Austin, A. Odena, M. I. Nye, M. Bosma, H. Michalewski, D. Dohan,\\nE. Jiang, C. J. Cai, M. Terry, Q. V . Le, C. Sutton, Program synthesis with\\nlarge language models, CoRR abs/2108.07732 (2021). 29\\n[386] F. Shi, M. Suzgun, M. Freitag, X. Wang, S. Srivats, S. V osoughi, H. W.\\nChung, Y . Tay, S. Ruder, D. Zhou, et al., Language models are mul-\\ntilingual chain-of-thought reasoners, arXiv preprint arXiv:2210.03057\\n(2022). 29\\n[387] S. Roy, D. Roth, Solving general arithmetic word problems, arXiv\\npreprint arXiv:1608.01413 (2016). 29\\n[388] S.-Y . Miao, C.-C. Liang, K.-Y . Su, A diverse corpus for evaluating\\nand developing english math word problem solvers, arXiv preprint\\narXiv:2106.15772 (2021). 29\\n[389] R. Koncel-Kedziorski, S. Roy, A. Amini, N. Kushman, H. Hajishirzi,\\nMawps: A math word problem repository, in: Proceedings of the 2016\\nconference of the north american chapter of the association for computa-\\ntional linguistics: human language technologies, 2016, pp. 1152–1157.\\n29\\n[390] A. Patel, S. Bhattamishra, N. Goyal, Are nlp models really able to solve\\nsimple math word problems?, arXiv preprint arXiv:2103.07191 (2021).\\n29\\n[391] Y . Lai, C. Li, Y . Wang, T. Zhang, R. Zhong, L. Zettlemoyer, W.-t. Yih,\\nD. Fried, S. Wang, T. Yu, Ds-1000: A natural and reliable benchmark for\\ndata science code generation, in: International Conference on Machine\\nLearning, PMLR, 2023, pp. 18319–18345. 29\\n[392] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan,\\nE. Jiang, C. Cai, M. Terry, Q. Le, et al., Program synthesis with large\\nlanguage models, arXiv preprint arXiv:2108.07732 (2021). 29\\n[393] Y . Nie, A. Williams, E. Dinan, M. Bansal, J. Weston, D. Kiela, Adver-\\nsarial nli: A new benchmark for natural language understanding, arXiv\\npreprint arXiv:1910.14599 (2019). 29, 31\\n[394] A. Williams, N. Nangia, S. R. Bowman, A broad-coverage challenge\\ncorpus for sentence understanding through inference, arXiv preprint\\narXiv:1704.05426 (2017). 29\\n[395] R. T. McCoy, E. Pavlick, T. Linzen, Right for the wrong reasons: Diag-\\n44'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 44, 'page_label': '45', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='nosing syntactic heuristics in natural language inference, arXiv preprint\\narXiv:1902.01007 (2019). 29\\n[396] J. Liu, L. Cui, H. Liu, D. Huang, Y . Wang, Y . Zhang, Logiqa: A chal-\\nlenge dataset for machine reading comprehension with logical reason-\\ning, arXiv preprint arXiv:2007.08124 (2020). 29\\n[397] P. Lewis, B. O ˘guz, R. Rinott, S. Riedel, H. Schwenk, Mlqa: Eval-\\nuating cross-lingual extractive question answering, arXiv preprint\\narXiv:1910.07475 (2019). 29\\n[398] A. Conneau, G. Lample, R. Rinott, A. Williams, S. R. Bowman,\\nH. Schwenk, V . Stoyanov, Xnli: Evaluating cross-lingual sentence rep-\\nresentations, arXiv preprint arXiv:1809.05053 (2018). 29, 31\\n[399] Y . Yang, Y . Zhang, C. Tar, J. Baldridge, Paws-x: A cross-\\nlingual adversarial dataset for paraphrase identification, arXiv preprint\\narXiv:1908.11828 (2019). 29, 31\\n[400] S. Narayan, S. B. Cohen, M. Lapata, Don’t give me the details, just the\\nsummary!, Topic-Aware Convolutional Neural Networks for Extreme\\nSummarization. ArXiv, abs (1808). 29\\n[401] E. M. Ponti, G. Glavaš, O. Majewska, Q. Liu, I. Vuli ´c, A. Korhonen,\\nXcopa: A multilingual dataset for causal commonsense reasoning, arXiv\\npreprint arXiv:2005.00333 (2020). 29\\n[402] A. Tikhonov, M. Ryabinin, It’s all in the heads: Using attention heads\\nas a baseline for cross-lingual transfer in commonsense reasoning, arXiv\\npreprint arXiv:2106.12066 (2021). 29\\n[403] J. H. Clark, E. Choi, M. Collins, D. Garrette, T. Kwiatkowski, V . Niko-\\nlaev, J. Palomaki, Tydi qa: A benchmark for information-seeking ques-\\ntion answering in typologically diverse languages, Transactions of the\\nAssociation for Computational Linguistics 8 (2020) 454–470. 29\\n[404] T. Scialom, P.-A. Dray, S. Lamprier, B. Piwowarski, J. Staiano,\\nMlsum: The multilingual summarization corpus, arXiv preprint\\narXiv:2004.14900 (2020). 29\\n[405] S. Lin, J. Hilton, O. Evans, Truthfulqa: Measuring how models mimic\\nhuman falsehoods, arXiv preprint arXiv:2109.07958 (2021). 29, 32\\n[406] I. Augenstein, C. Lioma, D. Wang, L. C. Lima, C. Hansen,\\nC. Hansen, J. G. Simonsen, Multifc: A real-world multi-domain\\ndataset for evidence-based fact checking of claims, arXiv preprint\\narXiv:1909.03242 (2019). 29\\n[407] J. Thorne, A. Vlachos, C. Christodoulopoulos, A. Mittal, Fever: a\\nlarge-scale dataset for fact extraction and verification, arXiv preprint\\narXiv:1803.05355 (2018). 29\\n[408] I. Mollas, Z. Chrysopoulou, S. Karlos, G. Tsoumakas, Ethos: an online\\nhate speech detection dataset, arXiv preprint arXiv:2006.08328 (2020).\\n29, 32\\n[409] M. Nadeem, A. Bethke, S. Reddy, Stereoset: Measuring stereotypical\\nbias in pretrained language models, arXiv preprint arXiv:2004.09456\\n(2020). 29, 32\\n[410] A. Parrish, A. Chen, N. Nangia, V . Padmakumar, J. Phang, J. Thomp-\\nson, P. M. Htut, S. R. Bowman, Bbq: A hand-built bias benchmark for\\nquestion answering, arXiv preprint arXiv:2110.08193 (2021). 29\\n[411] J. Zhao, T. Wang, M. Yatskar, V . Ordonez, K.-W. Chang, Gender bias\\nin coreference resolution: Evaluation and debiasing methods, arXiv\\npreprint arXiv:1804.06876 (2018). 29\\n[412] N. Nangia, C. Vania, R. Bhalerao, S. R. Bowman, Crows-pairs: A chal-\\nlenge dataset for measuring social biases in masked language models,\\narXiv preprint arXiv:2010.00133 (2020). 29\\n[413] S. Gehman, S. Gururangan, M. Sap, Y . Choi, N. A. Smith, Realtoxic-\\nityprompts: Evaluating neural toxic degeneration in language models,\\narXiv preprint arXiv:2009.11462 (2020). 29\\n[414] D. Borkan, L. Dixon, J. Sorensen, N. Thain, L. Vasserman, Nuanced\\nmetrics for measuring unintended bias with real data for text classifica-\\ntion, in: Companion proceedings of the 2019 world wide web confer-\\nence, 2019, pp. 491–500. 29\\n[415] O. Bojar, R. Chatterjee, C. Federmann, Y . Graham, B. Haddow,\\nM. Huck, A. J. Yepes, P. Koehn, V . Logacheva, C. Monz, et al., Find-\\nings of the 2016 conference on machine translation, in: Proceedings of\\nthe First Conference on Machine Translation: V olume 2, Shared Task\\nPapers, 2016, pp. 131–198. 29\\n[416] B. Loïc, B. Magdalena, B. Ond ˇrej, F. Christian, G. Yvette, G. Ro-\\nman, H. Barry, H. Matthias, J. Eric, K. Tom, et al., Findings of the\\n2020 conference on machine translation (wmt20), in: Proceedings of\\nthe Fifth Conference on Machine Translation, Association for Compu-\\ntational Linguistics„ 2020, pp. 1–55. 29\\n[417] W. Li, F. Qi, M. Sun, X. Yi, J. Zhang, Ccpm: A chinese classical poetry\\nmatching dataset, arXiv preprint arXiv:2106.01979 (2021). 29\\n[418] E. Dinan, S. Roller, K. Shuster, A. Fan, M. Auli, J. Weston, Wizard of\\nwikipedia: Knowledge-powered conversational agents, arXiv preprint\\narXiv:1811.01241 (2018). 29\\n[419] H. Rashkin, E. M. Smith, M. Li, Y .-L. Boureau, Towards empathetic\\nopen-domain conversation models: A new benchmark and dataset, arXiv\\npreprint arXiv:1811.00207 (2018). 29\\n[420] E. Dinan, V . Logacheva, V . Malykh, A. Miller, K. Shuster, J. Urbanek,\\nD. Kiela, A. Szlam, I. Serban, R. Lowe, et al., The second conversa-\\ntional intelligence challenge (convai2), in: The NeurIPS’18 Competi-\\ntion: From Machine Learning to Intelligent Conversations, Springer,\\n2020, pp. 187–208. 29\\n[421] H. Zhou, C. Zheng, K. Huang, M. Huang, X. Zhu, Kdconv: A chinese\\nmulti-domain dialogue dataset towards multi-turn knowledge-driven\\nconversation, arXiv preprint arXiv:2004.04100 (2020). 29\\n[422] L. CO, Iflytek: a multiple categories chinese text classifier. competition\\nofficial website (2019). 29\\n[423] J. Baumgartner, S. Zannettou, B. Keegan, M. Squire, J. Blackburn, The\\npushshift reddit dataset, in: Proceedings of the international AAAI con-\\nference on web and social media, V ol. 14, 2020, pp. 830–839. 30\\n[424] A. Fan, Y . Jernite, E. Perez, D. Grangier, J. Weston, M. Auli, Eli5: Long\\nform question answering, arXiv preprint arXiv:1907.09190 (2019). 31\\n[425] Y . Wang, S. Mishra, P. Alipoormolabashi, Y . Kordi, A. Mirzaei,\\nA. Arunkumar, A. Ashok, A. S. Dhanasekaran, A. Naik, D. Stap, et al.,\\nBenchmarking generalization via in-context instructions on 1,600+ lan-\\nguage tasks, arXiv preprint arXiv:2204.07705 (2022). 31\\n[426] T. Xie, C. H. Wu, P. Shi, R. Zhong, T. Scholak, M. Yasunaga, C.-S. Wu,\\nM. Zhong, P. Yin, S. I. Wang, et al., Unifiedskg: Unifying and multi-\\ntasking structured knowledge grounding with text-to-text language mod-\\nels, arXiv preprint arXiv:2201.05966 (2022). 31\\n[427] Q. Ye, B. Y . Lin, X. Ren, Crossfit: A few-shot learning challenge\\nfor cross-task generalization in nlp, arXiv preprint arXiv:2104.08835\\n(2021). 31\\n[428] V . Aribandi, Y . Tay, T. Schuster, J. Rao, H. S. Zheng, S. V . Mehta,\\nH. Zhuang, V . Q. Tran, D. Bahri, J. Ni, et al., Ext5: Towards extreme\\nmulti-task scaling for transfer learning, arXiv preprint arXiv:2111.10952\\n(2021). 31\\n[429] A. Williams, N. Nangia, S. Bowman, A broad-coverage challenge cor-\\npus for sentence understanding through inference, in: Proceedings of\\nthe 2018 Conference of the North American Chapter of the Associ-\\nation for Computational Linguistics: Human Language Technologies,\\nV olume 1 (Long Papers), Association for Computational Linguistics,\\nNew Orleans, Louisiana, 2018, pp. 1112–1122. doi:10.18653/v1/\\nN18-1101.\\nURL https://aclanthology.org/N18-1101 31\\n[430] Y . Zhang, J. Baldridge, L. He, PAWS: Paraphrase adversaries from word\\nscrambling, in: Proceedings of the 2019 Conference of the North Amer-\\nican Chapter of the Association for Computational Linguistics: Human\\nLanguage Technologies, V olume 1 (Long and Short Papers), Associa-\\ntion for Computational Linguistics, Minneapolis, Minnesota, 2019, pp.\\n1298–1308. doi:10.18653/v1/N19-1131.\\nURL https://aclanthology.org/N19-1131 32\\n[431] C. Qin, A. Zhang, Z. Zhang, J. Chen, M. Yasunaga, D. Yang, Is chat-\\nGPT a general-purpose natural language processing task solver?, in: The\\n2023 Conference on Empirical Methods in Natural Language Process-\\ning, 2023.\\nURL https://openreview.net/forum?id=u03xn1COsO 32\\n[432] M. U. Hadi, R. Qureshi, A. Shah, M. Irfan, A. Zafar, M. B. Shaikh,\\nN. Akhtar, J. Wu, S. Mirjalili, et al., Large language models: a com-\\nprehensive survey of its applications, challenges, limitations, and future\\nprospects, TechRxiv (2023). 32\\n[433] X. L. Dong, S. Moon, Y . E. Xu, K. Malik, Z. Yu, Towards next-\\ngeneration intelligent assistants leveraging llm techniques, in: Proceed-\\nings of the 29th ACM SIGKDD Conference on Knowledge Discovery\\nand Data Mining, 2023, pp. 5792–5793. 32\\n[434] K. Pandya, M. Holia, Automating customer service using langchain:\\nBuilding custom open-source gpt chatbot for organizations, arXiv\\npreprint arXiv:2310.05421 (2023). 32\\n[435] J. Li, B. Hui, G. Qu, B. Li, J. Yang, B. Li, B. Wang, B. Qin, R. Cao,\\nR. Geng, et al., Can llm already serve as a database interface? a\\n45'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 45, 'page_label': '46', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='big bench for large-scale database grounded text-to-sqls, arXiv preprint\\narXiv:2305.03111 (2023). 32\\n[436] A. Rao, J. Kim, M. Kamineni, M. Pang, W. Lie, M. D. Succi, Evaluating\\nchatgpt as an adjunct for radiologic decision-making, medRxiv (2023)\\n2023–02. 32\\n[437] M. Benary, X. D. Wang, M. Schmidt, D. Soll, G. Hilfenhaus, M. Nas-\\nsir, C. Sigler, M. Knödler, U. Keller, D. Beule, et al., Leveraging large\\nlanguage models for decision support in personalized oncology, JAMA\\nNetwork Open 6 (11) (2023) e2343689–e2343689. 32\\n[438] C. M. Chiesa-Estomba, J. R. Lechien, L. A. Vaira, A. Brunet, G. Cam-\\nmaroto, M. Mayo-Yanez, A. Sanchez-Barrueco, C. Saga-Gutierrez, Ex-\\nploring the potential of chat-gpt as a supportive tool for sialendoscopy\\nclinical decision making and patient information support, European\\nArchives of Oto-Rhino-Laryngology (2023) 1–6. 32\\n[439] S. Montagna, S. Ferretti, L. C. Klopfenstein, A. Florio, M. F. Pengo,\\nData decentralisation of llm-based chatbot systems in chronic disease\\nself-management, in: Proceedings of the 2023 ACM Conference on In-\\nformation Technology for Social Good, 2023, pp. 205–212. 32\\n[440] D. Bill, T. Eriksson, Fine-tuning a llm using reinforcement learning from\\nhuman feedback for a therapy chatbot application (2023). 32\\n[441] M. Abbasian, I. Azimi, A. M. Rahmani, R. Jain, Conversational health\\nagents: A personalized llm-powered agent framework, arXiv preprint\\narXiv:2310.02374 (2023). 32\\n[442] K. V . Lemley, Does chatgpt help us understand the medical literature?,\\nJournal of the American Society of Nephrology (2023) 10–1681. 32\\n[443] S. Pal, M. Bhattacharya, S.-S. Lee, C. Chakraborty, A domain-specific\\nnext-generation large language model (llm) or chatgpt is required for\\nbiomedical engineering and research, Annals of Biomedical Engineering\\n(2023) 1–4. 32\\n[444] Y . Du, S. Zhao, Y . Chen, R. Bai, J. Liu, H. Wu, H. Wang, B. Qin, The\\ncalla dataset: Probing llms’ interactive knowledge acquisition from chi-\\nnese medical literature, arXiv preprint arXiv:2309.04198 (2023). 32\\n[445] A. Abd-Alrazaq, R. AlSaad, D. Alhuwail, A. Ahmed, P. M. Healy,\\nS. Latifi, S. Aziz, R. Damseh, S. A. Alrazak, J. Sheikh, et al., Large\\nlanguage models in medical education: Opportunities, challenges, and\\nfuture directions, JMIR Medical Education 9 (1) (2023) e48291. 32\\n[446] A. B. Mbakwe, I. Lourentzou, L. A. Celi, O. J. Mechanic, A. Dagan,\\nChatgpt passing usmle shines a spotlight on the flaws of medical educa-\\ntion (2023). 32\\n[447] S. Ahn, The impending impacts of large language models on medical\\neducation, Korean Journal of Medical Education 35 (1) (2023) 103. 32\\n[448] E. Waisberg, J. Ong, M. Masalkhi, A. G. Lee, Large language model\\n(llm)-driven chatbots for neuro-ophthalmic medical education, Eye\\n(2023) 1–3. 32\\n[449] G. Deiana, M. Dettori, A. Arghittu, A. Azara, G. Gabutti, P. Castiglia,\\nArtificial intelligence and public health: Evaluating chatgpt responses to\\nvaccination myths and misconceptions, Vaccines 11 (7) (2023) 1217. 32\\n[450] L. De Angelis, F. Baglivo, G. Arzilli, G. P. Privitera, P. Ferragina, A. E.\\nTozzi, C. Rizzo, Chatgpt and the rise of large language models: the new\\nai-driven infodemic threat in public health, Frontiers in Public Health 11\\n(2023) 1166120. 32\\n[451] N. L. Rane, A. Tawde, S. P. Choudhary, J. Rane, Contribution and per-\\nformance of chatgpt and other large language models (llm) for scientific\\nand research advancements: a double-edged sword, International Re-\\nsearch Journal of Modernization in Engineering Technology and Science\\n5 (10) (2023) 875–899. 32\\n[452] W. Dai, J. Lin, H. Jin, T. Li, Y .-S. Tsai, D. Gaševi´c, G. Chen, Can large\\nlanguage models provide feedback to students? a case study on chatgpt,\\nin: 2023 IEEE International Conference on Advanced Learning Tech-\\nnologies (ICALT), IEEE, 2023, pp. 323–325. 32\\n[453] E. Kasneci, K. Seßler, S. Küchemann, M. Bannert, D. Dementieva,\\nF. Fischer, U. Gasser, G. Groh, S. Günnemann, E. Hüllermeier, et al.,\\nChatgpt for good? on opportunities and challenges of large language\\nmodels for education, Learning and individual di fferences 103 (2023)\\n102274. 32\\n[454] N. Rane, Enhancing the quality of teaching and learning through chat-\\ngpt and similar large language models: Challenges, future prospects,\\nand ethical considerations in education, Future Prospects, and Ethical\\nConsiderations in Education (September 15, 2023) (2023). 32\\n[455] J. C. Young, M. Shishido, Investigating openai’s chatgpt potentials in\\ngenerating chatbot’s dialogue for english as a foreign language learning,\\nInternational Journal of Advanced Computer Science and Applications\\n14 (6) (2023). 32\\n[456] J. Irons, C. Mason, P. Cooper, S. Sidra, A. Reeson, C. Paris, Exploring\\nthe impacts of chatgpt on future scientific work, SocArXiv (2023). 32\\n[457] P. G. Schmidt, A. J. Meir, Using generative ai for literature searches and\\nscholarly writing: Is the integrity of the scientific discourse in jeopardy?,\\narXiv preprint arXiv:2311.06981 (2023). 32\\n[458] Y . Zheng, H. Y . Koh, J. Ju, A. T. Nguyen, L. T. May, G. I. Webb, S. Pan,\\nLarge language models for scientific synthesis, inference and explana-\\ntion, arXiv preprint arXiv:2310.07984 (2023). 33\\n[459] B. Aczel, E.-J. Wagenmakers, Transparency guidance for chatgpt usage\\nin scientific writing, PsyArXiv (2023). 33\\n[460] S. Altmäe, A. Sola-Leyva, A. Salumets, Artificial intelligence in sci-\\nentific writing: a friend or a foe?, Reproductive BioMedicine Online\\n(2023). 33\\n[461] S. Imani, L. Du, H. Shrivastava, Mathprompter: Mathematical reasoning\\nusing large language models, arXiv preprint arXiv:2303.05398 (2023).\\n33\\n[462] Z. Yuan, H. Yuan, C. Li, G. Dong, C. Tan, C. Zhou, Scaling relationship\\non learning mathematical reasoning with large language models, arXiv\\npreprint arXiv:2308.01825 (2023). 33\\n[463] K. Yang, A. M. Swope, A. Gu, R. Chalamala, P. Song, S. Yu, S. Godil,\\nR. Prenger, A. Anandkumar, Leandojo: Theorem proving with retrieval-\\naugmented language models, arXiv preprint arXiv:2306.15626 (2023).\\n33\\n[464] K. M. Collins, A. Q. Jiang, S. Frieder, L. Wong, M. Zilka, U. Bhatt,\\nT. Lukasiewicz, Y . Wu, J. B. Tenenbaum, W. Hart, et al., Evaluating\\nlanguage models for mathematics through interactions, arXiv preprint\\narXiv:2306.01694 (2023). 33\\n[465] Y . Liu, T. Han, S. Ma, J. Zhang, Y . Yang, J. Tian, H. He, A. Li, M. He,\\nZ. Liu, et al., Summary of chatgpt-related research and perspective\\ntowards the future of large language models, Meta-Radiology (2023)\\n100017. 33\\n[466] J. Drápal, H. Westermann, J. Savelka, Using large language models\\nto support thematic analysis in empirical legal studies, arXiv preprint\\narXiv:2310.18729 (2023). 33\\n[467] J. Savelka, K. D. Ashley, M. A. Gray, H. Westermann, H. Xu, Explain-\\ning legal concepts with augmented large language models (gpt-4), arXiv\\npreprint arXiv:2306.09525 (2023). 33\\n[468] N. Guha, J. Nyarko, D. E. Ho, C. Ré, A. Chilton, A. Narayana,\\nA. Chohlas-Wood, A. Peters, B. Waldon, D. N. Rockmore, et al., Legal-\\nbench: A collaboratively built benchmark for measuring legal reasoning\\nin large language models, arXiv preprint arXiv:2308.11462 (2023). 33\\n[469] J. Cui, Z. Li, Y . Yan, B. Chen, L. Yuan, Chatlaw: Open-source legal\\nlarge language model with integrated external knowledge bases, arXiv\\npreprint arXiv:2306.16092 (2023). 33\\n[470] H. Yang, X.-Y . Liu, C. D. Wang, Fingpt: Open-source financial large\\nlanguage models, arXiv preprint arXiv:2306.06031 (2023). 33\\n[471] Y . Li, S. Wang, H. Ding, H. Chen, Large language models in finance: A\\nsurvey, in: Proceedings of the Fourth ACM International Conference on\\nAI in Finance, 2023, pp. 374–382. 33\\n[472] A. Lykov, D. Tsetserukou, Llm-brain: Ai-driven fast generation of\\nrobot behaviour tree based on large language model, arXiv preprint\\narXiv:2305.19352 (2023). 33\\n[473] E. Billing, J. Rosén, M. Lamb, Language models for human-robot inter-\\naction, in: ACM/IEEE International Conference on Human-Robot Inter-\\naction, March 13–16, 2023, Stockholm, Sweden, ACM Digital Library,\\n2023, pp. 905–906. 33\\n[474] Y . Ye, H. You, J. Du, Improved trust in human-robot collaboration with\\nchatgpt, IEEE Access (2023). 33\\n[475] Y . Ding, X. Zhang, C. Paxton, S. Zhang, Leveraging commonsense\\nknowledge from large language models for task and motion planning,\\nin: RSS 2023 Workshop on Learning for Task and Motion Planning,\\n2023. 33\\n[476] J. Wu, R. Antonova, A. Kan, M. Lepert, A. Zeng, S. Song, J. Bohg,\\nS. Rusinkiewicz, T. Funkhouser, Tidybot: Personalized robot assistance\\nwith large language models, arXiv preprint arXiv:2305.05658 (2023).\\n33\\n[477] E. Strubell, A. Ganesh, A. McCallum, Energy and policy considerations\\nfor deep learning in nlp, arXiv preprint arXiv:1906.02243 (2019). 34\\n[478] E. M. Bender, T. Gebru, A. McMillan-Major, S. Shmitchell, On the dan-\\n46'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': '../data/pdfs/llms.pdf', 'total_pages': 47, 'page': 46, 'page_label': '47', 'source_file': 'llms.pdf', 'file_type': 'pdf'}, page_content='gers of stochastic parrots: Can language models be too big?, in: Pro-\\nceedings of the 2021 ACM conference on fairness, accountability, and\\ntransparency, 2021, pp. 610–623. 34\\n[479] C. Zhang, S. Bengio, M. Hardt, B. Recht, O. Vinyals, Understanding\\ndeep learning (still) requires rethinking generalization, Communications\\nof the ACM 64 (3) (2021) 107–115. 34\\n[480] M. Tänzer, S. Ruder, M. Rei, Memorisation versus generalisation in pre-\\ntrained language models, arXiv preprint arXiv:2105.00828 (2021). 34\\n[481] S. M. West, M. Whittaker, K. Crawford, Discriminating systems, AI\\nNow (2019) 1–33. 34\\n[482] K. Valmeekam, A. Olmo, S. Sreedharan, S. Kambhampati, Large lan-\\nguage models still can’t plan (a benchmark for llms on planning and\\nreasoning about change), arXiv preprint arXiv:2206.10498 (2022). 34\\n[483] Y . Zhang, Y . Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao,\\nY . Zhang, Y . Chen, et al., Siren’s song in the ai ocean: A survey on hal-\\nlucination in large language models, arXiv preprint arXiv:2309.01219\\n(2023). 34\\n[484] A. Webson, E. Pavlick, Do prompt-based models really understand the\\nmeaning of their prompts?, arXiv preprint arXiv:2109.01247 (2021). 34\\n[485] O. Shaikh, H. Zhang, W. Held, M. Bernstein, D. Yang, On second\\nthought, let’s not think step by step! bias and toxicity in zero-shot rea-\\nsoning, arXiv preprint arXiv:2212.08061 (2022). 34\\n[486] B. C. Das, M. H. Amini, Y . Wu, Security and privacy challenges of large\\nlanguage models: A survey, arXiv preprint arXiv:2402.00888 (2024). 34\\n[487] X. Liu, H. Cheng, P. He, W. Chen, Y . Wang, H. Poon, J. Gao, Adversar-\\nial training for large neural language models, ArXiv (April 2020).\\nURL https://www.microsoft.com/en-us/research/\\npublication/adversarial-training-for-large-neural-language-models/\\n34\\n[488] E. Shayegani, M. A. A. Mamun, Y . Fu, P. Zaree, Y . Dong, N. Abu-\\nGhazaleh, Survey of vulnerabilities in large language models revealed\\nby adversarial attacks (2023). arXiv:2310.10844. 34\\n[489] X. Xu, K. Kong, N. Liu, L. Cui, D. Wang, J. Zhang, M. Kankanhalli, An\\nllm can fool itself: A prompt-based adversarial attack (2023). arXiv:\\n2310.13345. 34\\n[490] H. Zhao, H. Chen, F. Yang, N. Liu, H. Deng, H. Cai, S. Wang, D. Yin,\\nM. Du, Explainability for large language models: A survey (2023).\\narXiv:2309.01029. 35\\n[491] S. Huang, S. Mamidanna, S. Jangam, Y . Zhou, L. H. Gilpin, Can large\\nlanguage models explain themselves? a study of llm-generated self-\\nexplanations (2023). arXiv:2310.11207. 35\\n[492] H. Brown, K. Lee, F. Mireshghallah, R. Shokri, F. Tramèr, What does it\\nmean for a language model to preserve privacy?, in: Proceedings of the\\n2022 ACM Conference on Fairness, Accountability, and Transparency,\\n2022, pp. 2280–2292. 35\\n[493] R. Plant, V . Giu ffrida, D. Gkatzia, You are what you write: Pre-\\nserving privacy in the era of large language models, arXiv preprint\\narXiv:2204.09391 (2022). 35\\n[494] W. Niu, Z. Kong, G. Yuan, W. Jiang, J. Guan, C. Ding, P. Zhao, S. Liu,\\nB. Ren, Y . Wang, Real-time execution of large-scale language models\\non mobile (2020). arXiv:2009.06823. 35\\n[495] C. Guo, J. Tang, W. Hu, J. Leng, C. Zhang, F. Yang, Y . Liu, M. Guo,\\nY . Zhu, Olive: Accelerating large language models via hardware-\\nfriendly outlier-victim pair quantization, in: Proceedings of the 50th\\nAnnual International Symposium on Computer Architecture, 2023, pp.\\n1–15. 35\\n[496] B. Meskó, E. J. Topol, The imperative for regulatory oversight of large\\nlanguage models (or generative ai) in healthcare, npj Digital Medicine\\n6 (1) (2023) 120. 35\\n[497] J. Zhang, X. Ji, Z. Zhao, X. Hei, K.-K. R. Choo, Ethical considerations\\nand policy implications for large language models: Guiding responsible\\ndevelopment and deployment, arXiv preprint arXiv:2308.02678 (2023).\\n35\\n[498] J. Mökander, J. Schuett, H. R. Kirk, L. Floridi, Auditing large language\\nmodels: a three-layered approach, AI and Ethics (2023) 1–31. 35\\n47'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdfs/agentic_rag.pdf', 'total_pages': 39, 'page': 0, 'page_label': '1', 'source_file': 'agentic_rag.pdf', 'file_type': 'pdf'}, page_content='AGENTIC RETRIEVAL -AUGMENTED GENERATION : A S URVEY ON\\nAGENTIC RAG\\nAditi Singh\\nDepartment of Computer Science\\nCleveland State University\\nCleveland, OH, USA\\na.singh22@csuohio.edu\\nAbul Ehtesham\\nThe Davey Tree Expert Company\\nKent, OH, USA\\nabul.ehtesham@davey.com\\nSaket Kumar\\nThe MathWorks Inc\\nNatick, MA, USA\\nsaketk@mathworks.com\\nTala Talaei Khoei\\nKhoury College of Computer Science\\nRoux Institute at Northeastern University\\nPortland, ME, USA\\nt.talaeikhoei@northeastern.edu\\nABSTRACT\\nLarge Language Models (LLMs) have revolutionized artificial intelligence (AI) by enabling human-\\nlike text generation and natural language understanding. However, their reliance on static training\\ndata limits their ability to respond to dynamic, real-time queries, resulting in outdated or inaccurate\\noutputs. Retrieval-Augmented Generation (RAG) has emerged as a solution, enhancing LLMs by\\nintegrating real-time data retrieval to provide contextually relevant and up-to-date responses. Despite\\nits promise, traditional RAG systems are constrained by static workflows and lack the adaptability\\nrequired for multi-step reasoning and complex task management.\\nAgentic Retrieval-Augmented Generation (Agentic RAG) transcends these limitations by embedding\\nautonomous AI agents into the RAG pipeline. These agents leverage agentic design patterns reflec-\\ntion, planning, tool use, and multi-agent collaboration to dynamically manage retrieval strategies,\\niteratively refine contextual understanding, and adapt workflows through clearly defined operational\\nstructures ranging from sequential steps to adaptive collaboration. This integration enables Agentic\\nRAG systems to deliver unparalleled flexibility, scalability, and context-awareness across diverse\\napplications.\\nThis survey provides a comprehensive exploration of Agentic RAG, beginning with its foundational\\nprinciples and the evolution of RAG paradigms. It presents a detailed taxonomy of Agentic RAG archi-\\ntectures, highlights key applications in industries such as healthcare, finance, and education, and exam-\\nines practical implementation strategies. Additionally, it addresses challenges in scaling these systems,\\nensuring ethical decision-making, and optimizing performance for real-world applications, while\\nproviding detailed insights into frameworks and tools for implementing Agentic RAG 1. The GitHub\\nlink for this survey is available at: https://github.com/asinghcsu/AgenticRAG-Survey.\\nKeywords Large Language Models (LLMs) · Artificial Intelligence (AI) · Natural Language Understanding ·\\nRetrieval-Augmented Generation (RAG) · Agentic RAG · Autonomous AI Agents · Reflection · Planning · Tool\\nUse · Multi-Agent Collaboration · Agentic Patterns · Contextual Understanding · Dynamic Adaptability · Scalability ·\\nReal-Time Data Retrieval · Taxonomy of Agentic RAG · Healthcare Applications · Finance Applications · Educational\\nApplications · Ethical AI Decision-Making · Performance Optimization · Multi-Step Reasoning\\n1GitHub link: https://github.com/asinghcsu/AgenticRAG-Survey\\narXiv:2501.09136v3  [cs.AI]  4 Feb 2025'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdfs/agentic_rag.pdf', 'total_pages': 39, 'page': 1, 'page_label': '2', 'source_file': 'agentic_rag.pdf', 'file_type': 'pdf'}, page_content='1 Introduction\\nLarge Language Models (LLMs) [1, 2] [3], such as OpenAI’s GPT-4, Google’s PaLM, and Meta’s LLaMA, have signifi-\\ncantly transformed artificial intelligence (AI) with their ability to generate human-like text and perform complex natural\\nlanguage processing tasks. These models have driven innovation across diverse domains, including conversational\\nagents [4], automated content creation, and real-time translation. Recent advancements have extended their capabilities\\nto multimodal tasks, such as text-to-image and text-to-video generation [5], enabling the creation and editing of videos\\nand images from detailed prompts [6], which broadens the potential applications of generative AI.\\nDespite these advancements, LLMs face significant limitations due to their reliance on static pre-training data. This\\nreliance often results in outdated information, hallucinated responses [7], and an inability to adapt to dynamic, real-world\\nscenarios. These challenges emphasize the need for systems that can integrate real-time data and dynamically refine\\nresponses to maintain contextual relevance and accuracy.\\nRetrieval-Augmented Generation (RAG) [8, 9] emerged as a promising solution to these challenges. By combining\\nthe generative capabilities of LLMs with external retrieval mechanisms [10], RAG systems enhance the relevance and\\ntimeliness of responses. These systems retrieve real-time information from sources such as knowledge bases [11], APIs,\\nor the web, effectively bridging the gap between static training data and the demands of dynamic applications. However,\\ntraditional RAG workflows remain limited by their linear and static design, which restricts their ability to perform\\ncomplex multi-step reasoning, integrate deep contextual understanding, and iteratively refine responses.\\nThe evolution of agents [12] has significantly enhanced the capabilities of AI systems. Modern agents, including\\nLLM-powered and mobile agents [13], are intelligent entities capable of perceiving, reasoning, and autonomously\\nexecuting tasks. These agents leverage agentic patterns, such as reflection [14], planning [15], tool use, and multi-agent\\ncollaboration [16], to enhance decision-making and adaptability.\\nFurthermore, these agents employ agentic workflow patterns [12, 13], such as prompt chaining, routing, parallelization,\\norchestrator-worker models, and evaluator-optimizer , to structure and optimize task execution. By integrating these\\npatterns, Agentic RAG systems can efficiently manage dynamic workflows and address complex problem-solving\\nscenarios. The convergence of RAG and agentic intelligence has given rise to Agentic Retrieval-Augmented Generation\\n(Agentic RAG) [14], a paradigm that integrates agents into the RAG pipeline. Agentic RAG enables dynamic retrieval\\nstrategies, contextual understanding, and iterative refinement [ 15], allowing for adaptive and efficient information\\nprocessing. Unlike traditional RAG, Agentic RAG employs autonomous agents to orchestrate retrieval, filter relevant\\ninformation, and refine responses, excelling in scenarios requiring precision and adaptability. The overview of Agentic\\nRAG is in figure 1.\\nThis survey explores the foundational principles, taxonomy, and applications of Agentic RAG. It provides a comprehen-\\nsive overview of RAG paradigms, such as Naïve RAG, Modular RAG, and Graph RAG [16], alongside their evolution\\ninto Agentic RAG systems. Key contributions include a detailed taxonomy of Agentic RAG frameworks, applications\\nacross domains such as healthcare [17, 18], finance, and education [19], and insights into implementation strategies,\\nbenchmarks, and ethical considerations.\\nThe structure of this paper is as follows: Section 2 introduces RAG and its evolution, highlighting the limitations of\\ntraditional approaches. Section 3 elaborates on the principles of agentic intelligence and agentic patterns. Section 4\\nelaborates agentic workflow patterns. Section 5 provides a taxonomy of Agentic RAG systems, including single-agent,\\nmulti-agent, and graph-based frameworks. Section 6 provides comparative analysis of Agentic RAG frameworks.\\nSection 7 examines applications of Agentic RAG, while Section 8 discusses implementation tools and frameworks.\\nSection 9 focuses on benchmarks and dataset, and Section 10 concludes with future directions for Agentic RAG systems.\\n2 Foundations of Retrieval-Augmented Generation\\n2.1 Overview of Retrieval-Augmented Generation (RAG)\\nRetrieval-Augmented Generation (RAG) represents a significant advancement in the field of artificial intelligence,\\ncombining the generative capabilities of Large Language Models (LLMs) with real-time data retrieval. While LLMs\\nhave demonstrated remarkable capabilities in natural language processing, their reliance on static pre-trained data\\noften results in outdated or incomplete responses. RAG addresses this limitation by dynamically retrieving relevant\\ninformation from external sources and incorporating it into the generative process, enabling contextually accurate and\\nup-to-date outputs.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdfs/agentic_rag.pdf', 'total_pages': 39, 'page': 2, 'page_label': '3', 'source_file': 'agentic_rag.pdf', 'file_type': 'pdf'}, page_content='Figure 1: An Overview of Agentic RAG\\n2.2 Core Components of RAG\\nThe architecture of RAG systems integrates three primary components (Figure2):\\n• Retrieveal: Responsible for querying external data sources such as knowledge bases, APIs, or vector databases.\\nAdvanced retrievers leverage dense vector search and transformer-based models to improve retrieval precision\\nand semantic relevance.\\n• Augmentation: Processes retrieved data, extracting and summarizing the most relevant information to align\\nwith the query context.\\n• Generation: Combines retrieved information with the LLM’s pre-trained knowledge to generate coherent,\\ncontextually appropriate responses.\\n2.3 Evolution of RAG Paradigms\\nThe field of Retrieval-Augmented Generation (RAG) has evolved significantly to address the increasing complexity of\\nreal-world applications, where contextual accuracy, scalability, and multi-step reasoning are critical. What began as\\nsimple keyword-based retrieval has transitioned into sophisticated, modular, and adaptive systems capable of integrating\\ndiverse data sources and autonomous decision-making processes. This evolution underscores the growing need for\\nRAG systems to handle complex queries efficiently and effectively.\\nThis section examines the progression of RAG paradigms, presenting key stages of development—Naïve RAG,\\nAdvanced RAG, Modular RAG, Graph RAG, and Agentic RAG alongside their defining characteristics, strengths, and\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdfs/agentic_rag.pdf', 'total_pages': 39, 'page': 3, 'page_label': '4', 'source_file': 'agentic_rag.pdf', 'file_type': 'pdf'}, page_content='Figure 2: Core Components of RAG\\nlimitations. By understanding the evolution of these paradigms, readers can appreciate the advancements made in\\nretrieval and generative capabilities and their application in various domains\\n2.3.1 Naïve RAG\\nNaïve RAG [20] represents the foundational implementation of retrieval-augmented generation. Figure 3 illustrates the\\nsimple retrieve-read workflow of Naive RAG, focusing on keyword-based retrieval and static datasets.. These systems\\nrely on simple keyword-based retrieval techniques, such as TF-IDF and BM25, to fetch documents from static datasets.\\nThe retrieved documents are then used to augment the language model’s generative capabilities.\\nFigure 3: An Overview of Naive RAG.\\nNaïve RAG is characterized by its simplicity and ease of implementation, making it suitable for tasks involving\\nfact-based queries with minimal contextual complexity. However, it suffers from several limitations:\\n• Lack of Contextual Awareness: Retrieved documents often fail to capture the semantic nuances of the query\\ndue to reliance on lexical matching rather than semantic understanding.\\n• Fragmented Outputs: The absence of advanced preprocessing or contextual integration often leads to\\ndisjointed or overly generic responses.\\n• Scalability Issues: Keyword-based retrieval techniques struggle with large datasets, often failing to identify\\nthe most relevant information.\\nDespite these limitations, Naïve RAG systems provided a critical proof-of-concept for integrating retrieval with\\ngeneration, laying the foundation for more sophisticated paradigms.\\n2.3.2 Advanced RAG\\nAdvanced RAG [20] systems build upon the limitations of Naïve RAG by incorporating semantic understanding and\\nenhanced retrieval techniques. Figure 4 highlights the semantic enhancements in retrieval and the iterative, context-\\naware pipeline of Advanced RAG. These systems leverage dense retrieval models, such as Dense Passage Retrieval\\n(DPR), and neural ranking algorithms to improve retrieval precision.\\nKey features of Advanced RAG include:\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdfs/agentic_rag.pdf', 'total_pages': 39, 'page': 4, 'page_label': '5', 'source_file': 'agentic_rag.pdf', 'file_type': 'pdf'}, page_content='Figure 4: Overview of Advanced RAG\\n• Dense Vector Search: Queries and documents are represented in high-dimensional vector spaces, enabling\\nbetter semantic alignment between the user query and retrieved documents.\\n• Contextual Re-Ranking: Neural models re-rank retrieved documents to prioritize the most contextually\\nrelevant information.\\n• Iterative Retrieval: Advanced RAG introduces multi-hop retrieval mechanisms, enabling reasoning across\\nmultiple documents for complex queries.\\nThese advancements make Advanced RAG suitable for applications requiring high precision and nuanced understanding,\\nsuch as research synthesis and personalized recommendations. However, challenges such as computational overhead\\nand limited scalability persist, particularly when dealing with large datasets or multi-step queries.\\n2.3.3 Modular RAG\\nModular RAG [ 20] represents the latest evolution in RAG paradigms, emphasizing flexibility and customization.\\nThese systems decompose the retrieval and generation pipeline into independent, reusable components, enabling\\ndomain-specific optimization and task adaptability. Figure 5 demonstrates the modular architecture, showcasing hybrid\\nretrieval strategies, composable pipelines, and external tool integration.\\nKey innovations in Modular RAG include:\\n• Hybrid Retrieval Strategies: Combining sparse retrieval methods (e.g., a sparse encoder-BM25) with dense\\nretrieval techniques [21] (e.g., DPR - Dense Passage Retrieval ) to maximize accuracy across diverse query\\ntypes.\\n• Tool Integration: Incorporating external APIs, databases, or computational tools to handle specialized tasks,\\nsuch as real-time data analysis or domain-specific computations.\\n• Composable Pipelines: Modular RAG enables retrievers, generators, and other components to be replaced,\\nenhanced, or reconfigured independently, allowing high adaptability to specific use cases.\\nFor instance, a Modular RAG system designed for financial analytics might retrieve live stock prices via APIs, analyze\\nhistorical trends using dense retrieval, and generate actionable investment insights through a tailored language model.\\nThis modularity and customization make Modular RAG ideal for complex, multi-domain tasks, offering both scalability\\nand precision.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdfs/agentic_rag.pdf', 'total_pages': 39, 'page': 5, 'page_label': '6', 'source_file': 'agentic_rag.pdf', 'file_type': 'pdf'}, page_content='Figure 5: Overview of Modular RAG\\n2.3.4 Graph RAG\\nGraph RAG [16] extends traditional Retrieval-Augmented Generation systems by integrating graph-based data structures\\nas illustrated in Figure 6. These systems leverage the relationships and hierarchies within graph data to enhance multi-\\nhop reasoning and contextual enrichment. By incorporating graph-based retrieval, Graph RAG enables richer and more\\naccurate generative outputs, particularly for tasks requiring relational understanding.\\nGraph RAG is characterized by its ability to:\\n• Node Connectivity: Captures and reasons over relationships between entities.\\n• Hierarchical Knowledge Management : Handles structured and unstructured data through graph-based\\nhierarchies.\\n• Context Enrichment: Adds relational understanding by leveraging graph-based pathways.\\nHowever, Graph RAG has some limitations:\\n• Limited Scalability: The reliance on graph structures can restrict scalability, especially with extensive data\\nsources.\\n• Data Dependency: High-quality graph data is essential for meaningful outputs, limiting its applicability in\\nunstructured or poorly annotated datasets.\\n• Complexity of Integration: Integrating graph data with unstructured retrieval systems increases design and\\nimplementation complexity.\\nGraph RAG is well-suited for applications such as healthcare diagnostics, legal research, and other domains where\\nreasoning over structured relationships is crucial.\\n2.3.5 Agentic RAG\\nAgentic RAG represents a paradigm shift by introducing autonomous agents capable of dynamic decision-making\\nand workflow optimization. Unlike static systems, Agentic RAG employs iterative refinement and adaptive retrieval\\nstrategies to address complex, real-time, and multi-domain queries. This paradigm leverages the modularity of retrieval\\nand generation processes while introducing agent-based autonomy.\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdfs/agentic_rag.pdf', 'total_pages': 39, 'page': 6, 'page_label': '7', 'source_file': 'agentic_rag.pdf', 'file_type': 'pdf'}, page_content='Figure 6: Overview of Graph RAG\\nKey characteristics of Agentic RAG include:\\n• Autonomous Decision-Making: Agents independently evaluate and manage retrieval strategies based on\\nquery complexity.\\n• Iterative Refinement: Incorporates feedback loops to improve retrieval accuracy and response relevance.\\n• Workflow Optimization: Dynamically orchestrates tasks, enabling efficiency in real-time applications.\\nDespite its advancements, Agentic RAG faces some challenges:\\n• Coordination Complexity : Managing interactions between agents requires sophisticated orchestration\\nmechanisms.\\n• Computational Overhead: The use of multiple agents increases resource requirements for complex work-\\nflows.\\n• Scalability Limitations: While scalable, the dynamic nature of the system can strain computational resources\\nfor high query volumes.\\nAgentic RAG excels in domains like customer support, financial analytics, and adaptive learning platforms, where\\ndynamic adaptability and contextual precision are paramount.\\n2.4 Challenges and Limitations of Traditional RAG Systems\\nTraditional Retrieval-Augmented Generation (RAG) systems have significantly expanded the capabilities of Large\\nLanguage Models (LLMs) by integrating real-time data retrieval. However, these systems still face critical challenges\\nthat hinder their effectiveness in complex, real-world applications. The most notable limitations revolve around\\ncontextual integration, multi-step reasoning, and scalability and latency issues.\\n2.4.1 Contextual Integration\\nEven when RAG systems successfully retrieve relevant information, they often struggle to seamlessly incorporate it\\ninto generated responses. The static nature of retrieval pipelines and limited contextual awareness lead to fragmented,\\ninconsistent, or overly generic outputs.\\nExample: A query such as, \"What are the latest advancements in Alzheimer’s research and their implications for\\nearly-stage treatment?\" might yield relevant research papers and medical guidelines. However, traditional RAG\\nsystems often fail to synthesize these findings into a coherent explanation that connects the new treatments to specific\\npatient scenarios. Similarly, for a query like, \"What are the best sustainable practices for small-scale agriculture in\\narid regions?\", traditional systems might retrieve documents on general agricultural methods but overlook critical\\nsustainability practices tailored to arid environments.\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdfs/agentic_rag.pdf', 'total_pages': 39, 'page': 7, 'page_label': '8', 'source_file': 'agentic_rag.pdf', 'file_type': 'pdf'}, page_content='Table 1: Comparative Analysis of RAG Paradigms\\nParadigm Key Features Strengths\\nNaïve RAG\\n• Keyword-based retrieval (e.g.,\\nTF-IDF, BM25)\\n• Simple and easy to implement\\n• Suitable for fact-based queries\\nAdvanced RAG\\n• Dense retrieval models (e.g.,\\nDPR)\\n• Neural ranking and re-ranking\\n• Multi-hop retrieval\\n• High precision retrieval\\n• Improved contextual relevance\\nModular RAG\\n• Hybrid retrieval (sparse and\\ndense)\\n• Tool and API integration\\n• Composable, domain-specific\\npipelines\\n• High flexibility and customization\\n• Suitable for diverse applications\\n• Scalable\\nGraph RAG\\n• Integration of graph-based\\nstructures\\n• Multi-hop reasoning\\n• Contextual enrichment via\\nnodes\\n• Relational reasoning capabilities\\n• Mitigates hallucinations\\n• Ideal for structured data tasks\\nAgentic RAG\\n• Autonomous agents\\n• Dynamic decision-making\\n• Iterative refinement and work-\\nflow optimization\\n• Adaptable to real-time changes\\n• Scalable for multi-domain tasks\\n• High accuracy\\n2.4.2 Multi-Step Reasoning\\nMany real-world queries require iterative or multi-hop reasoning—retrieving and synthesizing information across\\nmultiple steps. Traditional RAG systems are often ill-equipped to refine retrieval based on intermediate insights or user\\nfeedback, resulting in incomplete or disjointed responses.\\nExample: A complex query like, \"What lessons from renewable energy policies in Europe can be applied to developing\\nnations, and what are the potential economic impacts?\" demands the orchestration of multiple types of information,\\nincluding policy data, contextualization for developing regions, and economic analysis. Traditional RAG systems\\ntypically fail to connect these disparate elements into a cohesive response.\\n2.4.3 Scalability and Latency Issues\\nAs the volume of external data sources grows, querying and ranking large datasets becomes increasingly computationally\\nintensive. This results in significant latency, which undermines the system’s ability to provide timely responses in\\nreal-time applications.\\nExample: In time-sensitive settings such as financial analytics or live customer support, delays caused by querying\\nmultiple databases or processing large document sets can hinder the system’s overall utility. For example, a delay in\\nretrieving market trends during high-frequency trading could result in missed opportunities.\\n2.5 Agentic RAG: A Paradigm Shift\\nTraditional RAG systems, with their static workflows and limited adaptability, often struggle to handle dynamic, multi-\\nstep reasoning and complex real-world tasks. These limitations have spurred the integration of agentic intelligence,\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdfs/agentic_rag.pdf', 'total_pages': 39, 'page': 8, 'page_label': '9', 'source_file': 'agentic_rag.pdf', 'file_type': 'pdf'}, page_content='resulting in Agentic RAG. By incorporating autonomous agents capable of dynamic decision-making, iterative reasoning,\\nand adaptive retrieval strategies, Agentic RAG builds on the modularity of earlier paradigms while overcoming their\\ninherent constraints. This evolution enables more complex, multi-domain tasks to be addressed with enhanced precision\\nand contextual understanding, positioning Agentic RAG as a cornerstone for next-generation AI applications. In\\nparticular, Agentic RAG systems reduce latency through optimized workflows and refine outputs iteratively, tackling\\nthe very challenges that have historically hindered traditional RAG’s scalability and effectiveness.\\n3 Core Principles and Background of Agentic Intelligence\\nAgentic Intelligence forms the foundation of Agentic Retrieval-Augmented Generation (RAG) systems, enabling them\\nto transcend the static and reactive nature of traditional RAG. By integrating autonomous agents capable of dynamic\\ndecision-making, iterative reasoning, and collaborative workflows, Agentic RAG systems exhibit enhanced adaptability\\nand precision. This section explores the core principles underpinning agentic intelligence.\\nComponents of an AI Agent. In essence, an AI agent comprises (Figure. 7):\\n• LLM (with defined Role and Task): Serves as the agent’s primary reasoning engine and dialogue interface.\\nIt interprets user queries, generates responses, and maintains coherence.\\n• Memory (Short-Term and Long-Term): Captures context and relevant data across interactions. Short-term\\nmemory [22] tracks immediate conversation state, while long-term memory [22]stores accumulated knowledge\\nand agent experiences.\\n• Planning (Reflection & Self-Critique): Guides the agent’s iterative reasoning process through reflection,\\nquery routing, or self-critique[23], ensuring that complex tasks are broken down effectively [24].\\n• Tools Vector Search, Web Search, APIs, etc.): Expands the agent’s capabilities beyond text generation,\\nenabling access to external resources, real-time data, or specialized computations.\\nFigure 7: An Overview of AI Agents\\nAgentic Patterns [25, 26] provide structured methodologies that guide the behavior of agents in Agentic Retrieval-\\nAugmented Generation (RAG) systems. These patterns enable agents to dynamically adapt, plan, and collaborate,\\nensuring that the system can handle complex, real-world tasks with precision and scalability. Four key patterns underpin\\nagentic workflows:\\n3.1 Reflection\\nReflection is a foundational design pattern in agentic workflows, enabling agents to iteratively evaluate and refine their\\noutputs. By incorporating self-feedback mechanisms, agents can identify and address errors, inconsistencies, and areas\\nfor improvement, enhancing performance across tasks like code generation, text production, and question answering (\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdfs/agentic_rag.pdf', 'total_pages': 39, 'page': 9, 'page_label': '10', 'source_file': 'agentic_rag.pdf', 'file_type': 'pdf'}, page_content='as shown in Figure 8). In practical use, Reflection involves prompting an agent to critique its outputs for correctness,\\nstyle, and efficiency, then incorporating this feedback into subsequent iterations. External tools, such as unit tests or\\nweb searches, can further enhance this process by validating results and highlighting gaps.\\nIn multi-agent systems, Reflection can involve distinct roles, such as one agent generating outputs while another\\ncritiques them, fostering collaborative improvement. For instance, in legal research, agents can iteratively refine\\nresponses by re-evaluating retrieved case law, ensuring accuracy and comprehensiveness. Reflection has demonstrated\\nsignificant performance improvements in studies like Self-Refine [27], Reflexion [28], and CRITIC [23].\\nFigure 8: An Overview of Agentic Self- Reflection\\n3.2 Planning\\nPlanning [24] is a key design pattern in agentic workflows that enables agents to autonomously decompose complex tasks\\ninto smaller, manageable subtasks. This capability is essential for multi-hop reasoning and iterative problem-solving in\\ndynamic and uncertain scenarios as shown in Figure 9a.\\nBy leveraging planning, agents can dynamically determine the sequence of steps needed to accomplish a larger objective.\\nThis adaptability allows agents to handle tasks that cannot be predefined, ensuring flexibility in decision-making.\\nWhile powerful, Planning can produce less predictable outcomes compared to deterministic workflows like Reflection.\\nPlanning is particularly suited for tasks that require dynamic adaptation, where predefined workflows are insufficient.\\nAs the technology matures, its potential to drive innovative applications across domains will continue to grow.\\n3.3 Tool Use\\nTool Use enables agents to extend their capabilities by interacting with external tools, APIs, or computational resources\\nas illustrated in 9b. This pattern allows agents to gather information, perform computations, and manipulate data beyond\\ntheir pre-trained knowledge. By dynamically integrating tools into workflows, agents can adapt to complex tasks and\\nprovide more accurate and contextually relevant outputs.\\nModern agentic workflows incorporate tool use for a variety of applications, including information retrieval, computa-\\ntional reasoning, and interfacing with external systems. The implementation of this pattern has evolved significantly\\nwith advancements like GPT-4’s function calling capabilities and systems capable of managing access to numerous\\ntools. These developments facilitate sophisticated workflows where agents autonomously select and execute the most\\nrelevant tools for a given task.\\nWhile tool use significantly enhances agentic workflows, challenges remain in optimizing the selection of tools,\\nparticularly in contexts with a large number of available options. Techniques inspired by retrieval-augmented generation\\n(RAG), such as heuristic-based selection, have been proposed to address this issue.\\n3.4 Multi-Agent\\nMulti-agent collaboration [29] is a key design pattern in agentic workflows that enables task specialization and parallel\\nprocessing. Agents communicate and share intermediate results, ensuring the overall workflow remains efficient and\\ncoherent. By distributing subtasks among specialized agents, this pattern improves the scalability and adaptability\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdfs/agentic_rag.pdf', 'total_pages': 39, 'page': 10, 'page_label': '11', 'source_file': 'agentic_rag.pdf', 'file_type': 'pdf'}, page_content='(a) An Overview of Agentic Planning\\n (b) An Overview of Tool Use\\nFigure 9: Overview of Agentic Planning and Tool Use\\nof complex workflows. Multi-agent systems allow developers to decompose intricate tasks into smaller, manageable\\nsubtasks assigned to different agents. This approach not only enhances task performance but also provides a robust\\nframework for managing complex interactions. Each agent operates with its own memory and workflow, which can\\ninclude the use of tools, reflection, or planning, enabling dynamic and collaborative problem-solving (see Figure 10).\\nWhile multi-agent collaboration offers significant potential, it is a less predictable design pattern compared to more\\nmature workflows like Reflection and Tool Use. Nevertheless, emerging frameworks such as AutoGen, Crew AI, and\\nLangGraph are providing new avenues for implementing effective multi-agent solutions.\\nFigure 10: An Overview of MultiAgent\\nThese design patterns form the foundation for the success of Agentic RAG systems. By structuring workflows—from\\nsimple, sequential steps to more adaptive, collaborative processes—these patterns enable systems to dynamically\\nadapt their retrieval and generative strategies to the diverse and ever-changing demands of real-world environments.\\nLeveraging these patterns, agents are capable of handling iterative, context-aware tasks that significantly exceed the\\ncapabilities of traditional RAG systems.\\n4 Agentic Workflow Patterns: Adaptive Strategies for Dynamic Collaboration\\nAgentic workflow patterns, [12, 13] structure LLM-based applications to optimize performance, accuracy, and efficiency.\\nDifferent approaches are suitable depending on task complexity and processing requirements.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdfs/agentic_rag.pdf', 'total_pages': 39, 'page': 11, 'page_label': '12', 'source_file': 'agentic_rag.pdf', 'file_type': 'pdf'}, page_content='4.1 Prompt Chaining: Enhancing Accuracy Through Sequential Processing\\nPrompt chaining [12, 13] decomposes a complex task into multiple steps, where each step builds upon the previous\\none. This structured approach improves accuracy by simplifying each subtask before moving forward. However, it may\\nincrease latency due to sequential processing.\\nFigure 11: Illustration of Prompt Chaining Workflow\\nWhen to Use: This workflow is most effective when a task can be broken down into fixed subtasks, each contributing\\nto the final output. It is particularly useful in scenarios where step-by-step reasoning enhances accuracy.\\nExample Applications:\\n• Generating marketing content in one language and then translating it into another while preserving nuances.\\n• Structuring document creation by first generating an outline, verifying its completeness, and then developing\\nthe full text.\\n4.2 Routing:Directing Inputs to Specialized Processes\\nRouting [12, 13] involves classifying an input and directing it to an appropriate specialized prompt or process. This\\nmethod ensures distinct queries or tasks are handled separately, improving efficiency and response quality.\\nFigure 12: Illustration Routing Workflow\\nWhen to Use: Ideal for scenarios where different types of input require distinct handling strategies, ensuring optimized\\nperformance for each category.\\nExample Applications:\\n• Directing customer service queries into categories such as technical support, refund requests, or general\\ninquiries.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdfs/agentic_rag.pdf', 'total_pages': 39, 'page': 12, 'page_label': '13', 'source_file': 'agentic_rag.pdf', 'file_type': 'pdf'}, page_content='• Assigning simple queries to smaller models for cost efficiency, while complex requests go to advanced models.\\n4.3 Parallelization: Speeding Up Processing Through Concurrent Execution\\nParallelization [12, 13] divides a task into independent processes that run simultaneously, reducing latency and\\nimproving throughput. It can be categorized into sectioning (independent subtasks) and voting (multiple outputs for\\naccuracy).\\nFigure 13: Illustration of Parallelization Workflow\\nWhen to Use: Useful when tasks can be executed independently to enhance speed or when multiple outputs improve\\nconfidence.\\nExample Applications:\\n• Sectioning: Splitting tasks like content moderation, where one model screens input while another generates a\\nresponse.\\n• Voting: Using multiple models to cross-check code for vulnerabilities or analyze content moderation decisions.\\n4.4 Orchestrator-Workers: Dynamic Task Delegation\\nThis workflow [12, 13] features a central orchestrator model that dynamically breaks tasks into subtasks, assigns them\\nto specialized worker models, and compiles the results. Unlike parallelization, it adapts to varying input complexity.\\nFigure 14: Illustration of Orchestrator-Workers Workflow\\nWhen to Use: Best suited for tasks requiring dynamic decomposition and real-time adaptation, where subtasks are not\\npredefined.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdfs/agentic_rag.pdf', 'total_pages': 39, 'page': 13, 'page_label': '14', 'source_file': 'agentic_rag.pdf', 'file_type': 'pdf'}, page_content='Example Applications:\\n• Automatically modifying multiple files in a codebase based on the nature of requested changes.\\n• Conducting real-time research by gathering and synthesizing relevant information from multiple sources.\\n4.5 Evaluator-Optimizer: Refining Output Through Iteration\\nThe evaluator-optimizer [12, 13] workflow iteratively improves content by generating an initial output and refining it\\nbased on feedback from an evaluation model.\\nFigure 15: Illustration of Evaluator-Optimizer Workflow\\nWhen to Use: Effective when iterative refinement significantly enhances response quality, especially when clear\\nevaluation criteria exist.\\nExample Applications:\\n• Improving literary translations through multiple evaluation and refinement cycles.\\n• Conducting multi-round research queries where additional iterations refine search results.\\n5 Taxonomy of Agentic RAG Systems\\nAgentic Retrieval-Augmented Generation (RAG) systems can be categorized into distinct architectural frameworks\\nbased on their complexity and design principles. These include single-agent architectures, multi-agent systems, and hi-\\nerarchical agentic architectures. Each framework is tailored to address specific challenges and optimize performance for\\ndiverse applications. This section provides a detailed taxonomy of these architectures, highlighting their characteristics,\\nstrengths, and limitations.\\n5.1 Single-Agent Agentic RAG: Router\\nA Single-Agent Agentic RAG: [30] serves as a centralized decision-making system where a single agent manages the\\nretrieval, routing, and integration of information (as shown in Figure. 16). This architecture simplifies the system by\\nconsolidating these tasks into one unified agent, making it particularly effective for setups with a limited number of\\ntools or data sources.\\nWorkflow\\n1. Query Submission and Evaluation: The process begins when a user submits a query. A coordinating\\nagent (or master retrieval agent) receives the query and analyzes it to determine the most suitable sources of\\ninformation.\\n2. Knowledge Source Selection: Based on the query’s type, the coordinating agent chooses from a variety of\\nretrieval options:\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdfs/agentic_rag.pdf', 'total_pages': 39, 'page': 14, 'page_label': '15', 'source_file': 'agentic_rag.pdf', 'file_type': 'pdf'}, page_content='• Structured Databases: For queries requiring tabular data access, the system may use a Text-to-SQL\\nengine that interacts with databases like PostgreSQL or MySQL.\\n• Semantic Search: When dealing with unstructured information, it retrieves relevant documents (e.g.,\\nPDFs, books, organizational records) using vector-based retrieval.\\n• Web Search: For real-time or broad contextual information, the system leverages a web search tool to\\naccess the latest online data.\\n• Recommendation Systems: For personalized or contextual queries, the system taps into recommendation\\nengines that provide tailored suggestions.\\n3. Data Integration and LLM Synthesis: Once the relevant data is retrieved from the chosen sources, it is\\npassed to a Large Language Model (LLM). The LLM synthesizes the gathered information, integrating insights\\nfrom multiple sources into a coherent and contextually relevant response.\\n4. Output Generation: Finally, the system delivers a comprehensive, user-facing answer that addresses the\\noriginal query. This response is presented in an actionable, concise format and may optionally include\\nreferences or citations to the sources used.\\nFigure 16: An Overview of Single Agentic RAG\\nKey Features and Advantages.\\n• Centralized Simplicity: A single agent handles all retrieval and routing tasks, making the architecture\\nstraightforward to design, implement, and maintain.\\n• Efficiency & Resource Optimization: With fewer agents and simpler coordination, the system demands\\nfewer computational resources and can handle queries more quickly.\\n• Dynamic Routing: The agent evaluates each query in real-time, selecting the most appropriate knowledge\\nsource (e.g., structured DB, semantic search, web search).\\n• Versatility Across Tools:Supports a variety of data sources and external APIs, enabling both structured and\\nunstructured workflows.\\n• Ideal for Simpler Systems: Suited for applications with well-defined tasks or limited integration requirements\\n(e.g., document retrieval, SQL-based workflows).\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdfs/agentic_rag.pdf', 'total_pages': 39, 'page': 15, 'page_label': '16', 'source_file': 'agentic_rag.pdf', 'file_type': 'pdf'}, page_content='Use Case: Customer Support\\nPrompt: Can you tell me the delivery status of my order?\\nSystem Process (Single-Agent Workflow):\\n1. Query Submission and Evaluation:\\n• The user submits the query, which is received by the coordinating agent.\\n• The coordinating agent analyzes the query and determines the most appropriate sources of\\ninformation.\\n2. Knowledge Source Selection:\\n• Retrieves tracking details from the order management database.\\n• Fetches real-time updates from the shipping provider’s API.\\n• Optionally conducts a web search to identify local conditions affecting delivery, such as weather\\nor logistical delays.\\n3. Data Integration and LLM Synthesis:\\n• The relevant data is passed to the LLM, which synthesizes the information into a coherent\\nresponse.\\n4. Output Generation:\\n• The system generates an actionable and concise response, providing live tracking updates and\\npotential alternatives.\\nResponse:\\nIntegrated Response: “Your package is currently in transit and expected to arrive tomorrow evening. The live\\ntracking from UPS indicates it is at the regional distribution center.”\\n5.2 Multi-Agent Agentic RAG Systems:\\nMulti-Agent RAG [30] represents a modular and scalable evolution of single-agent architectures, designed to handle\\ncomplex workflows and diverse query types by leveraging multiple specialized agents (as shown in Figure 17). Instead\\nof relying on a single agent to manage all tasks—reasoning, retrieval, and response generation—this system distributes\\nresponsibilities across multiple agents, each optimized for a specific role or data source.\\nWorkflow\\n1. Query Submission: The process begins with a user query, which is received by a coordinator agent or master\\nretrieval agent. This agent acts as the central orchestrator, delegating the query to specialized retrieval agents\\nbased on the query’s requirements.\\n2. Specialized Retrieval Agents: The query is distributed among multiple retrieval agents, each focusing on a\\nspecific type of data source or task. Examples include:\\n• Agent 1: Handles structured queries, such as interacting with SQL-based databases like PostgreSQL or\\nMySQL.\\n• Agent 2: Manages semantic searches for retrieving unstructured data from sources like PDFs, books, or\\ninternal records.\\n• Agent 3: Focuses on retrieving real-time public information from web searches or APIs.\\n• Agent 4: Specializes in recommendation systems, delivering context-aware suggestions based on user\\nbehavior or profiles.\\n3. Tool Access and Data Retrieval: Each agent routes the query to the appropriate tools or data sources within\\nits domain, such as:\\n• Vector Search: For semantic relevance.\\n• Text-to-SQL: For structured data.\\n• Web Search: For real-time public information.\\n• APIs: For accessing external services or proprietary systems.\\nThe retrieval process is executed in parallel, allowing for efficient processing of diverse query types.\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdfs/agentic_rag.pdf', 'total_pages': 39, 'page': 16, 'page_label': '17', 'source_file': 'agentic_rag.pdf', 'file_type': 'pdf'}, page_content='Figure 17: An Overview of Multi-Agent Agentic RAG Systems\\n4. Data Integration and LLM Synthesis: Once retrieval is complete, the data from all agents is passed to a\\nLarge Language Model (LLM). The LLM synthesizes the retrieved information into a coherent and contextually\\nrelevant response, integrating insights from multiple sources seamlessly.\\n5. Output Generation: The system generates a comprehensive response, which is delivered back to the user in\\nan actionable and concise format.\\nKey Features and Advantages.\\n• Modularity: Each agent operates independently, allowing for seamless addition or removal of agents based on\\nsystem requirements.\\n• Scalability: Parallel processing by multiple agents enables the system to handle high query volumes efficiently.\\n• Task Specialization: Each agent is optimized for a specific type of query or data source, improving accuracy\\nand retrieval relevance.\\n• Efficiency: By distributing tasks across specialized agents, the system minimizes bottlenecks and enhances\\nperformance for complex workflows.\\n• Versatility: Suitable for applications spanning multiple domains, including research, analytics, decision-\\nmaking, and customer support.\\nChallenges\\n• Coordination Complexity: Managing inter-agent communication and task delegation requires sophisticated\\norchestration mechanisms.\\n• Computational Overhead: Parallel processing of multiple agents can increase resource usage.\\n• Data Integration: Synthesizing outputs from diverse sources into a cohesive response is non-trivial and\\nrequires advanced LLM capabilities.\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdfs/agentic_rag.pdf', 'total_pages': 39, 'page': 17, 'page_label': '18', 'source_file': 'agentic_rag.pdf', 'file_type': 'pdf'}, page_content='Use Case: Multi-Domain Research Assistant\\nPrompt: What are the economic and environmental impacts of renewable energy adoption in Europe?\\nSystem Process (Multi-Agent Workflow):\\n• Agent 1: Retrieves statistical data from economic databases using SQL-based queries.\\n• Agent 2: Searches for relevant academic papers using semantic search tools.\\n• Agent 3: Performs a web search for recent news and policy updates on renewable energy.\\n• Agent 4: Consults a recommendation system to suggest related content, such as reports or expert\\ncommentary.\\nResponse:\\nIntegrated Response: “Adopting renewable energy in Europe has led to a 20% reduction in greenhouse gas\\nemissions over the past decade, according to EU policy reports. Economically, renewable energy investments\\nhave generated approximately 1.2 million jobs, with significant growth in solar and wind sectors. Recent\\nacademic studies also highlight potential trade-offs in grid stability and energy storage costs.”\\n5.3 Hierarchical Agentic RAG Systems\\nHierarchical Agentic RAG: [14] systems employ a structured, multi-tiered approach to information retrieval and\\nprocessing, enhancing both efficiency and strategic decision-making as shown in Figure 18. Agents are organized in\\na hierarchy, with higher-level agents overseeing and directing lower-level agents. This structure enables multi-level\\ndecision-making, ensuring that queries are handled by the most appropriate resources.\\nFigure 18: An illustration of Hierarchical Agentic RAG\\nWorkflow\\n1. Query Reception: A user submits a query, received by a top-tier agent responsible for initial assessment and\\ndelegation.\\n2. Strategic Decision-Making: The top-tier agent evaluates the query’s complexity and decides which subor-\\ndinate agents or data sources to prioritize. Certain databases, APIs, or retrieval tools may be deemed more\\nreliable or relevant based on the query’s domain.\\n3. Delegation to Subordinate Agents : The top-tier agent assigns tasks to lower-level agents specialized in\\nparticular retrieval methods (e.g., SQL databases, web search, or proprietary systems). These agents execute\\ntheir assigned tasks independently.\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdfs/agentic_rag.pdf', 'total_pages': 39, 'page': 18, 'page_label': '19', 'source_file': 'agentic_rag.pdf', 'file_type': 'pdf'}, page_content='4. Aggregation and Synthesis: The results from subordinate agents are collected and integrated by the higher-\\nlevel agent, which synthesizes the information into a coherent response.\\n5. Response Delivery: The final, synthesized answer is returned to the user, ensuring that the response is both\\ncomprehensive and contextually relevant.\\nKey Features and Advantages.\\n• Strategic Prioritization: Top-tier agents can prioritize data sources or tasks based on query complexity,\\nreliability, or context.\\n• Scalability: Distributing tasks across multiple agent tiers enables handling of highly complex or multi-faceted\\nqueries.\\n• Enhanced Decision-Making: Higher-level agents apply strategic oversight to improve overall accuracy and\\ncoherence of responses.\\nChallenges\\n• Coordination Complexity: Maintaining robust inter-agent communication across multiple levels can increase\\norchestration overhead.\\n• Resource Allocation: Efficiently distributing tasks among tiers to avoid bottlenecks is non-trivial.\\nUse Case: Financial Analysis System\\nPrompt: What are the best investment options given the current market trends in renewable energy?\\nSystem Process (Hierarchical Agentic Workflow):\\n1. Top-Tier Agent: Assesses the query’s complexity and prioritizes reliable financial databases and\\neconomic indicators over less validated data sources.\\n2. Mid-Level Agent: Retrieves real-time market data (e.g., stock prices, sector performance) from\\nproprietary APIs and structured SQL databases.\\n3. Lower-Level Agent(s): Conducts web searches for recent policy announcements and consults recom-\\nmendation systems that track expert opinions and news analytics.\\n4. Aggregation and Synthesis: The top-tier agent compiles the results, integrating quantitative data with\\npolicy insights.\\nResponse:\\nIntegrated Response: “Based on current market data, renewable energy stocks have shown a 15% growth over\\nthe past quarter, driven by supportive government policies and heightened investor interest. Analysts suggest\\nthat wind and solar sectors, in particular, may experience continued momentum, while emerging technologies\\nlike green hydrogen present moderate risk but potentially high returns.”\\n5.4 Agentic Corrective RAG\\nCorrective RAG :introduces mechanisms to self-correct retrieval results, enhancing document utilization and improving\\nresponse generation quality as demonstrated in Figure 19. By embedding intelligent agents into the workflow, Corrective\\nRAG [31] [32] ensures iterative refinement of context documents and responses, minimizing errors and maximizing\\nrelevance.\\nKey Idea of Corrective RAG: The core principle of Corrective RAG lies in its ability to evaluate retrieved documents\\ndynamically, perform corrective actions, and refine queries to enhance the quality of generated responses. Corrective\\nRAG adjusts its approach as follows:\\n• Document Relevance Evaluation: Retrieved documents are assessed for relevance by the Relevance Evalua-\\ntion Agent. Documents below the relevance threshold trigger corrective steps.\\n• Query Refinement and Augmentation: Queries are refined by the Query Refinement Agent, which leverages\\nsemantic understanding to optimize retrieval for better results.\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdfs/agentic_rag.pdf', 'total_pages': 39, 'page': 19, 'page_label': '20', 'source_file': 'agentic_rag.pdf', 'file_type': 'pdf'}, page_content='Figure 19: Overview of Agentic Corrective RAG\\n• Dynamic Retrieval from External Sources: When context is insufficient, the External Knowledge Retrieval\\nAgent performs web searches or accesses alternative data sources to supplement the retrieved documents.\\n• Response Synthesis: All validated and refined information is passed to the Response Synthesis Agent for final\\nresponse generation.\\nWorkflow: The Corrective RAG system is built on five key agents:\\n1. Context Retrieval Agent: Responsible for retrieving initial context documents from a vector database.\\n2. Relevance Evaluation Agent: Assesses the retrieved documents for relevance and flags any irrelevant or\\nambiguous documents for corrective actions.\\n3. Query Refinement Agent: Rewrites queries to improve retrieval, leveraging semantic understanding to\\noptimize results.\\n4. External Knowledge Retrieval Agent: Performs web searches or accesses alternative data sources when the\\ncontext documents are insufficient.\\n5. Response Synthesis Agent: Synthesizes all validated information into a coherent and accurate response.\\nKey Features and Advantages:\\n• Iterative Correction: Ensures high response accuracy by dynamically identifying and correcting irrelevant or\\nambiguous retrieval results.\\n• Dynamic Adaptability: Incorporates real-time web searches and query refinement for enhanced retrieval\\nprecision.\\n• Agentic Modularity: Each agent performs specialized tasks, ensuring efficient and scalable operation.\\n• Factuality Assurance: By validating all retrieved and generated content, Corrective RAG minimizes the risk\\nof hallucination or misinformation.\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdfs/agentic_rag.pdf', 'total_pages': 39, 'page': 20, 'page_label': '21', 'source_file': 'agentic_rag.pdf', 'file_type': 'pdf'}, page_content='Use Case: Academic Research Assistant\\nPrompt: What are the latest findings in generative AI research?\\nSystem Process (Corrective RAG Workflow):\\n1. Query Submission: A user submits the query to the system.\\n2. Context Retrieval:\\n• The Context Retrieval Agent retrieves initial documents from a database of published papers on\\ngenerative AI.\\n• The retrieved documents are passed to the next step for evaluation.\\n3. Relevance Evaluation:\\n• The Relevance Evaluation Agent assesses the documents for alignment with the query.\\n• Documents are classified into relevant, ambiguous, or irrelevant categories. Irrelevant documents\\nare flagged for corrective actions.\\n4. Corrective Actions (if needed):\\n• The Query Refinement Agent rewrites the query to improve specificity and relevance.\\n• The External Knowledge Retrieval Agent performs web searches to fetch additional papers and\\nreports from external sources.\\n5. Response Synthesis:\\n• The Response Synthesis Agent integrates validated documents into a coherent and comprehensive\\nsummary.\\nResponse:\\nIntegrated Response: “Recent findings in generative AI highlight advancements in diffusion models, reinforce-\\nment learning for text-to-video tasks, and optimization techniques for large-scale model training. For more\\ndetails, refer to studies published in NeurIPS 2024 and AAAI 2025.”\\n5.5 Adaptive Agentic RAG\\nAdaptive Retrieval-Augmented Generation (Adaptive RAG)[33] enhances the flexibility and efficiency of large\\nlanguage models (LLMs) by dynamically adjusting query handling strategies based on the complexity of the incoming\\nquery. Unlike static retrieval workflows, Adaptive RAG [ 34] employs a classifier to assess query complexity and\\ndetermine the most appropriate approach, ranging from single-step retrieval to multi-step reasoning, or even bypassing\\nretrieval altogether for straightforward queries as illustrated in Figure 20.\\nFigure 20: An Overview of Adaptive Agentic RAG\\nKey Idea of Adaptive RAG The core principle of Adaptive RAG lies in its ability to dynamically tailor retrieval\\nstrategies based on the complexity of the query. Adaptive RAG adjusts its approach as follows:\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdfs/agentic_rag.pdf', 'total_pages': 39, 'page': 21, 'page_label': '22', 'source_file': 'agentic_rag.pdf', 'file_type': 'pdf'}, page_content='• Straightforward Queries: For fact-based questions that require no additional retrieval (e.g., \"What is the\\nboiling point of water?\"), the system directly generates an answer using pre-existing knowledge.\\n• Simple Queries: For moderately complex tasks requiring minimal context (e.g., \"What is the status of my\\nlatest electricity bill?\"), the system performs a single-step retrieval to fetch the relevant details.\\n• Complex Queries: For multi-layered queries requiring iterative reasoning (e.g., \"How has the population of\\nCity X changed over the past decade, and what are the contributing factors?\"), the system employs multi-step\\nretrieval, progressively refining intermediate results to provide a comprehensive answer.\\nWorkflow: The Adaptive RAG system is built on three primary components:\\n1. Classifier Role:\\n• A smaller language model analyzes the query to predict its complexity.\\n• The classifier is trained using automatically labeled datasets, derived from past model outcomes and\\nquery patterns.\\n2. Dynamic Strategy Selection:\\n• For straightforward queries, the system avoids unnecessary retrieval, directly leveraging the LLM for\\nresponse generation.\\n• For simple queries, it employs a single-step retrieval process to fetch relevant context.\\n• For complex queries, it activates multi-step retrieval to ensure iterative refinement and enhanced reasoning.\\n3. LLM Integration:\\n• The LLM synthesizes retrieved information into a coherent response.\\n• Iterative interactions between the LLM and the classifier enable refinement for complex queries.\\nKey Features and Advantages\\n• Dynamic Adaptability: Adjusts retrieval strategies based on query complexity, optimizing both computational\\nefficiency and response accuracy.\\n• Resource Efficiency: Minimizes unnecessary overhead for simple queries while ensuring thorough processing\\nfor complex ones.\\n• Enhanced Accuracy: Iterative refinement ensures that complex queries are resolved with high precision.\\n• Flexibility: Can be extended to incorporate additional pathways, such as domain-specific tools or external\\nAPIs.\\n22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdfs/agentic_rag.pdf', 'total_pages': 39, 'page': 22, 'page_label': '23', 'source_file': 'agentic_rag.pdf', 'file_type': 'pdf'}, page_content='Use Case: Customer Support Assistant\\nPrompt: Why is my package delayed, and what alternatives do I have?\\nSystem Process (Adaptive RAG Workflow):\\n1. Query Classification:\\n• The classifier analyzes the query and determines it to be complex, requiring multi-step reasoning.\\n2. Dynamic Strategy Selection:\\n• The system activates a multi-step retrieval process based on the complexity classification.\\n3. Multi-Step Retrieval:\\n• Retrieves tracking details from the order database.\\n• Fetches real-time status updates from the shipping provider API.\\n• Conducts a web search for external factors such as weather conditions or local disruptions.\\n4. Response Synthesis:\\n• The LLM integrates all retrieved information, synthesizing a comprehensive and actionable\\nresponse.\\nResponse:\\nIntegrated Response: “Your package is delayed due to severe weather conditions in your region. It is currently\\nat the local distribution center and will be delivered in 2 days. Alternatively, you may opt for a local pickup\\nfrom the facility.”\\n5.6 Graph-Based Agentic RAG\\n5.6.1 Agent-G: Agentic Framework for Graph RAG\\nAgent-G [8]: introduces a novel agentic architecture that integrates graph knowledge bases with unstructured document\\nretrieval. By combining structured and unstructured data sources, this framework enhances retrieval-augmented\\ngeneration (RAG) systems with improved reasoning and retrieval accuracy. It employs modular retriever banks,\\ndynamic agent interaction, and feedback loops to ensure high-quality outputs as shown in Figure 21.\\nFigure 21: An Overview of Agent-G: Agentic Framework for Graph RAG [8]\\n23'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdfs/agentic_rag.pdf', 'total_pages': 39, 'page': 23, 'page_label': '24', 'source_file': 'agentic_rag.pdf', 'file_type': 'pdf'}, page_content='Key Idea of Agent-G The core principle of Agent-G lies in its ability to dynamically assign retrieval tasks to\\nspecialized agents, leveraging both graph knowledge bases and textual documents. Agent-G adjusts its retrieval strategy\\nas follows:\\n• Graph Knowledge Bases: Structured data is used to extract relationships, hierarchies, and connections (e.g.,\\ndisease-to-symptom mappings in healthcare).\\n• Unstructured Documents: Traditional text retrieval systems provide contextual information to complement\\ngraph data.\\n• Critic Module: Evaluates the relevance and quality of retrieved information, ensuring alignment with the\\nquery.\\n• Feedback Loops: Refines retrieval and synthesis through iterative validation and re-querying.\\nWorkflow: The Agent-G system is built on four primary components:\\n1. Retriever Bank:\\n• A modular set of agents specializes in retrieving graph-based or unstructured data.\\n• Agents dynamically select relevant sources based on the query’s requirements.\\n2. Critic Module:\\n• Validates retrieved data for relevance and quality.\\n• Flags low-confidence results for re-retrieval or refinement.\\n3. Dynamic Agent Interaction:\\n• Task-specific agents collaborate to integrate diverse data types.\\n• Ensures cohesive retrieval and synthesis across graph and text sources.\\n4. LLM Integration:\\n• Synthesizes validated data into a coherent response.\\n• Iterative feedback from the critic ensures alignment with the query’s intent.\\nKey Features and Advantages\\n• Enhanced Reasoning: Combines structured relationships from graphs with contextual information from\\nunstructured documents.\\n• Dynamic Adaptability: Adjusts retrieval strategies dynamically based on query requirements.\\n• Improved Accuracy: Critic module reduces the risk of irrelevant or low-quality data in responses.\\n• Scalable Modularity: Supports the addition of new agents for specialized tasks, enhancing scalability.\\n24'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdfs/agentic_rag.pdf', 'total_pages': 39, 'page': 24, 'page_label': '25', 'source_file': 'agentic_rag.pdf', 'file_type': 'pdf'}, page_content='Use Case: Healthcare Diagnostics\\nPrompt: What are the common symptoms of Type 2 Diabetes, and how are they related to heart disease?\\nSystem Process (Agent-G Workflow):\\n1. Query Reception and Assignment: The system receives the query and identifies the need for both\\ngraph-structured and unstructured data to answer the question comprehensively.\\n2. Graph Retriever:\\n• Extracts relationships between Type 2 Diabetes and heart disease from a medical knowledge\\ngraph.\\n• Identifies shared risk factors such as obesity and high blood pressure by exploring graph hierar-\\nchies and relationships.\\n3. Document Retriever:\\n• Retrieves descriptions of Type 2 Diabetes symptoms (e.g., increased thirst, frequent urination,\\nfatigue) from medical literature.\\n• Adds contextual information to complement the graph-based insights.\\n4. Critic Module:\\n• Evaluates the relevance and quality of the retrieved graph data and document data.\\n• Flags low-confidence results for refinement or re-querying.\\n5. Response Synthesis: The LLM integrates validated data from the Graph Retriever and Document\\nRetriever into a coherent response, ensuring alignment with the query’s intent.\\nResponse:\\nIntegrated Response: “Type 2 Diabetes symptoms include increased thirst, frequent urination, and fatigue.\\nStudies show a 50% correlation between diabetes and heart disease, primarily through shared risk factors such\\nas obesity and high blood pressure.”\\n5.6.2 GeAR: Graph-Enhanced Agent for Retrieval-Augmented Generation\\nGeAR [35]: introduces an agentic framework that enhances traditional Retrieval-Augmented Generation (RAG) systems\\nby incorporating graph-based retrieval mechanisms. By leveraging graph expansion techniques and an agent-based\\narchitecture, GeAR addresses challenges in multi-hop retrieval scenarios, improving the system’s ability to handle\\ncomplex queries as shown in Figure 22.\\nKey Idea of GeAR GeAR advances RAG performance through two primary innovations:\\n• Graph Expansion: Enhances conventional base retrievers (e.g., BM25) by expanding the retrieval process to\\ninclude graph-structured data, enabling the system to capture complex relationships and dependencies between\\nentities.\\n• Agent Framework: Incorporates an agent-based architecture that utilizes graph expansion to manage retrieval\\ntasks more effectively, allowing for dynamic and autonomous decision-making in the retrieval process.\\nWorkflow: The GeAR system operates through the following components:\\n1. Graph Expansion Module:\\n• Integrates graph-based data into the retrieval process, allowing the system to consider relationships\\nbetween entities during retrieval.\\n• Enhances the base retriever’s ability to handle multi-hop queries by expanding the search space to include\\nconnected entities.\\n2. Agent-Based Retrieval:\\n• Employs an agent framework to manage the retrieval process, enabling dynamic selection and combination\\nof retrieval strategies based on the query’s complexity.\\n• Agents can autonomously decide to utilize graph-expanded retrieval paths to improve the relevance and\\naccuracy of retrieved information.\\n25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdfs/agentic_rag.pdf', 'total_pages': 39, 'page': 25, 'page_label': '26', 'source_file': 'agentic_rag.pdf', 'file_type': 'pdf'}, page_content='3. LLM Integration:\\n• Combines the retrieved information, enriched by graph expansion, with the capabilities of a Large\\nLanguage Model (LLM) to generate coherent and contextually relevant responses.\\n• The integration ensures that the generative process is informed by both unstructured documents and\\nstructured graph data.\\nFigure 22: An Overview of GeAR: Graph-Enhanced Agent for Retrieval-Augmented Generation[35]\\nKey Features and Advantages\\n• Enhanced Multi-Hop Retrieval: GeAR’s graph expansion allows the system to handle complex queries that\\nrequire reasoning over multiple interconnected pieces of information.\\n• Agentic Decision-Making: The agent framework enables dynamic and autonomous selection of retrieval\\nstrategies, improving efficiency and relevance.\\n• Improved Accuracy: By incorporating structured graph data, GeAR enhances the precision of retrieved\\ninformation, leading to more accurate and contextually appropriate responses.\\n• Scalability: The modular nature of the agent framework allows for the integration of additional retrieval\\nstrategies and data sources as needed.\\n26'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdfs/agentic_rag.pdf', 'total_pages': 39, 'page': 26, 'page_label': '27', 'source_file': 'agentic_rag.pdf', 'file_type': 'pdf'}, page_content='Use Case: Multi-Hop Question Answering\\nPrompt: Which author influenced the mentor of J.K. Rowling?\\nSystem Process (GeAR Workflow):\\n1. Top-Tier Agent: Evaluates the query’s multi-hop nature and determines that a combination of graph\\nexpansion and document retrieval is necessary to answer the question.\\n2. Graph Expansion Module:\\n• Identifies that J.K. Rowling’s mentor is a key entity in the query.\\n• Traces the literary influences on that mentor by exploring graph-structured data on literary\\nrelationships.\\n3. Agent-Based Retrieval:\\n• An agent autonomously selects the graph-expanded retrieval path to gather relevant information\\nabout the mentor’s influences.\\n• Integrates additional context by querying textual data sources for unstructured details about the\\nmentor and their influences.\\n4. Response Synthesis: Combines insights from the graph and document retrieval processes using the\\nLLM to generate a response that accurately reflects the complex relationships in the query.\\nResponse:\\nIntegrated Response: “J.K. Rowling’s mentor, [Mentor Name], was heavily influenced by [Author Name],\\nknown for their [notable works or genre]. This connection highlights the layered relationships in literary history,\\nwhere influential ideas often pass through multiple generations of authors.”\\n5.7 Agentic Document Workflows in Agentic RAG\\nAgentic Document Workflows (ADW) [36] extend traditional Retrieval-Augmented Generation (RAG) paradigms by\\nenabling end-to-end knowledge work automation. These workflows orchestrate complex document-centric processes,\\nintegrating document parsing, retrieval, reasoning, and structured outputs with intelligent agents (see Figure 23). ADW\\nsystems address limitations of Intelligent Document Processing (IDP) and RAG by maintaining state, coordinating\\nmulti-step workflows, and applying domain-specific logic to documents.\\nWorkflow\\n1. Document Parsing and Information Structuring:\\n• Documents are parsed using enterprise-grade tools (e.g., LlamaParse) to extract relevant data fields such\\nas invoice numbers, dates, vendor information, line items, and payment terms.\\n• Structured data is organized for downstream processing.\\n2. State Maintenance Across Processes:\\n• The system maintains state about document context, ensuring consistency and relevance across multi-step\\nworkflows.\\n• Tracks the progression of the document through various processing stages.\\n3. Knowledge Retrieval:\\n• Relevant references are retrieved from external knowledge bases (e.g., LlamaCloud) or vector indexes.\\n• Retrieves real-time, domain-specific guidelines for enhanced decision-making.\\n4. Agentic Orchestration:\\n• Intelligent agents apply business rules, perform multi-hop reasoning, and generate actionable recommen-\\ndations.\\n• Orchestrates components such as parsers, retrievers, and external APIs for seamless integration.\\n5. Actionable Output Generation:\\n• Outputs are presented in structured formats, tailored to specific use cases.\\n• Recommendations and extracted insights are synthesized into concise and actionable reports.\\n27'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdfs/agentic_rag.pdf', 'total_pages': 39, 'page': 27, 'page_label': '28', 'source_file': 'agentic_rag.pdf', 'file_type': 'pdf'}, page_content='Figure 23: An Overview of Agentic Document Workflows (ADW)\\n[36]\\nUse Case: Invoice Payments Workflow\\nPrompt: Generate a payment recommendation report based on the submitted invoice and associated vendor\\ncontract terms.\\nSystem Process (ADW Workflow):\\n1. Parse the invoice to extract key details such as invoice number, date, vendor information, line items,\\nand payment terms.\\n2. Retrieve the corresponding vendor contract to verify payment terms and identify any applicable\\ndiscounts or compliance requirements.\\n3. Generate a payment recommendation report that includes original amount due, potential early payment\\ndiscounts, budget impact analysis, and strategic payment actions.\\nResponse: Integrated Response: \"Invoice INV-2025-045 for $15,000.00 has been processed. An early payment\\ndiscount of 2% is available if paid by 2025-04-10, reducing the amount due to $14,700.00. A bulk order discount\\nof 5% was applied as the subtotal exceeded $10,000.00. It is recommended to approve early payment to save\\n2% and ensure timely fund allocation for upcoming project phases.\"\\nKey Features and Advantages\\n• State Maintenance: Tracks document context and workflow stage, ensuring consistency across processes.\\n• Multi-Step Orchestration: Handles complex workflows involving multiple components and external tools.\\n• Domain-Specific Intelligence: Applies tailored business rules and guidelines for precise recommendations.\\n• Scalability: Supports large-scale document processing with modular and dynamic agent integration.\\n• Enhanced Productivity: Automates repetitive tasks while augmenting human expertise in decision-making.\\n28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdfs/agentic_rag.pdf', 'total_pages': 39, 'page': 28, 'page_label': '29', 'source_file': 'agentic_rag.pdf', 'file_type': 'pdf'}, page_content='6 Comparative Analysis of Agentic RAG Frameworks\\nTable 2 provides a comprehensive comparative analysis of the three architectural frameworks: Traditional RAG, Agentic\\nRAG, and Agentic Document Workflows (ADW). This analysis highlights their respective strengths, weaknesses, and\\nbest-fit scenarios, offering valuable insights into their applicability across diverse use cases.\\nTable 2: Comparative Analysis: Traditional RAG vs Agentic RAG vs Agentic Document Workflows (ADW)\\nFeature Traditional RAG Agentic RAG Agentic Document\\nWorkflows (ADW)\\nFocus Isolated retrieval and\\ngeneration tasks\\nMulti-agent\\ncollaboration and\\nreasoning\\nDocument-centric\\nend-to-end workflows\\nContext Maintenance Limited Enabled through\\nmemory modules\\nMaintains state across\\nmulti-step workflows\\nDynamic Adaptability Minimal High Tailored to document\\nworkflows\\nWorkflow\\nOrchestration\\nAbsent Orchestrates multi-agent\\ntasks\\nIntegrates multi-step\\ndocument processing\\nUse of External\\nTools/APIs\\nBasic integration (e.g.,\\nretrieval tools)\\nExtends via tools like\\nAPIs and knowledge\\nbases\\nDeeply integrates business\\nrules and domain-specific\\ntools\\nScalability Limited to small\\ndatasets or queries\\nScalable for multi-agent\\nsystems\\nScales for multi-domain\\nenterprise workflows\\nComplex Reasoning Basic (e.g., simple\\nQ&A)\\nMulti-step reasoning\\nwith agents\\nStructured reasoning across\\ndocuments\\nPrimary Applications QA systems, knowledge\\nretrieval\\nMulti-domain\\nknowledge and\\nreasoning\\nContract review, invoice\\nprocessing, claims analysis\\nStrengths Simplicity, quick setup High accuracy,\\ncollaborative reasoning\\nEnd-to-end automation,\\ndomain-specific intelligence\\nChallenges Poor contextual\\nunderstanding\\nCoordination\\ncomplexity\\nResource overhead, domain\\nstandardization\\nThe comparative analysis underscores the evolutionary trajectory from Traditional RAG to Agentic RAG and further to\\nAgentic Document Workflows (ADW). While Traditional RAG offers simplicity and ease of deployment for basic tasks,\\nAgentic RAG introduces enhanced reasoning and scalability through multi-agent collaboration. ADW builds upon these\\nadvancements by providing robust, document-centric workflows that facilitate end-to-end automation and integration\\nwith domain-specific processes. Understanding the strengths and limitations of each framework is crucial for selecting\\nthe most appropriate architecture to meet specific application requirements and operational demands.\\n7 Applications of Agentic RAG\\nAgentic Retrieval-Augmented Generation (RAG) systems have demonstrated transformative potential across a variety\\nof domains. By combining real-time data retrieval, generative capabilities, and autonomous decision-making, these\\nsystems address complex, dynamic, and multi-modal challenges. This section explores the key applications of Agentic\\nRAG, providing detailed insights into how these systems are shaping industries such as customer support, healthcare,\\nfinance, education, legal workflows, and creative industries.\\n7.1 Customer Support and Virtual Assistants\\nAgentic RAG systems are revolutionizing customer support by enabling real-time, context-aware query resolution.\\nTraditional chatbots and virtual assistants often rely on static knowledge bases, leading to generic or outdated responses.\\n29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdfs/agentic_rag.pdf', 'total_pages': 39, 'page': 29, 'page_label': '30', 'source_file': 'agentic_rag.pdf', 'file_type': 'pdf'}, page_content='By contrast, Agentic RAG systems dynamically retrieve the most relevant information, adapt to the user’s context, and\\ngenerate personalized responses.\\nUse Case: Twitch Ad Sales Enhancement [37]\\nFor instance, Twitch leveraged an agentic workflow with RAG on Amazon Bedrock to streamline ad sales. The system\\ndynamically retrieved advertiser data, historical campaign performance, and audience demographics to generate detailed\\nad proposals, significantly boosting operational efficiency.\\nKey Benefits:\\n• Improved Response Quality: Personalized and context-aware replies enhance user engagement.\\n• Operational Efficiency: Reduces the workload on human support agents by automating complex queries.\\n• Real-Time Adaptability: Dynamically integrates evolving data, such as live service outages or pricing\\nupdates.\\n7.2 Healthcare and Personalized Medicine\\nIn healthcare, the integration of patient-specific data with the latest medical research is critical for informed decision-\\nmaking. Agentic RAG systems enable this by retrieving real-time clinical guidelines, medical literature, and patient\\nhistory to assist clinicians in diagnostics and treatment planning.\\nUse Case: Patient Case Summary [38]\\nAgentic RAG systems have been applied in generating patient case summaries. For example, by integrating electronic\\nhealth records (EHR) and up-to-date medical literature, the system generates comprehensive summaries for clinicians\\nto make faster and more informed decisions.\\nKey Benefits:\\n• Personalized Care: Tailors recommendations to individual patient needs.\\n• Time Efficiency: Streamlines the retrieval of relevant research, saving valuable time for healthcare providers.\\n• Accuracy: Ensures recommendations are based on the latest evidence and patient-specific parameters.\\n7.3 Legal and Contract Analysis\\nAgentic RAG systems are redefining how legal workflows are conducted, offering tools for rapid document analysis and\\ndecision-making.\\nUse Case: Contract Review [39]\\nA legal agentic RAG system can analyze contracts, extract critical clauses, and identify potential risks. By combining\\nsemantic search capabilities with legal knowledge graphs, it automates the tedious process of contract review, ensuring\\ncompliance and mitigating risks.\\nKey Benefits:\\n• Risk Identification: Automatically flags clauses that deviate from standard terms.\\n• Efficiency: Reduces the time spent on contract review processes.\\n• Scalability: Handles large volumes of contracts simultaneously.\\n7.4 Finance and Risk Analysis\\nAgentic RAG systems are transforming the finance industry by providing real-time insights for investment decisions,\\nmarket analysis, and risk management. These systems integrate live data streams, historical trends, and predictive\\nmodeling to generate actionable outputs.\\nUse Case: Auto Insurance Claims Processing [40]\\nIn auto insurance, Agentic RAG can automate claim processing. For example, by retrieving policy details and combining\\nthem with accident data, it generates claim recommendations while ensuring compliance with regulatory requirements.\\nKey Benefits:\\n• Real-Time Analytics: Delivers insights based on live market data.\\n30'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdfs/agentic_rag.pdf', 'total_pages': 39, 'page': 30, 'page_label': '31', 'source_file': 'agentic_rag.pdf', 'file_type': 'pdf'}, page_content='• Risk Mitigation: Identifies potential risks using predictive analysis and multi-step reasoning.\\n• Enhanced Decision-Making: Combines historical and live data for comprehensive strategies.\\n7.5 Education and Personalized Learning\\nEducation is another domain where Agentic RAG systems are making significant strides. These systems enable adaptive\\nlearning by generating explanations, study materials, and feedback tailored to the learner’s progress and preferences.\\nUse Case: Research Paper Generation [41]\\nIn higher education, Agentic RAG has been used to assist researchers by synthesizing key findings from multiple\\nsources. For instance, a researcher querying, “What are the latest advancements in quantum computing?” receives a\\nconcise summary enriched with references, enhancing the quality and efficiency of their work.\\nKey Benefits:\\n• Tailored Learning Paths: Adapts content to individual student needs and performance levels.\\n• Engaging Interactions: Provides interactive explanations and personalized feedback.\\n• Scalability: Supports large-scale deployments for diverse educational environments.\\n7.6 Graph-Enhanced Applications in Multimodal Workflows\\nGraph-Enhanced Agentic RAG (GEAR) combines graph structures with retrieval mechanisms, making it particularly\\neffective in multimodal workflows where interconnected data sources are essential.\\nUse Case: Market Survey Generation\\nGEAR enables the synthesis of text, images, and videos for marketing campaigns. For example, querying, “What\\nare the emerging trends in eco-friendly products?” generates a detailed report enriched with customer preferences,\\ncompetitor analysis, and multimedia content.\\nKey Benefits:\\n• Multi-Modal Capabilities: Integrates text, image, and video data for comprehensive outputs.\\n• Enhanced Creativity: Generates innovative ideas and solutions for marketing and entertainment.\\n• Dynamic Adaptability: Adapts to evolving market trends and customer needs.\\nThe applications of Agentic RAG systems span a wide range of industries, showcasing their versatility and transformative\\npotential. From personalized customer support to adaptive education and graph-enhanced multimodal workflows, these\\nsystems address complex, dynamic, and knowledge-intensive challenges. By integrating retrieval, generation, and\\nagentic intelligence, Agentic RAG systems are paving the way for next-generation AI applications.\\n8 Tools and Frameworks for Agentic RAG\\nAgentic Retrieval-Augmented Generation (RAG) systems represent a significant evolution in combining retrieval,\\ngeneration, and agentic intelligence. These systems extend the capabilities of traditional RAG by integrating decision-\\nmaking, query reformulation, and adaptive workflows. The following tools and frameworks provide robust support for\\ndeveloping Agentic RAG systems, addressing the complex requirements of real-world applications.\\nKey Tools and Frameworks:\\n• LangChain and LangGraph: LangChain [42] provides modular components for building RAG pipelines,\\nseamlessly integrating retrievers, generators, and external tools. LangGraph complements this by introducing\\ngraph-based workflows that support loops, state persistence, and human-in-the-loop interactions, enabling\\nsophisticated orchestration and self-correction mechanisms in agentic systems.\\n• LlamaIndex: LlamaIndex’s [43] Agentic Document Workflows (ADW) enable end-to-end automation of\\ndocument processing, retrieval, and structured reasoning. It introduces a meta-agent architecture where\\nsub-agents manage smaller document sets, coordinating through a top-level agent for tasks such as compliance\\nanalysis and contextual understanding.\\n• Hugging Face Transformers and Qdrant: Hugging Face [44] offers pre-trained models for embedding and\\ngeneration tasks, while Qdrant [ 45] enhances retrieval workflows with adaptive vector search capabilities,\\nallowing agents to optimize performance by dynamically switching between sparse and dense vector methods.\\n31'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdfs/agentic_rag.pdf', 'total_pages': 39, 'page': 31, 'page_label': '32', 'source_file': 'agentic_rag.pdf', 'file_type': 'pdf'}, page_content='• CrewAI and AutoGen: These frameworks emphasize multi-agent architectures. CrewAI [ 46] supports\\nhierarchical and sequential processes, robust memory systems, and tool integrations. AG2 [ 47] (formerly\\nknows as AutoGen [48, 49]) excels in multi-agent collaboration with advanced support for code generation,\\ntool execution, and decision-making.\\n• OpenAI Swarm Framework: An educational framework designed for ergonomic, lightweight multi-agent\\norchestration [50], emphasizing agent autonomy and structured collaboration.\\n• Agentic RAG with Vertex AI: Developed by Google, Vertex AI [51] integrates seamlessly with Agentic\\nRetrieval-Augmented Generation (RAG), providing a platform to build, deploy, and scale machine learning\\nmodels while leveraging advanced AI capabilities for robust, contextually aware retrieval and decision-making\\nworkflows.\\n• Semantic Kernel: Semantic Kernel [52, 53] is an open-source SDK by Microsoft that integrates large language\\nmodels (LLMs) into applications. It supports agentic patterns, enabling the creation of autonomous AI agents\\nfor natural language understanding, task automation, and decision-making. It has been used in scenarios like\\nServiceNow’s P1 incident management to facilitate real-time collaboration, automate task execution, and\\nretrieve contextual information seamlessly\\n• Amazon Bedrock for Agentic RAG: Amazon Bedrock [37] provides a robust platform for implementing\\nAgentic Retrieval-Augmented Generation (RAG) workflows.\\n• IBM Watson and Agentic RAG:IBM’s watsonx.ai [54] supports building Agentic RAG systems, exemplified\\nby using the Granite-3-8B-Instruct model to answer complex queries by integrating external information and\\nenhancing response accuracy.\\n• Neo4j and Vector Databases: Neo4j, a prominent open-source graph database, excels in handling complex\\nrelationships and semantic queries. Alongside Neo4j, vector databases like Weaviate, Pinecone, Milvus, and\\nQdrant provide efficient similarity search and retrieval capabilities, forming the backbone of high-performance\\nAgentic Retrieval-Augmented Generation (RAG) workflows.\\n9 Benchmarks and Datasets\\nCurrent benchmarks and datasets provide valuable insights into evaluating Retrieval-Augmented Generation (RAG)\\nsystems, including those with agentic and graph-based enhancements. While some are explicitly designed for RAG,\\nothers are adapted to test retrieval, reasoning, and generation capabilities in diverse scenarios. Datasets are crucial for\\ntesting the retrieval, reasoning, and generation components of RAG systems. Table 3 discusses some key datasets based\\non the dowstream task for RAG Evaluation.\\nBenchmarks play a critical role in standardizing the evaluation of RAG systems by providing structured tasks and\\nmetrics. The following benchmarks are particularly relevant:\\n• BEIR (Benchmarking Information Retrieval): A versatile benchmark designed for evaluating embedding\\nmodels on a variety of information retrieval tasks, encompassing 17 datasets across diverse domains like\\nbioinformatics, finance, and question answering [55].\\n• MS MARCO (Microsoft Machine Reading Comprehension): Focused on passage ranking and question\\nanswering, this benchmark is widely used for dense retrieval tasks in RAG systems [56].\\n• TREC (Text REtrieval Conference, Deep Learning Track):Provides datasets for passage and document\\nretrieval, emphasizing the quality of ranking models in retrieval pipelines [57].\\n• MuSiQue (Multihop Sequential Questioning): A benchmark for multihop reasoning across multiple\\ndocuments, emphasizing the importance of retrieving and synthesizing information from disconnected contexts\\n[58].\\n• 2WikiMultihopQA: A dataset designed for multihop QA tasks over two Wikipedia articles, focusing on the\\nability to connect knowledge across multiple sources [59].\\n• AgentG (Agentic RAG for Knowledge Fusion): Tailored for agentic RAG tasks, this benchmark assesses\\ndynamic information synthesis across multiple knowledge bases [8].\\n• HotpotQA: A multi-hop QA benchmark requiring retrieval and reasoning over interconnected contexts, ideal\\nfor evaluating complex RAG workflows[60].\\n• RAGBench: A large-scale, explainable benchmark featuring 100,000 examples across industry domains, with\\na TRACe evaluation framework for actionable RAG metrics [61].\\n32'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdfs/agentic_rag.pdf', 'total_pages': 39, 'page': 32, 'page_label': '33', 'source_file': 'agentic_rag.pdf', 'file_type': 'pdf'}, page_content='• BERGEN (Benchmarking Retrieval-Augmented Generation): A library for systematically benchmarking\\nRAG systems with standardized experiments [62].\\n• FlashRAG Toolkit: Implements 12 RAG methods and includes 32 benchmark datasets to support efficient\\nand standardized RAG evaluation [63].\\n• GNN-RAG: This benchmark evaluates graph-based RAG systems on tasks like node-level and edge-level\\npredictions, focusing on retrieval quality and reasoning performance in Knowledge Graph Question Answering\\n(KGQA) [64].\\nTable 3: Downstream Tasks and Datasets for RAG Evaluation (Adapted from [20]\\nCategory Task Type Datasets and References\\nQA\\nSingle-hop QA Natural Questions (NQ) [65], TriviaQA [66], SQuAD [67],\\nWeb Questions (WebQ) [68], PopQA [69], MS MARCO\\n[56]\\nMulti-hop QA HotpotQA [60], 2WikiMultiHopQA [59], MuSiQue [58]\\nLong-form QA ELI5 [70], NarrativeQA (NQA) [71], ASQA [72], QM-\\nSum [73]\\nDomain-specific QA Qasper [74], COVID-QA [ 75], CMB/MMCU Medical\\n[76]\\nMulti-choice QA QuALITY [77], ARC (No reference available), Common-\\nsenseQA [78]\\nGraph-based QA Graph QA GraphQA [79]\\nEvent Argument Extraction WikiEvent [80], RAMS [81]\\nDialog\\nOpen-domain Dialog Wizard of Wikipedia (WoW) [82]\\nPersonalized Dialog KBP [83], DuleMon [84]\\nTask-oriented Dialog CamRest [85]\\nRecommendation Personalized Content Amazon Datasets (Toys, Sports, Beauty) [86]\\nReasoning\\nCommonsense Reasoning HellaSwag [87], CommonsenseQA [78]\\nCoT Reasoning CoT Reasoning [88]\\nComplex Reasoning CSQA [89]\\nOthers\\nLanguage Understanding MMLU (No reference available), WikiText-103 [65]\\nFact Checking/Verification FEVER [90], PubHealth [91]\\nStrategy QA StrategyQA [92]\\nSummarization Text Summarization WikiASP [93], XSum [94]\\nLong-form Summarization NarrativeQA (NQA) [71], QMSum [73]\\nText Generation Biography Biography Dataset (No reference available)\\nText Classification Sentiment Analysis SST-2 [95]\\nGeneral Classification VioLens[96], TREC [57]\\nCode Search Programming Search CodeSearchNet [97]\\nRobustness Retrieval Robustness NoMIRACL [98]\\nLanguage Modeling Robustness WikiText-103 [99]\\nMath Math Reasoning GSM8K [100]\\nMachine Translation Translation Tasks JRC-Acquis [101]\\n10 Conclusion\\nAgentic Retrieval-Augmented Generation (RAG) represents a transformative advancement in artificial intelligence,\\naddressing the limitations of traditional RAG systems through the integration of autonomous agents. By leveraging\\n33'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdfs/agentic_rag.pdf', 'total_pages': 39, 'page': 33, 'page_label': '34', 'source_file': 'agentic_rag.pdf', 'file_type': 'pdf'}, page_content='agentic intelligence, these systems introduce capabilities such as dynamic decision-making, iterative reasoning, and\\ncollaborative workflows, enabling them to tackle complex, real-world tasks with enhanced precision and adaptability.\\nThis survey explored the evolution of RAG systems, from their initial implementations to advanced paradigms like\\nModular RAG, highlighting the contributions and limitations of each. The integration of agents into the RAG pipeline\\nhas emerged as a pivotal development, resulting in Agentic RAG systems that overcome static workflows and limited\\ncontextual adaptability. Applications across healthcare, finance, education, and creative industries demonstrate the\\ntransformative potential of these systems, showcasing their ability to deliver personalized, real-time, and context-aware\\nsolutions.\\nDespite their promise, Agentic RAG systems face challenges that require further research and innovation. Coordination\\ncomplexity in multi-agent architectures, scalability, and latency issues, as well as ethical considerations, must be\\naddressed to ensure robust and responsible deployment. Additionally, the lack of specialized benchmarks and datasets\\ntailored to evaluate agentic capabilities poses a significant hurdle. Developing evaluation methodologies that capture\\nthe unique aspects of Agentic RAG, such as multi-agent collaboration and dynamic adaptability, will be crucial for\\nadvancing the field.\\nLooking ahead, the convergence of retrieval-augmented generation and agentic intelligence has the potential to redefine\\nAI’s role in dynamic and complex environments. By addressing these challenges and exploring future directions,\\nresearchers and practitioners can unlock the full potential of Agentic RAG systems, paving the way for transformative\\napplications across industries and domains. As AI systems continue to evolve, Agentic RAG stands as a cornerstone for\\ncreating adaptive, context-aware, and impactful solutions that meet the demands of a rapidly changing world.\\nReferences\\n[1] Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, and\\nJianfeng Gao. Large language models: A survey, 2024.\\n[2] Aditi Singh. Exploring language models: A comprehensive survey and analysis. In 2023 International Con-\\nference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication\\nEngineering (RMKMATE), pages 1–4, 2023.\\n[3] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang,\\nJunjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren,\\nYifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of large language\\nmodels, 2024.\\n[4] Sumit Kumar Dam, Choong Seon Hong, Yu Qiao, and Chaoning Zhang. A complete survey on llm-based ai\\nchatbots, 2024.\\n[5] Aditi Singh. A survey of ai text-to-image and ai text-to-video generators. In 2023 4th International Conference\\non Artificial Intelligence, Robotics and Control (AIRC), pages 32–36, 2023.\\n[6] Aditi Singh, Abul Ehtesham, Gaurav Kumar Gupta, Nikhil Kumar Chatta, Saket Kumar, and Tala Talaei Khoei.\\nExploring prompt engineering: A systematic review with swot analysis, 2024.\\n[7] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua\\nPeng, Xiaocheng Feng, Bing Qin, and Ting Liu. A survey on hallucination in large language models: Principles,\\ntaxonomy, challenges, and open questions. ACM Transactions on Information Systems, November 2024.\\n[8] Meng-Chieh Lee, Qi Zhu, Costas Mavromatis, Zhen Han, Soji Adeshina, Vassilis N. Ioannidis, Huzefa Rangwala,\\nand Christos Faloutsos. Agent-g: An agentic framework for graph retrieval augmented generation, 2024.\\n[9] Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao\\nZhang, Jie Jiang, and Bin Cui. Retrieval-augmented generation for ai-generated content: A survey, 2024.\\n[10] Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan,\\nand Graham Neubig. Active retrieval augmented generation, 2023.\\n[11] Yikun Han, Chunjiang Liu, and Pengfei Wang. A comprehensive survey on vector database: Storage and retrieval\\ntechnique, challenge, 2023.\\n[12] Anthropic. Building effective agents, 2024. https://www.anthropic.com/research/\\nbuilding-effective-agents. Accessed: February 2, 2025.\\n[13] LangChain. Langgraph workflows tutorial, 2025. https://langchain-ai.github.io/langgraph/\\ntutorials/workflows/. Accessed: February 2, 2025.\\n34'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdfs/agentic_rag.pdf', 'total_pages': 39, 'page': 34, 'page_label': '35', 'source_file': 'agentic_rag.pdf', 'file_type': 'pdf'}, page_content='[14] Chidaksh Ravuru, Sagar Srinivas Sakhinana, and Venkataramana Runkana. Agentic retrieval-augmented\\ngeneration for time series analysis, 2024.\\n[15] Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: A survey, 2023.\\n[16] Boci Peng, Yun Zhu, Yongchao Liu, Xiaohe Bo, Haizhou Shi, Chuntao Hong, Yan Zhang, and Siliang Tang.\\nGraph retrieval-augmented generation: A survey, 2024.\\n[17] Aditi Singh, Abul Ehtesham, Saifuddin Mahmud, and Jong-Hoon Kim. Revolutionizing mental health care\\nthrough langchain: A journey with a large language model. In 2024 IEEE 14th Annual Computing and\\nCommunication Workshop and Conference (CCWC), pages 0073–0078, 2024.\\n[18] Gaurav Kumar Gupta, Aditi Singh, Sijo Valayakkad Manikandan, and Abul Ehtesham. Digital diagnostics: The\\npotential of large language models in recognizing symptoms of common illnesses. AI, 6(1), 2025.\\n[19] Aditi Singh, Abul Ehtesham, Saket Kumar, Gaurav Kumar Gupta, and Tala Talaei Khoei. Encouraging responsible\\nuse of generative ai in education: A reward-based learning approach. In Tim Schlippe, Eric C. K. Cheng, and\\nTianchong Wang, editors, Artificial Intelligence in Education Technologies: New Development and Innovative\\nPractices, pages 404–413, Singapore, 2025. Springer Nature Singapore.\\n[20] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and\\nHaofen Wang. Retrieval-augmented generation for large language models: A survey, 2024.\\n[21] Vladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen\\ntau Yih. Dense passage retrieval for open-domain question answering, 2020.\\n[22] Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and Ji-Rong\\nWen. A survey on the memory mechanism of large language model based agents, 2024.\\n[23] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. Critic: Large\\nlanguage models can self-correct with tool-interactive critiquing, 2024.\\n[24] Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang,\\nand Enhong Chen. Understanding the planning of llm agents: A survey, 2024.\\n[25] Aditi Singh, Abul Ehtesham, Saket Kumar, and Tala Talaei Khoei. Enhancing ai systems with agentic workflows\\npatterns in large language model. In 2024 IEEE World AI IoT Congress (AIIoT), pages 527–532, 2024.\\n[26] DeepLearning.AI. How agents can improve llm performance. https://www.deeplearning.ai/the-batch/\\nhow-agents-can-improve-llm-performance/?ref=dl-staging-website.ghost.io , 2024. Ac-\\ncessed: 2025-01-13.\\n[27] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha\\nDziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann,\\nSean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback, 2023.\\n[28] Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao.\\nReflexion: Language agents with verbal reinforcement learning, 2023.\\n[29] Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V . Chawla, Olaf Wiest, and\\nXiangliang Zhang. Large language model based multi-agents: A survey of progress and challenges, 2024.\\n[30] Weaviate Blog. What is agentic rag? https://weaviate.io/blog/what-is-agentic-rag#:~:text=is%\\n20Agentic%20RAG%3F-,%E2%80%8B,of%20the%20non%2Dagentic%20pipeline. Accessed: 2025-01-14.\\n[31] Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. Corrective retrieval augmented generation, 2024.\\n[32] LangGraph CRAG Tutorial. Langgraph crag: Contextualized retrieval-augmented generation tutorial. https:\\n//langchain-ai.github.io/langgraph/tutorials/rag/langgraph_crag/. Accessed: 2025-01-14.\\n[33] Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong C. Park. Adaptive-rag: Learning to adapt\\nretrieval-augmented large language models through question complexity, 2024.\\n[34] LangGraph Adaptive RAG Tutorial. Langgraph adaptive rag: Adaptive retrieval-augmented generation tu-\\ntorial. https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/.\\nAccessed: 2025-01-14.\\n[35] Zhili Shen, Chenxin Diao, Pavlos V ougiouklis, Pascual Merita, Shriram Piramanayagam, Damien Graux, Dandan\\nTu, Zeren Jiang, Ruofei Lai, Yang Ren, and Jeff Z. Pan. Gear: Graph-enhanced agent for retrieval-augmented\\ngeneration, 2024.\\n[36] LlamaIndex. Introducing agentic document workflows. https://www.llamaindex.ai/blog/\\nintroducing-agentic-document-workflows , 2025. Accessed: 2025-01-13.\\n35'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdfs/agentic_rag.pdf', 'total_pages': 39, 'page': 35, 'page_label': '36', 'source_file': 'agentic_rag.pdf', 'file_type': 'pdf'}, page_content='[37] AWS Machine Learning Blog. How twitch used agentic workflow with rag on amazon\\nbedrock to supercharge ad sales. https://aws.amazon.com/blogs/machine-learning/\\nhow-twitch-used-agentic-workflow-with-rag-on-amazon-bedrock-to-supercharge-ad-sales/ ,\\n2025. Accessed: 2025-01-13.\\n[38] LlamaCloud Demo Repository. Patient case summary workflow using llamacloud. https:\\n//github.com/run-llama/llamacloud-demo/blob/main/examples/document_workflows/\\npatient_case_summary/patient_case_summary.ipynb, 2025. Accessed: 2025-01-13.\\n[39] LlamaCloud Demo Repository. Contract review workflow using llamacloud. https://github.com/\\nrun-llama/llamacloud-demo/blob/main/examples/document_workflows/contract_review/\\ncontract_review.ipynb, 2025. Accessed: 2025-01-13.\\n[40] LlamaCloud Demo Repository. Auto insurance claims workflow using llamacloud. https:\\n//github.com/run-llama/llamacloud-demo/blob/main/examples/document_workflows/auto_\\ninsurance_claims/auto_insurance_claims.ipynb, 2025. Accessed: 2025-01-13.\\n[41] LlamaCloud Demo Repository. Research paper report generation workflow using llamacloud.\\nhttps://github.com/run-llama/llamacloud-demo/blob/main/examples/report_generation/\\nresearch_paper_report_generation.ipynb, 2025. Accessed: 2025-01-13.\\n[42] LangGraph Agentic RAG Tutorial. Langgraph agentic rag: Nodes and edges tutorial. https://langchain-ai.\\ngithub.io/langgraph/tutorials/rag/langgraph_agentic_rag/#nodes-and-edges. Accessed:\\n2025-01-14.\\n[43] LlamaIndex Blog. Agentic rag with llamaindex. https://www.llamaindex.ai/blog/\\nagentic-rag-with-llamaindex-2721b8a49ff6 . Accessed: 2025-01-14.\\n[44] Hugging Face Cookbook. Agentic rag: Turbocharge your retrieval-augmented generation with query reformula-\\ntion and self-query. https://huggingface.co/learn/cookbook/en/agent_rag. Accessed: 2025-01-14.\\n[45] Qdrant Blog. Agentic rag: Combining rag with agents for enhanced information retrieval. https://qdrant.\\ntech/articles/agentic-rag/. Accessed: 2025-01-14.\\n[46] crewAI Inc. crewai: A github repository for ai projects. https://github.com/crewAIInc/crewAI, 2025.\\nAccessed: 2025-01-15.\\n[47] AG2AI Contributors. Ag2: A github repository for advanced generative ai research. https://github.com/\\nag2ai/ag2, 2025. Accessed: 2025-01-15.\\n[48] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun\\nZhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, and Chi Wang. Autogen: Enabling\\nnext-gen llm applications via multi-agent conversation framework. 2023.\\n[49] Shaokun Zhang, Jieyu Zhang, Jiale Liu, Linxin Song, Chi Wang, Ranjay Krishna, and Qingyun Wu. Training\\nlanguage model agents without modifying language models. ICML’24, 2024.\\n[50] OpenAI. Swarm: Lightweight multi-agent orchestration framework. https://github.com/openai/swarm.\\nAccessed: 2025-01-14.\\n[51] LlamaIndex Documentation. Agentic rag using vertex ai. https://docs.llamaindex.ai/en/stable/\\nexamples/agent/agentic_rag_using_vertex_ai/. Accessed: 2025-01-14.\\n[52] Microsoft. Semantic kernel overview, 2025. https://learn.microsoft.com/en-us/semantic-kernel/\\noverview/. Accessed: February 2, 2025.\\n[53] Microsoft. Semantic kernel github repository, 2025. https://github.com/microsoft/semantic-kernel.\\nAccessed: February 2, 2025.\\n[54] IBM Granite Community. Agentic rag: Ai agents with ibm granite models. https://github.com/\\nibm-granite-community/granite-snack-cookbook/blob/main/recipes/AI-Agents/Agentic_\\nRAG.ipynb. Accessed: 2025-01-14.\\n[55] Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. Beir: A heterogenous\\nbenchmark for zero-shot evaluation of information retrieval models, 2021.\\n[56] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew\\nMcNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong\\nWang. Ms marco: A human generated machine reading comprehension dataset, 2018.\\n[57] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, Jimmy Lin, Ellen M. V oorhees, and Ian Soboroff.\\nOverview of the trec 2022 deep learning track. In Text REtrieval Conference (TREC). NIST, TREC, March 2023.\\n36'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdfs/agentic_rag.pdf', 'total_pages': 39, 'page': 36, 'page_label': '37', 'source_file': 'agentic_rag.pdf', 'file_type': 'pdf'}, page_content='[58] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop questions\\nvia single-hop question composition, 2022.\\n[59] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing a multi-hop qa dataset\\nfor comprehensive evaluation of reasoning steps, 2020.\\n[60] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christo-\\npher D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering, 2018.\\n[61] Robert Friel, Masha Belyi, and Atindriyo Sanyal. Ragbench: Explainable benchmark for retrieval-augmented\\ngeneration systems, 2024.\\n[62] David Rau, Hervé Déjean, Nadezhda Chirkova, Thibault Formal, Shuai Wang, Vassilina Nikoulina, and Stéphane\\nClinchant. Bergen: A benchmarking library for retrieval-augmented generation, 2024.\\n[63] Jiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, and Zhicheng Dou. Flashrag: A modular toolkit for efficient\\nretrieval-augmented generation research, 2024.\\n[64] Costas Mavromatis and George Karypis. Gnn-rag: Graph neural retrieval for large language model reasoning,\\n2024.\\n[65] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle\\nEpstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei\\nChang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark for question\\nanswering research. Transactions of the Association for Computational Linguistics, 7:452–466, 2019.\\n[66] Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised\\nchallenge dataset for reading comprehension, 2017.\\n[67] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine\\ncomprehension of text, 2016.\\n[68] Jonathan Berant, Andrew K. Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from question-\\nanswer pairs. In Conference on Empirical Methods in Natural Language Processing, 2013.\\n[69] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When not to\\ntrust language models: Investigating effectiveness of parametric and non-parametric memories. In Anna Rogers,\\nJordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers), pages 9802–9822, Toronto, Canada, July 2023. Association\\nfor Computational Linguistics.\\n[70] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. Eli5: Long form\\nquestion answering, 2019.\\n[71] Tomáš Koˇciský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward\\nGrefenstette. The narrativeqa reading comprehension challenge. 2017.\\n[72] Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. Asqa: Factoid questions meet long-form\\nanswers, 2023.\\n[73] Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli\\nCelikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir Radev. QMSum: A new benchmark for query-based\\nmulti-domain meeting summarization. pages 5905–5921, June 2021.\\n[74] Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. A dataset of information-\\nseeking questions and answers anchored in research papers. In Kristina Toutanova, Anna Rumshisky, Luke\\nZettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao\\nZhou, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for\\nComputational Linguistics: Human Language Technologies, pages 4599–4610, Online, June 2021. Association\\nfor Computational Linguistics.\\n[75] Timo Möller, Anthony Reina, Raghavan Jayakumar, and Malte Pietsch. COVID-QA: A question answering\\ndataset for COVID-19. In ACL 2020 Workshop on Natural Language Processing for COVID-19 (NLP-COVID),\\n2020.\\n[76] Xidong Wang, Guiming Hardy Chen, Dingjie Song, Zhiyi Zhang, Zhihong Chen, Qingying Xiao, Feng Jiang,\\nJianquan Li, Xiang Wan, Benyou Wang, and Haizhou Li. Cmb: A comprehensive medical benchmark in chinese,\\n2024.\\n[77] Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh\\nPadmakumar, Johnny Ma, Jana Thompson, He He, and Samuel R. Bowman. Quality: Question answering with\\nlong input texts, yes!, 2022.\\n37'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdfs/agentic_rag.pdf', 'total_pages': 39, 'page': 37, 'page_label': '38', 'source_file': 'agentic_rag.pdf', 'file_type': 'pdf'}, page_content='[78] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: A question answering\\nchallenge targeting commonsense knowledge. In Jill Burstein, Christy Doran, and Thamar Solorio, editors,\\nProceedings of the 2019 Conference of the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4149–4158, Minneapolis,\\nMinnesota, June 2019. Association for Computational Linguistics.\\n[79] Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V . Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, and Bryan\\nHooi. G-retriever: Retrieval-augmented generation for textual graph understanding and question answering,\\n2024.\\n[80] Sha Li, Heng Ji, and Jiawei Han. Document-level event argument extraction by conditional generation, 2021.\\n[81] Seth Ebner, Patrick Xia, Ryan Culkin, Kyle Rawlins, and Benjamin Van Durme. Multi-sentence argument\\nlinking, 2020.\\n[82] Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard of wikipedia:\\nKnowledge-powered conversational agents, 2019.\\n[83] Hongru Wang, Minda Hu, Yang Deng, Rui Wang, Fei Mi, Weichao Wang, Yasheng Wang, Wai-Chung Kwan,\\nIrwin King, and Kam-Fai Wong. Large language models as source planner for personalized knowledge-grounded\\ndialogue, 2023.\\n[84] Xinchao Xu, Zhibin Gou, Wenquan Wu, Zheng-Yu Niu, Hua Wu, Haifeng Wang, and Shihang Wang. Long time\\nno see! open-domain conversation with long-term persona memory, 2022.\\n[85] Tsung-Hsien Wen, Milica Gaši´c, Nikola Mrkši´c, Lina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes, David\\nVandyke, and Steve Young. Conditional generation and snapshot learning in neural dialogue systems. In\\nProceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2153–2162,\\nAustin, Texas, November 2016. Association for Computational Linguistics.\\n[86] Ruining He and Julian McAuley. Ups and downs: Modeling the visual evolution of fashion trends with one-class\\ncollaborative filtering. In Proceedings of the 25th International Conference on World Wide Web, WWW ’16, page\\n507–517, Republic and Canton of Geneva, CHE, 2016. International World Wide Web Conferences Steering\\nCommittee.\\n[87] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine really\\nfinish your sentence? In Anna Korhonen, David Traum, and Lluís Màrquez, editors, Proceedings of the 57th\\nAnnual Meeting of the Association for Computational Linguistics, pages 4791–4800, Florence, Italy, July 2019.\\nAssociation for Computational Linguistics.\\n[88] Seungone Kim, Se June Joo, Doyoung Kim, Joel Jang, Seonghyeon Ye, Jamin Shin, and Minjoon Seo. The\\ncot collection: Improving zero-shot and few-shot learning of language models via chain-of-thought fine-tuning,\\n2023.\\n[89] Amrita Saha, Vardaan Pahuja, Mitesh M. Khapra, Karthik Sankaranarayanan, and Sarath Chandar. Complex\\nsequential question answering: Towards learning to converse over linked question answer pairs with a knowledge\\ngraph. 2018.\\n[90] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-scale dataset for\\nfact extraction and VERification. In Marilyn Walker, Heng Ji, and Amanda Stent, editors,Proceedings of the 2018\\nConference of the North American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long Papers), pages 809–819, New Orleans, Louisiana, June 2018. Association for\\nComputational Linguistics.\\n[91] Neema Kotonya and Francesca Toni. Explainable automated fact-checking for public health claims, 2020.\\n[92] Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use a laptop?\\na question answering benchmark with implicit reasoning strategies, 2021.\\n[93] Hiroaki Hayashi, Prashant Budania, Peng Wang, Chris Ackerson, Raj Neervannan, and Graham Neubig. Wikiasp:\\nA dataset for multi-domain aspect-based summarization, 2020.\\n[94] Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don’t give me the details, just the summary! topic-aware\\nconvolutional neural networks for extreme summarization, 2018.\\n[95] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher\\nPotts. Recursive deep models for semantic compositionality over a sentiment treebank. In David Yarowsky,\\nTimothy Baldwin, Anna Korhonen, Karen Livescu, and Steven Bethard, editors, Proceedings of the 2013\\nConference on Empirical Methods in Natural Language Processing, pages 1631–1642, Seattle, Washington,\\nUSA, October 2013. Association for Computational Linguistics.\\n38'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdfs/agentic_rag.pdf', 'total_pages': 39, 'page': 38, 'page_label': '39', 'source_file': 'agentic_rag.pdf', 'file_type': 'pdf'}, page_content='[96] Sourav Saha, Jahedul Alam Junaed, Maryam Saleki, Arnab Sen Sharma, Mohammad Rashidujjaman Rifat,\\nMohamed Rahouti, Syed Ishtiaque Ahmed, Nabeel Mohammed, and Mohammad Ruhul Amin. Vio-lens: A novel\\ndataset of annotated social network posts leading to different forms of communal violence and its evaluation. In\\nFiroj Alam, Sudipta Kar, Shammur Absar Chowdhury, Farig Sadeque, and Ruhul Amin, editors,Proceedings\\nof the First Workshop on Bangla Language Processing (BLP-2023), pages 72–84, Singapore, December 2023.\\nAssociation for Computational Linguistics.\\n[97] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. Codesearchnet\\nchallenge: Evaluating the state of semantic code search, 2020.\\n[98] Nandan Thakur, Luiz Bonifacio, Xinyu Zhang, Odunayo Ogundepo, Ehsan Kamalloo, David Alfonso-Hermelo,\\nXiaoguang Li, Qun Liu, Boxing Chen, Mehdi Rezagholizadeh, and Jimmy Lin. \"knowing when you don’t know\":\\nA multilingual relevance assessment dataset for robust retrieval-augmented generation, 2024.\\n[99] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016.\\n[100] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert,\\nJerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to\\nsolve math word problems, 2021.\\n[101] Ralf Steinberger, Bruno Pouliquen, Anna Widiger, Camelia Ignat, Tomaž Erjavec, Dan Tufi¸ s, and Dániel Varga.\\nThe JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages. In Nicoletta Calzolari, Khalid\\nChoukri, Aldo Gangemi, Bente Maegaard, Joseph Mariani, Jan Odijk, and Daniel Tapias, editors, Proceedings of\\nthe Fifth International Conference on Language Resources and Evaluation (LREC‘06), Genoa, Italy, May 2006.\\nEuropean Language Resources Association (ELRA).\\n39')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_dir = \"../data/pdfs\"\n",
    "documents  = process_all_pdf(pdf_dir)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "371c1706",
   "metadata": {},
   "outputs": [],
   "source": [
    "  ### Split into Chunks  \n",
    "  def split_document(document, chunk_size=1000, chunk_overlap=200):\n",
    "     text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size,\n",
    "        chunk_overlap = chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "     )\n",
    "\n",
    "     split_docs = text_splitter.split_documents(document)\n",
    "     print(f\"Splitted {len(document)} documents into {len(split_docs)} chunks\")\n",
    "\n",
    "     if(split_docs):\n",
    "        print(\"Example chunk\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}......\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "     return split_docs\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b6ad207f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted 97 documents into 539 chunks\n",
      "Example chunk\n",
      "Content: Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz......\n",
      "Metadata: {'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdfs/attention.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}\n"
     ]
    }
   ],
   "source": [
    "chunks = split_document(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741cdb34",
   "metadata": {},
   "source": [
    "### Embeddings and VectorStore DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "75881e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushigupta/Documents/Projects/rag/ytrag/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4c883241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded Successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x162ba7590>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using sentence transformers\"\"\"\n",
    "\n",
    "    def __init__(self, model_name : str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"Initializing embedding manager\n",
    "        Args: model_name : hugging face model name for sentence transformer\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load sentence transformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded Successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred in loading model {self.model_name} : {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Generate embedding for a list of texts\n",
    "        Args:\n",
    "            texts: list of text strings to embed\n",
    "        Returns: \n",
    "            numpy array of embeddings with shape (len(texts), embedding_dims)\n",
    "        \"\"\"\n",
    "\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        print(f\"Generating embeddings for {len(texts)} texts\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar = True)\n",
    "        print(f\"Generating embeddings with shape {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "    def get_embeddings_dimensions(self) -> int:\n",
    "        \"\"\"Get the embedding dimensions of the model\"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        return self.mode.get_sentence_embedding_dimension()\n",
    "\n",
    "##initializing embedding manager\n",
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3bac9b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x165417b10>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "import numpy as np\n",
    "import chromadb\n",
    "from typing import List, Any\n",
    "\n",
    "class VectorStore:\n",
    "    \"\"\"For storing embeddings in Chroma vector store\"\"\"\n",
    "\n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            collection_name : Name of the ChromaDB collection\n",
    "            persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory \n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "\n",
    "        Args:\n",
    "            documents: List of LangChain Document objects\n",
    "            embeddings: Corresponding embeddings for documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "\n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"  \n",
    "            ids.append(doc_id)\n",
    "\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata[\"doc_index\"] = i\n",
    "            metadata[\"content_length\"] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            documents_text.append(doc.page_content)\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "\n",
    "        try:\n",
    "            self.collection.add( \n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store.\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "# Initialize and test\n",
    "vector_store = VectorStore()\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8bfc25c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 539 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 17/17 [00:09<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings with shape (539, 384)\n",
      "Adding 539 documents to vector store...\n",
      "Successfully added 539 documents to vector store.\n",
      "Total documents in collection: 539\n"
     ]
    }
   ],
   "source": [
    "texts = [doc.page_content for doc in chunks]\n",
    "\n",
    "##Generate the embeddings\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "##Store embedding in vector store\n",
    "vector_store.add_documents(chunks, embeddings) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f1d276",
   "metadata": {},
   "source": [
    "### Retriever Pipeline from Vector Store "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f7927962",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query based retriever from the vectore store\"\"\"\n",
    "\n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "            Initialize the retriever\n",
    "            Args:\n",
    "                vector_store: vector database that contains vector embeddings\n",
    "                embedding_manager: Embedding manager for generate query embeddings\n",
    "        \"\"\"\n",
    "\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a particular query\n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to run\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "        \n",
    "        Return:\n",
    "            List of Dictionaries containing retrieved documnets with their metadata\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"Retrieving documents for query: {query}\")\n",
    "        print(f\"Top K {top_k} , Score Threshold: {score_threshold}\")\n",
    "\n",
    "        ##Generate embedding for the asked query\n",
    "        query_embedding = embedding_manager.generate_embeddings([query])[0]\n",
    "\n",
    "        ##Search context in vector store\n",
    "        try: \n",
    "            results =  self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results = top_k\n",
    "            )\n",
    "\n",
    "            retrieved_docs = []\n",
    "\n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "\n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    similarity = 1-distance\n",
    "\n",
    "                    if similarity >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id' : doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity,\n",
    "                            'distance': distance,\n",
    "                            'rank': i+1\n",
    "                        })\n",
    "                    \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "         \n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            return retrieved_docs \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return [] \n",
    "rag_retriever = RAGRetriever(vector_store, embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e6b448f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x1654cdc50>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3b773343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: What is Large Language Models\n",
      "Top K 5 , Score Threshold: 0.0\n",
      "Generating embeddings for 1 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings with shape (1, 384)\n",
      "Retrieved 5 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_973cf1c7_47',\n",
       "  'content': 'Qiu), muhammad.saqib@data61.csiro.au (Muhammad Saqib),\\nsaeed.anwar@kfupm.edu.sa (Saeed Anwar),\\nmuhammad.usman@kfupm.edu.sa (Muhammad Usman),\\nnaveed.akhtar1@unimelb.edu.au (Naveed Akhtar),\\nnick.barnes@anu.edu.au (Nick Barnes), ajmal.mian@uwa.edu.au\\n(Ajmal Mian)\\nFigure 1: The trend of papers released over the years containing keywords\\n\"Large Language Model\", \"Large Language Model+ Fine-Tuning\", and \"Large\\nLanguage Model + Alignment\".\\nPreprint submitted to Elsevier October 18, 2024\\narXiv:2307.06435v10  [cs.CL]  17 Oct 2024',\n",
       "  'metadata': {'keywords': '',\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'creationdate': '2024-10-18T00:29:28+00:00',\n",
       "   'source_file': 'llms.pdf',\n",
       "   'doc_index': 47,\n",
       "   'trapped': '/False',\n",
       "   'subject': '',\n",
       "   'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'source': '../data/pdfs/llms.pdf',\n",
       "   'page': 0,\n",
       "   'file_type': 'pdf',\n",
       "   'content_length': 525,\n",
       "   'title': 'A Comprehensive Overview of Large Language Models',\n",
       "   'moddate': '2024-10-18T00:29:28+00:00',\n",
       "   'total_pages': 47,\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'page_label': '1'},\n",
       "  'similarity_score': 0.5394578576087952,\n",
       "  'distance': 0.46054214239120483,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_dce1c55d_321',\n",
       "  'content': 'windows for large language models, in: Proceedings of the 61st Annual\\nMeeting of the Association for Computational Linguistics (V olume 1:\\nLong Papers), 2023, pp. 6383–6402. 18\\n[191] W. Wang, L. Dong, H. Cheng, X. Liu, X. Yan, J. Gao, F. Wei,\\nAugmenting language models with long-term memory, arXiv preprint\\narXiv:2306.07174 (2023). 18\\n[192] X. Xu, Z. Gou, W. Wu, Z.-Y . Niu, H. Wu, H. Wang, S. Wang, Long\\ntime no see! open-domain conversation with long-term persona memory,\\narXiv preprint arXiv:2203.05797 (2022). 18\\n[193] S. Borgeaud, A. Mensch, J. Ho ffmann, T. Cai, E. Rutherford, K. Milli-\\ncan, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark, et al.,\\nImproving language models by retrieving from trillions of tokens, in:\\nInternational conference on machine learning, PMLR, 2022, pp. 2206–\\n2240. 18, 19, 34\\n[194] W. Zhong, L. Guo, Q. Gao, Y . Wang, Memorybank: Enhanc-\\ning large language models with long-term memory, arXiv preprint\\narXiv:2305.10250 (2023). 18',\n",
       "  'metadata': {'source': '../data/pdfs/llms.pdf',\n",
       "   'title': 'A Comprehensive Overview of Large Language Models',\n",
       "   'keywords': '',\n",
       "   'source_file': 'llms.pdf',\n",
       "   'content_length': 976,\n",
       "   'trapped': '/False',\n",
       "   'page_label': '40',\n",
       "   'doc_index': 321,\n",
       "   'creationdate': '2024-10-18T00:29:28+00:00',\n",
       "   'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;',\n",
       "   'subject': '',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'total_pages': 47,\n",
       "   'file_type': 'pdf',\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'moddate': '2024-10-18T00:29:28+00:00',\n",
       "   'page': 39},\n",
       "  'similarity_score': 0.4400674104690552,\n",
       "  'distance': 0.5599325895309448,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_69f05f03_43',\n",
       "  'content': 'A Comprehensive Overview of Large Language Models\\nHumza Naveeda, Asad Ullah Khanb,∗, Shi Qiuc,∗, Muhammad Saqibd,e,∗, Saeed Anwarf,g, Muhammad Usmanf,g, Naveed Akhtarh,j,\\nNick Barnesi, Ajmal Mianj\\naThe University of Sydney, Sydney, Australia\\nbUniversity of Engineering and Technology (UET), Lahore, Pakistan\\ncThe Chinese University of Hong Kong (CUHK), HKSAR, China\\ndUniversity of Technology Sydney (UTS), Sydney, Australia\\neCommonwealth Scientific and Industrial Research Organisation (CSIRO), Sydney, Australia\\nfKing Fahd University of Petroleum and Minerals (KFUPM), Dhahran, Saudi Arabia\\ngSDAIA-KFUPM Joint Research Center for Artificial Intelligence (JRCAI), Dhahran, Saudi Arabia\\nhThe University of Melbourne (UoM), Melbourne, Australia\\niAustralian National University (ANU), Canberra, Australia\\njThe University of Western Australia (UWA), Perth, Australia\\nAbstract\\nLarge Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and',\n",
       "  'metadata': {'keywords': '',\n",
       "   'file_type': 'pdf',\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'total_pages': 47,\n",
       "   'page_label': '1',\n",
       "   'title': 'A Comprehensive Overview of Large Language Models',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'source': '../data/pdfs/llms.pdf',\n",
       "   'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'trapped': '/False',\n",
       "   'doc_index': 43,\n",
       "   'content_length': 992,\n",
       "   'page': 0,\n",
       "   'creationdate': '2024-10-18T00:29:28+00:00',\n",
       "   'source_file': 'llms.pdf',\n",
       "   'subject': '',\n",
       "   'moddate': '2024-10-18T00:29:28+00:00'},\n",
       "  'similarity_score': 0.3886106610298157,\n",
       "  'distance': 0.6113893389701843,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_ad8ceee6_328',\n",
       "  'content': 'guage models with massive tools via tool embeddings, arXiv preprint\\narXiv:2305.11554 (2023). 19\\n[221] S. G. Patil, T. Zhang, X. Wang, J. E. Gonzalez, Gorilla: Large language\\nmodel connected with massive apis, arXiv preprint arXiv:2305.15334\\n(2023). 19\\n[222] Q. Xu, F. Hong, B. Li, C. Hu, Z. Chen, J. Zhang, On the tool manipu-\\nlation capability of open-source large language models, arXiv preprint\\narXiv:2305.16504 (2023). 19\\n[223] Y . Qin, S. Liang, Y . Ye, K. Zhu, L. Yan, Y . Lu, Y . Lin, X. Cong, X. Tang,\\nB. Qian, et al., Toolllm: Facilitating large language models to master\\n16000+ real-world apis, arXiv preprint arXiv:2307.16789 (2023). 19,\\n40',\n",
       "  'metadata': {'page_label': '40',\n",
       "   'trapped': '/False',\n",
       "   'source': '../data/pdfs/llms.pdf',\n",
       "   'doc_index': 328,\n",
       "   'title': 'A Comprehensive Overview of Large Language Models',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'keywords': '',\n",
       "   'content_length': 651,\n",
       "   'page': 39,\n",
       "   'subject': '',\n",
       "   'total_pages': 47,\n",
       "   'file_type': 'pdf',\n",
       "   'creationdate': '2024-10-18T00:29:28+00:00',\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'moddate': '2024-10-18T00:29:28+00:00',\n",
       "   'source_file': 'llms.pdf'},\n",
       "  'similarity_score': 0.3035588264465332,\n",
       "  'distance': 0.6964411735534668,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_11ec11e5_287',\n",
       "  'content': 'Q. Liu, Aligning large language models with human: A survey, arXiv\\npreprint arXiv:2307.12966 (2023). 2\\n[57] X. Zhu, J. Li, Y . Liu, C. Ma, W. Wang, A survey on model compression\\nfor large language models, arXiv preprint arXiv:2308.07633 (2023). 2\\n[58] S. Yin, C. Fu, S. Zhao, K. Li, X. Sun, T. Xu, E. Chen, A survey on multi-\\nmodal large language models, arXiv preprint arXiv:2306.13549 (2023).\\n2, 22, 23\\n[59] J. J. Webster, C. Kit, Tokenization as the initial phase in nlp, in: COL-\\nING 1992 volume 4: The 14th international conference on computa-\\ntional linguistics, 1992. 4\\n[60] T. Kudo, Subword regularization: Improving neural network translation\\nmodels with multiple subword candidates, in: Proceedings of the 56th\\nAnnual Meeting of the Association for Computational Linguistics (V ol-\\nume 1: Long Papers), 2018, pp. 66–75. 4\\n[61] R. Sennrich, B. Haddow, A. Birch, Neural machine translation of rare\\nwords with subword units, in: Proceedings of the 54th Annual Meet-',\n",
       "  'metadata': {'title': 'A Comprehensive Overview of Large Language Models',\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'doc_index': 287,\n",
       "   'subject': '',\n",
       "   'moddate': '2024-10-18T00:29:28+00:00',\n",
       "   'source': '../data/pdfs/llms.pdf',\n",
       "   'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;',\n",
       "   'keywords': '',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'file_type': 'pdf',\n",
       "   'source_file': 'llms.pdf',\n",
       "   'total_pages': 47,\n",
       "   'content_length': 972,\n",
       "   'page_label': '37',\n",
       "   'page': 36,\n",
       "   'creationdate': '2024-10-18T00:29:28+00:00',\n",
       "   'trapped': '/False',\n",
       "   'creator': 'LaTeX with hyperref'},\n",
       "  'similarity_score': 0.26904547214508057,\n",
       "  'distance': 0.7309545278549194,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"What is Large Language Models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d61426",
   "metadata": {},
   "source": [
    "## Integrationg VectorDB Context Pipeline with LLM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "51b2e00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## simple rag pipeline with GROQ LLM\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() \n",
    "\n",
    "##Initialize groq LLM and set GROQ_API_KEY in Environments\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\") \n",
    "llm=ChatGroq(groq_api_key=groq_api_key, model_name=\"llama-3.1-8b-instant\",temperature=0.1,max_tokens=1024)\n",
    "\n",
    "def rag(query, retriever, llm , top_k=3):\n",
    "    \"\"\"Retreieve the context\"\"\"\n",
    "    results = retriever.retrieve(query, top_k=top_k)\n",
    "    context=\"\\n\\n\".join([doc[\"content\"] for doc in results]) if results else \"\"\n",
    "    if not context:\n",
    "        print(f\"No relevant context found to answer the question\")\n",
    "    \n",
    "    \"\"\"Generate the answer using groq LLM\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"Use the following context to answer the question concisely.\n",
    "        Context: {context}\n",
    "        Question: {query}\n",
    "        Answer: \n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "    return response.content\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "320189f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: What is a Large Language Model\n",
      "Top K 3 , Score Threshold: 0.0\n",
      "Generating embeddings for 1 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings with shape (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A Large Language Model (LLM) is a type of artificial intelligence model that has demonstrated remarkable capabilities in natural language processing tasks.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = rag(\"What is a Large Language Model\", rag_retriever, llm)\n",
    "ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3502d6",
   "metadata": {},
   "source": [
    "### Enhanced RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4df23b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_advanced(query, retriever, llm, top_k=5, min_score=0.2, return_context=False):\n",
    "    \"\"\"\n",
    "    Rag with extra features :\n",
    "        retruns answers, sources, confidence, and optionally full context\n",
    "    \"\"\"\n",
    "\n",
    "    results = retriever.retrieve(query, top_k=top_k, score_threshold=min_score)\n",
    "    if not results:\n",
    "        return {\"source\": \"No relevant context found\", \"source\": [] , \"confidence\": 0.0, \"context\": \"\"}\n",
    "\n",
    "    context = \"\\n\\n\".join([doc[\"content\"] for doc in results])\n",
    "    sources = [{\n",
    "         \"source\" : doc[\"metadata\"].get(\"source\", doc[\"metadata\"].get(\"source\", \"unknown\")),\n",
    "         \"page\" : doc[\"metadata\"].get(\"page\", \"unknown\"),\n",
    "         \"score\" : doc[\"similarity_score\"],\n",
    "         \"preview\" : doc[\"content\"][:300] + \".....\"\n",
    "    } for doc in results]\n",
    "\n",
    "    confidence = max([doc[\"similarity_score\"] for doc in results])\n",
    "\n",
    "    prompt = f\"\"\"Use the follwoing context to answer the following question.\n",
    "        context : {context}\n",
    "        question : {query}\n",
    "        answer : \"\"\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "\n",
    "    output = {\n",
    "        \"answer\" : response.content,\n",
    "        \"source\" : sources,\n",
    "        \"confidence\" : confidence\n",
    "    }\n",
    "\n",
    "    if return_context:\n",
    "        output[\"context\"] = context\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6353b39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: What is multi head attention?\n",
      "Top K 3 , Score Threshold: 0.0\n",
      "Generating embeddings for 1 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings with shape (1, 384)\n",
      "Retrieved 2 documents (after filtering)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': 'Multi-head attention is a mechanism used in the Transformer model that allows the model to jointly attend to information from different representation subspaces at different positions. It is achieved by applying attention in parallel to linearly projected versions of the queries, keys, and values, and then concatenating and projecting the output. This is done using multiple attention \"heads\" (in this case, 8 parallel attention layers), each with its own set of learned linear projections.',\n",
       " 'source': [{'source': '../data/pdfs/attention.pdf',\n",
       "   'page': 4,\n",
       "   'score': 0.13510406017303467,\n",
       "   'preview': 'MultiHead(Q,K,V ) = Concat(head1,..., headh)WO\\nwhere headi = Attention(QWQ\\ni ,KW K\\ni ,VW V\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈Rdmodel×dk , WK\\ni ∈Rdmodel×dk , WV\\ni ∈Rdmodel×dv\\nand WO ∈Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these .....'},\n",
       "  {'source': '../data/pdfs/attention.pdf',\n",
       "   'page': 3,\n",
       "   'score': 0.104733407497406,\n",
       "   'preview': 'we found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensio.....'}],\n",
       " 'confidence': 0.13510406017303467,\n",
       " 'context': 'MultiHead(Q,K,V ) = Concat(head1,..., headh)WO\\nwhere headi = Attention(QWQ\\ni ,KW K\\ni ,VW V\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈Rdmodel×dk , WK\\ni ∈Rdmodel×dk , WV\\ni ∈Rdmodel×dv\\nand WO ∈Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h= 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n\\nwe found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = ∑dk\\ni=1 qiki, has mean 0 and variance dk.\\n4'}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = rag_advanced(\"What is multi head attention?\", rag_retriever, llm, top_k=3, min_score=0.0, return_context=True)\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "99cf39b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Advanced RAG Pipeline: Streaming, Citations, History, Summarization ---\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "class AdvancedRAGPipeline:\n",
    "    def __init__(self, retriever, llm):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "        self.history = []  # Store query history\n",
    "\n",
    "    def query(self, question: str, top_k: int = 5, min_score: float = 0.2, stream: bool = False, summarize: bool = False) -> Dict[str, Any]:\n",
    "        # Retrieve relevant documents\n",
    "        results = self.retriever.retrieve(question, top_k=top_k, score_threshold=min_score)\n",
    "        if not results:\n",
    "            answer = \"No relevant context found.\"\n",
    "            sources = []\n",
    "            context = \"\"\n",
    "        else:\n",
    "            context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "            sources = [{\n",
    "                'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "                'page': doc['metadata'].get('page', 'unknown'),\n",
    "                'score': doc['similarity_score'],\n",
    "                'preview': doc['content'][:120] + '...'\n",
    "            } for doc in results]\n",
    "            # Streaming answer simulation\n",
    "            prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\"\"\n",
    "            if stream:\n",
    "                print(\"Streaming answer:\")\n",
    "                for i in range(0, len(prompt), 80):\n",
    "                    print(prompt[i:i+80], end='', flush=True)\n",
    "                    time.sleep(0.05)\n",
    "                print()\n",
    "            response = self.llm.invoke([prompt.format(context=context, question=question)])\n",
    "            answer = response.content\n",
    "\n",
    "        # Add citations to answer\n",
    "        citations = [f\"[{i+1}] {src['source']} (page {src['page']})\" for i, src in enumerate(sources)]\n",
    "        answer_with_citations = answer + \"\\n\\nCitations:\\n\" + \"\\n\".join(citations) if citations else answer\n",
    "\n",
    "        # Optionally summarize answer\n",
    "        summary = None\n",
    "        if summarize and answer:\n",
    "            summary_prompt = f\"Summarize the following answer in 2 sentences:\\n{answer}\"\n",
    "            summary_resp = self.llm.invoke([summary_prompt])\n",
    "            summary = summary_resp.content\n",
    "\n",
    "        # Store query history\n",
    "        self.history.append({\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'sources': sources,\n",
    "            'summary': summary\n",
    "        })\n",
    "\n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer_with_citations,\n",
    "            'sources': sources,\n",
    "            'summary': summary,\n",
    "            'history': self.history\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "955f8931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: what are Large Language Models\n",
      "Top K 3 , Score Threshold: 0.0\n",
      "Generating embeddings for 1 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings with shape (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n",
      "Streaming answer:\n",
      "Use the following context to answer the question concisely.\n",
      "Context:\n",
      "Qiu), muhammad.saqib@data61.csiro.au (Muhammad Saqib),\n",
      "saeed.anwar@kfupm.edu.sa (Saeed Anwar),\n",
      "muhammad.usman@kfupm.edu.sa (Muhammad Usman),\n",
      "naveed.akhtar1@unimelb.edu.au (Naveed Akhtar),\n",
      "nick.barnes@anu.edu.au (Nick Barnes), ajmal.mian@uwa.edu.au\n",
      "(Aj"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mal Mian)\n",
      "Figure 1: The trend of papers released over the years containing keywords\n",
      "\"Large Language Model\", \"Large Language Model+ Fine-Tuning\", and \"Large\n",
      "Language Model + Alignment\".\n",
      "Preprint submitted to Elsevier October 18, 2024\n",
      "arXiv:2307.06435v10  [cs.CL]  17 Oct 2024\n",
      "\n",
      "windows for large language models, in: Proceedings of the 61st Annual\n",
      "Meeting of the Association for Computational Linguistics (V olume 1:\n",
      "Long Papers), 2023, pp. 6383–6402. 18\n",
      "[191] W. Wang, L. Dong, H. Cheng, X. Liu, X. Yan, J. Gao, F. Wei,\n",
      "Augmenting language models with long-term memory, arXiv preprint\n",
      "arXiv:2306.07174 (2023). 18\n",
      "[192] X. Xu, Z. Gou, W. Wu, Z.-Y . Niu, H. Wu, H. Wang, S. Wang, Long\n",
      "time no see! open-domain conversation with long-term persona memory,\n",
      "arXiv preprint arXiv:2203.05797 (2022). 18\n",
      "[193] S. Borgeaud, A. Mensch, J. Ho ffmann, T. Cai, E. Rutherford, K. Milli-\n",
      "can, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark, et al.,\n",
      "Improving language models by retrieving from trillions of tokens, in:\n",
      "International conference on machine learning, PMLR, 2022, pp. 2206–\n",
      "2240. 18, 19, 34\n",
      "[194] W. Zhong, L. Guo, Q. Gao, Y . Wang, Memorybank: Enhanc-\n",
      "ing large language models with long-term memory, arXiv preprint\n",
      "arXiv:2305.10250 (2023). 18\n",
      "\n",
      "A Comprehensive Overview of Large Language Models\n",
      "Humza Naveeda, Asad Ullah Khanb,∗, Shi Qiuc,∗, Muhammad Saqibd,e,∗, Saeed Anwarf,g, Muhammad Usmanf,g, Naveed Akhtarh,j,\n",
      "Nick Barnesi, Ajmal Mianj\n",
      "aThe University of Sydney, Sydney, Australia\n",
      "bUniversity of Engineering and Technology (UET), Lahore, Pakistan\n",
      "cThe Chinese University of Hong Kong (CUHK), HKSAR, China\n",
      "dUniversity of Technology Sydney (UTS), Sydney, Australia\n",
      "eCommonwealth Scientific and Industrial Research Organisation (CSIRO), Sydney, Australia\n",
      "fKing Fahd University of Petroleum and Minerals (KFUPM), Dhahran, Saudi Arabia\n",
      "gSDAIA-KFUPM Joint Research Center for Artificial Intelligence (JRCAI), Dhahran, Saudi Arabia\n",
      "hThe University of Melbourne (UoM), Melbourne, Australia\n",
      "iAustralian National University (ANU), Canberra, Australia\n",
      "jThe University of Western Australia (UWA), Perth, Australia\n",
      "Abstract\n",
      "Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and\n",
      "\n",
      "Question: what are Large Language Models\n",
      "\n",
      "Answer:\n",
      "\n",
      "Final Answer: Large Language Models (LLMs) are a type of artificial intelligence model that have demonstrated remarkable capabilities in natural language processing tasks.\n",
      "\n",
      "Citations:\n",
      "[1] llms.pdf (page 0)\n",
      "[2] llms.pdf (page 39)\n",
      "[3] llms.pdf (page 0)\n",
      "Summary: Large Language Models (LLMs) are a type of artificial intelligence model that excel in natural language processing tasks. They have shown impressive capabilities in handling and understanding human language.\n",
      "History: {'question': 'what are Large Language Models', 'answer': 'Large Language Models (LLMs) are a type of artificial intelligence model that have demonstrated remarkable capabilities in natural language processing tasks.', 'sources': [{'source': 'llms.pdf', 'page': 0, 'score': 0.5477049350738525, 'preview': 'Qiu), muhammad.saqib@data61.csiro.au (Muhammad Saqib),\\nsaeed.anwar@kfupm.edu.sa (Saeed Anwar),\\nmuhammad.usman@kfupm.edu....'}, {'source': 'llms.pdf', 'page': 39, 'score': 0.4616367816925049, 'preview': 'windows for large language models, in: Proceedings of the 61st Annual\\nMeeting of the Association for Computational Lingu...'}, {'source': 'llms.pdf', 'page': 0, 'score': 0.396869421005249, 'preview': 'A Comprehensive Overview of Large Language Models\\nHumza Naveeda, Asad Ullah Khanb,∗, Shi Qiuc,∗, Muhammad Saqibd,e,∗, Sa...'}], 'summary': 'Large Language Models (LLMs) are a type of artificial intelligence model that excel in natural language processing tasks. They have shown impressive capabilities in handling and understanding human language.'}\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "adv_rag = AdvancedRAGPipeline(rag_retriever, llm)\n",
    "result = adv_rag.query(\"what are Large Language Models\", top_k=3, min_score=0.0, stream=True, summarize=True)\n",
    "print(\"\\nFinal Answer:\", result['answer'])\n",
    "print(\"Summary:\", result['summary'])\n",
    "print(\"History:\", result['history'][-1])\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52743d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
